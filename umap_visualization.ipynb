{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10820f-f2c8-43ff-9b60-1ee5a07e9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22378e-a165-496c-a0a1-c729e4f6da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== UMAP (GRID) — Save ONLY IMAGES into model-specific dir ======================\n",
    "# - Loads image-head from unimodal OR multimodal checkpoint\n",
    "# - Embeddings taken from the **deepest pooling layer** in img_enc.features (pre-GAP)\n",
    "# - Two eval pipelines: \"mm\" (PIL/torchvision) or \"uni\" (cv2/Albumentations)\n",
    "# - Grid search over n_neighbors × min_dist\n",
    "# - Saves figures into checkpoint-derived directory\n",
    "# ================================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, random, logging, pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import cv2, albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "try:\n",
    "    import umap   # pip install umap-learn\n",
    "except ImportError:\n",
    "    import umap.umap_ as umap\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# --------------------------- Repro & logging --------------------------- #\n",
    "def set_seed(seed: int = 1337) -> None:\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(1337)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "log = logging.getLogger(\"umap-save-exact\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ef8fa-e238-456a-ad83-fb7ffb45d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User config\n",
    "\n",
    "CKPT_PATH = Path(\"/mm_grid1/best_vgg11_multimodal_val_loss.pt\") #RAW, STRUCTURED, UNIMODAL\n",
    "DATASET_ROOT = Path(\"/dataset\")\n",
    "IMAGES_DIR   = DATASET_ROOT / \"images\"\n",
    "CSV_PATH   = DATASET_ROOT / \"label_test.csv\"  # <-- headerless: filename,label\n",
    "\n",
    "# Which eval pipeline to use:\n",
    "PIPELINE = \"uni\"              # set \"uni\" for unimodal checkpoints\n",
    "EXPECT = \"unimodal\"           # \"auto\" | \"unimodal\" | \"multimodal\"\n",
    "ARCH_NAME = \"vgg11\"\n",
    "ARCH_TAG = ARCH_NAME.lower()\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# UMAP grid\n",
    "UMAP_N_NEIGHBORS_GRID = [5, 10, 15, 20, 50, 100, 200]\n",
    "UMAP_MIN_DIST_GRID = [0.0, 0.05, 0.1, 0.2, 0.25, 0.4, 0.5]\n",
    "UMAP_METRIC  = \"euclidean\"\n",
    "UMAP_RANDOM_STATE = 42\n",
    "TOPK_PLOTS = 0\n",
    "\n",
    "# Plot styling EXACTLY as requested\n",
    "CLASS_NAMES = [\"Normal\", \"TB\"]                 # 0 -> Normal, 1 -> TB\n",
    "CMAP = ListedColormap([\"blue\", \"red\"])         # 0 -> blue, 1 -> red\n",
    "\n",
    "# Output dir\n",
    "CKPT_DIR: Path = CKPT_PATH if CKPT_PATH.is_dir() else CKPT_PATH.parent\n",
    "ckpt_dir_str = CKPT_DIR.as_posix()\n",
    "is_unimodal = \"models_img_\" in ckpt_dir_str\n",
    "\n",
    "if is_unimodal:\n",
    "    base_dir = CKPT_DIR.parent if CKPT_DIR.name.startswith((\"gradcam_\", \"cam_\", \"umap_\")) else CKPT_DIR\n",
    "    SAVE_DIR = base_dir / f\"umap_{ARCH_TAG}_unimodal\"\n",
    "else:\n",
    "    SAVE_DIR = CKPT_DIR / f\"umap_{ARCH_TAG}_multimodal_structured\"\n",
    "\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log.info(f\"[UMAP] Output directory: {SAVE_DIR}\")\n",
    "\n",
    "# Filenames for saving all combos & best-by-silhouette\n",
    "ALL_PNG_PATTERN = SAVE_DIR / f\"umap_{ARCH_TAG}_unimodal_nn{{nn}}_md{{md:.2f}}.png\"\n",
    "BEST_SIL_PNG = SAVE_DIR / f\"umap_best_{ARCH_TAG}_unimodal_by_silhouette.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce910750-6b5f-4a4f-9f6e-7c3e73618f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV loader\n",
    "\n",
    "def _is_image_name(name: str) -> bool:\n",
    "    return Path(name).suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "def load_headerless_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    if not csv_path.is_file():\n",
    "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"CSV must have >=2 columns: filename,label. Got {df.shape}\")\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = [\"img\", \"label\"]\n",
    "    df[\"img\"] = df[\"img\"].astype(str).map(lambda s: os.path.basename(s.strip()))\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"img\"].map(_is_image_name)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Image head (shared topology)\n",
    "def _ends_with_3x3(module: nn.Module) -> bool:\n",
    "    last = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d): last = m\n",
    "    return (last is not None) and (tuple(getattr(last, \"kernel_size\",(0,0)))==(3,3))\n",
    "\n",
    "def _post3x3(ch: int) -> nn.Sequential:\n",
    "    # If training used this stage, keep it; else set to nn.Identity()\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ch, ch, 3, 1, 1, bias=False),\n",
    "        nn.BatchNorm2d(ch),\n",
    "        nn.SiLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class VGG11BN_ImageEnc(nn.Module):\n",
    "    def __init__(self, p_drop: float = 0.3, use_post3x3: bool = True):\n",
    "        super().__init__()\n",
    "        m = torchvision.models.vgg11_bn(weights=None)\n",
    "        self.features = m.features\n",
    "        ch = None\n",
    "        for mod in self.features.modules():\n",
    "            if isinstance(mod, nn.Conv2d): ch = mod.out_channels\n",
    "        if ch is None: raise RuntimeError(\"Could not infer channels from VGG11-BN features.\")\n",
    "        self.post3x3 = _post3x3(ch) if (use_post3x3 and not _ends_with_3x3(self.features)) else nn.Identity()\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = int(ch)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.features(x); f = self.post3x3(f); f = self.gap(f).flatten(1); f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "class ImageHead(nn.Module):\n",
    "    \"\"\"img_enc → proj → ReLU → mid → cls; used by BOTH unimodal & multimodal image paths.\"\"\"\n",
    "    def __init__(self, hidden: int = 256, n_classes: int = 2, use_post3x3: bool = True):\n",
    "        super().__init__()\n",
    "        self.img_enc = VGG11BN_ImageEnc(p_drop=0.3, use_post3x3=use_post3x3)\n",
    "        C  = self.img_enc.out_dim\n",
    "        self.proj = nn.Linear(C, hidden)   # alias for 'img_proj.*'\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.mid = nn.Linear(hidden, hidden)\n",
    "        self.cls = nn.Linear(hidden, n_classes)\n",
    "    def forward_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.img_enc(image); z = self.mid(self.act(self.proj(f))); return self.cls(z)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward_image(x)\n",
    "\n",
    "# Checkpoint loader\n",
    "def _strip_prefix(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    if not sd: return sd\n",
    "    keys = list(sd.keys())\n",
    "    prefixes = [pref for k in keys for pref in (\"module.\",\"model.\",\"net.\") if k.startswith(pref)]\n",
    "    if not prefixes: return sd\n",
    "    pref = max(set(prefixes), key=prefixes.count)\n",
    "    return { (k[len(pref):] if k.startswith(pref) else k): v for k,v in sd.items() }\n",
    "\n",
    "def _load_sd(ckpt_path: Path) -> Dict[str, torch.Tensor]:\n",
    "    try:\n",
    "        chk = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        chk = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        chk = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False, pickle_module=pickle)\n",
    "    if isinstance(chk, nn.Module):\n",
    "        if hasattr(chk, \"state_dict\"): return chk.state_dict()\n",
    "        raise RuntimeError(\"Pickled nn.Module without state_dict; please save state_dict next time.\")\n",
    "    if isinstance(chk, dict) and \"state_dict\" in chk and isinstance(chk[\"state_dict\"], dict):\n",
    "        sd = chk[\"state_dict\"]\n",
    "    elif isinstance(chk, dict) and \"model\" in chk and isinstance(chk[\"model\"], dict):\n",
    "        sd = chk[\"model\"]\n",
    "    elif isinstance(chk, dict) and all(isinstance(v, torch.Tensor) for v in chk.values()):\n",
    "        sd = chk\n",
    "    else:\n",
    "        tensorish = {k: v for k, v in (chk if isinstance(chk, dict) else {}).items() if isinstance(v, torch.Tensor)}\n",
    "        if tensorish: sd = tensorish\n",
    "        else: raise RuntimeError(\"No tensor state_dict found in checkpoint.\")\n",
    "    return _strip_prefix(sd)\n",
    "\n",
    "def _has_image_head_keys(sd: Dict[str, torch.Tensor]) -> bool:\n",
    "    ks = sd.keys()\n",
    "    return any(k.startswith((\"img_enc.\",\"proj.\",\"img_proj.\",\"mid.\",\"cls.\",\"image_head.\")) for k in ks)\n",
    "\n",
    "def build_image_head_from_ckpt(ckpt_path: Path,\n",
    "                               n_classes: int = 2,\n",
    "                               expect: str = \"auto\",\n",
    "                               use_post3x3: bool = True) -> nn.Module:\n",
    "    sd = _load_sd(ckpt_path)\n",
    "    if not _has_image_head_keys(sd):\n",
    "        msg = (f\"Checkpoint lacks image-head keys ('img_enc.', 'proj.'/'img_proj.', 'mid.', 'cls.' or 'image_head.'). \"\n",
    "               f\"File: {ckpt_path}\")\n",
    "        raise RuntimeError(msg if expect == \"auto\" else f\"[{expect}] {msg}\")\n",
    "\n",
    "    # Unwrap common wrapper prefixes, alias img_proj -> proj\n",
    "    remap = {}\n",
    "    for k, v in sd.items():\n",
    "        nk = k\n",
    "        if nk.startswith(\"image_head.\"): nk = nk.replace(\"image_head.\", \"\", 1)\n",
    "        if nk.startswith(\"img_proj.\"): nk = nk.replace(\"img_proj.\",   \"proj.\", 1)\n",
    "        remap[nk] = v\n",
    "\n",
    "    model = ImageHead(hidden=256, n_classes=n_classes, use_post3x3=use_post3x3)\n",
    "    missing, unexpected = model.load_state_dict(remap, strict=False)\n",
    "    if missing: log.warning(f\"[CKPT] Missing keys (first few): {missing[:8]}\")\n",
    "    if unexpected: log.warning(f\"[CKPT] Unexpected keys (first few): {unexpected[:8]}\")\n",
    "    log.info(f\"[CKPT] Image head reconstructed from {ckpt_path.name} (strict=False).\")\n",
    "    return model\n",
    "\n",
    "# Datasets\n",
    "class PILImageDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, df: pd.DataFrame, size: int = 224) -> None:\n",
    "        super().__init__()\n",
    "        self.root   = Path(images_dir)\n",
    "        self.names  = df[\"img\"].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize((size, size)),\n",
    "            T.Lambda(lambda x: x.convert(\"RGB\") if x.mode != \"RGB\" else x),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "        ])\n",
    "    def __len__(self) -> int: return len(self.names)\n",
    "    def __getitem__(self, i: int):\n",
    "        name = self.names[i]; y = self.labels[i]\n",
    "        p = self.root / name\n",
    "        img = Image.open(p)\n",
    "        x = self.tf(img)\n",
    "        return x, int(y), name\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "class CV2ImageDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, df: pd.DataFrame, size: int = 224) -> None:\n",
    "        super().__init__()\n",
    "        self.root   = Path(images_dir)\n",
    "        self.names  = df[\"img\"].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "        self.tf = A.Compose([\n",
    "            A.Resize(size, size, interpolation=cv2.INTER_AREA),\n",
    "            A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    def __len__(self) -> int: return len(self.names)\n",
    "    def __getitem__(self, i: int):\n",
    "        name = self.names[i]; y = self.labels[i]\n",
    "        p = self.root / name\n",
    "        img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None: raise FileNotFoundError(f\"Image not found/unreadable: {p}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        x = self.tf(image=img)[\"image\"]\n",
    "        return x, int(y), name\n",
    "\n",
    "# --------------------------- Deepest pooling hook --------------------------- #\n",
    "def find_deepest_pool(module: nn.Module) -> nn.Module:\n",
    "    last_pool = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.MaxPool2d): last_pool = m\n",
    "    if last_pool is None:\n",
    "        raise RuntimeError(\"No MaxPool2d found under img_enc.features.\")\n",
    "    return last_pool\n",
    "\n",
    "class FeatureHook:\n",
    "    def __init__(self, layer: nn.Module):\n",
    "        self.feats = []\n",
    "        self.h = layer.register_forward_hook(self._hook)\n",
    "    def _hook(self, module, inp, out):\n",
    "        self.feats.append(out.detach().cpu())\n",
    "    def close(self): self.h.remove()\n",
    "\n",
    "def global_avg_pool_4d(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.mean(dim=(2,3))  # (B,C,H,W)->(B,C)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model: nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval().to(DEVICE)\n",
    "    if not hasattr(model, \"img_enc\") or not hasattr(model.img_enc, \"features\"):\n",
    "        raise AttributeError(\"Model lacks img_enc.features; not an image head.\")\n",
    "    target_layer = find_deepest_pool(model.img_enc.features)\n",
    "    hk = FeatureHook(target_layer)\n",
    "    labels = []\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        _ = model.forward_image(x) if hasattr(model, \"forward_image\") else model(x)\n",
    "        labels.append(y.numpy())\n",
    "\n",
    "    if not hk.feats:\n",
    "        hk.close(); raise RuntimeError(\"No features captured; wrong hook/layer?\")\n",
    "    fmaps = torch.cat(hk.feats, dim=0)  # (N,C,H,W)\n",
    "    hk.close()\n",
    "    emb = global_avg_pool_4d(fmaps).numpy()  # (N,C)\n",
    "    y = np.concatenate(labels).astype(np.int64)\n",
    "    return emb, y\n",
    "    \n",
    "# --------------------------- UMAP + grid (SAVE ALL @ 400 dpi + BEST-by-silhouette) --------------------------- #\n",
    "def run_umap(X: np.ndarray, n_neighbors: int, min_dist: float,\n",
    "             metric: str = UMAP_METRIC, random_state: int = UMAP_RANDOM_STATE) -> np.ndarray:\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist,\n",
    "                        metric=metric, random_state=random_state)\n",
    "    return reducer.fit_transform(X)\n",
    "\n",
    "def score_2d(Z: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    m = {}\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        try: m[\"silhouette\"] = float(silhouette_score(Z, labels))\n",
    "        except Exception: m[\"silhouette\"] = float(\"nan\")\n",
    "        try: m[\"db_index\"]  = float(davies_bouldin_score(Z, labels))\n",
    "        except Exception: m[\"db_index\"] = float(\"nan\")\n",
    "        try:\n",
    "            knn = KNeighborsClassifier(n_neighbors=1)\n",
    "            knn.fit(Z, labels)\n",
    "            m[\"knn1_acc\"] = float(knn.score(Z, labels))\n",
    "        except Exception:\n",
    "            m[\"knn1_acc\"] = float(\"nan\")\n",
    "    else:\n",
    "        m[\"silhouette\"] = m[\"db_index\"] = m[\"knn1_acc\"] = float(\"nan\")\n",
    "    return m\n",
    "\n",
    "# exact coloring and show-on-console\n",
    "def plot_umap_png(Z: np.ndarray, labels: np.ndarray, title: str, out_path: Path,\n",
    "                  display_dpi: int = 120, save_dpi: int = 400) -> None:\n",
    "    plt.figure(figsize=(7.5, 6.5), dpi=display_dpi)   # <-- console DPI\n",
    "    for lab, name in enumerate(CLASS_NAMES):\n",
    "        idx = (labels == lab)\n",
    "        if np.any(idx):\n",
    "            plt.scatter(Z[idx,0], Z[idx,1], s=10, alpha=0.85, label=name, c=[CMAP(lab)])\n",
    "    plt.legend(frameon=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "    plt.grid(alpha=0.15, linestyle=\"--\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(out_path), dpi=save_dpi, bbox_inches=\"tight\")  # <-- file DPI\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Main\n",
    "assert IMAGES_DIR.is_dir(), f\"Images dir not found: {IMAGES_DIR}\"\n",
    "assert CKPT_PATH.is_file(), f\"Checkpoint not found: {CKPT_PATH}\"\n",
    "df = load_headerless_csv(CSV_PATH)\n",
    "log.info(f\"[DATA] rows={len(df)} | PIPELINE={PIPELINE} | EXPECT={EXPECT}\")\n",
    "\n",
    "if PIPELINE == \"mm\":\n",
    "    ds = PILImageDataset(IMAGES_DIR, df, size=IMG_SIZE)\n",
    "elif PIPELINE == \"uni\":\n",
    "    ds = CV2ImageDataset(IMAGES_DIR, df, size=IMG_SIZE)\n",
    "else:\n",
    "    raise ValueError(\"PIPELINE must be 'mm' or 'uni'.\")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
    "\n",
    "model = build_image_head_from_ckpt(CKPT_PATH, n_classes=NUM_CLASSES, expect=EXPECT, use_post3x3=True)\n",
    "\n",
    "print(\"\\n================== MODEL (ImageHead\\n\")\n",
    "print(model)  # shows ImageHead(...) with img_enc/proj/mid/cls\n",
    "print(\"\\n======================================================================\\n\")\n",
    "\n",
    "# 1) Collect embeddings (deepest pooling layer)\n",
    "emb, y = collect_embeddings(model, dl)\n",
    "log.info(f\"[FEATS] emb={emb.shape}, labels={y.shape}\")\n",
    "\n",
    "# 2) Grid search: save ALL combos @400 dpi + track BEST-by-silhouette\n",
    "best_sil = -float(\"inf\")\n",
    "best_tuple = None  # (nn, md, Z, met)\n",
    "\n",
    "for nn_ in UMAP_N_NEIGHBORS_GRID:\n",
    "    for md_ in UMAP_MIN_DIST_GRID:\n",
    "        Z   = run_umap(emb, n_neighbors=nn_, min_dist=md_, metric=UMAP_METRIC, random_state=UMAP_RANDOM_STATE)\n",
    "        met = score_2d(Z, y)\n",
    "        # Save each combination (400 dpi) and DISPLAY\n",
    "        combo_png = ALL_PNG_PATTERN.with_name(ALL_PNG_PATTERN.name.format(nn=nn_, md=md_))\n",
    "        combo_title = (f\"UMAP — nn={nn_}, md={md_:.2f} | \"\n",
    "                       f\"sil={met.get('silhouette', np.nan):.3f}, \"\n",
    "                       f\"db={met.get('db_index', np.nan):.3f}, \"\n",
    "                       f\"1NN={met.get('knn1_acc', np.nan):.3f}\")\n",
    "        plot_umap_png(Z, y, title=combo_title, out_path=combo_png, display_dpi=60, save_dpi=400)\n",
    "        # Update best-by-silhouette\n",
    "        sil_val = met.get(\"silhouette\", float(\"nan\"))\n",
    "        sil_val = sil_val if np.isfinite(sil_val) else -1.0\n",
    "        if sil_val > best_sil:\n",
    "            best_sil = sil_val\n",
    "            best_tuple = (nn_, md_, Z, met)\n",
    "\n",
    "# 3) Save BEST-by-silhouette with separate name (and DISPLAY)\n",
    "if best_tuple is None:\n",
    "    raise RuntimeError(\"No valid UMAP embedding found to select by silhouette.\")\n",
    "\n",
    "best_nn, best_md, best_Z, best_met = best_tuple\n",
    "best_title = (f\"UMAP — BEST by Silhouette | nn={best_nn}, md={best_md:.2f} | \"\n",
    "              f\"sil={best_met.get('silhouette', np.nan):.3f}, \"\n",
    "              f\"db={best_met.get('db_index', np.nan):.3f}, \"\n",
    "              f\"1NN={best_met.get('knn1_acc', np.nan):.3f}\")\n",
    "# plot_umap_png(best_Z, y, title=best_title, out_path=BEST_SIL_PNG, dpi=400)\n",
    "plot_umap_png(best_Z, y, title=best_title, out_path=BEST_SIL_PNG, display_dpi=60, save_dpi=400)\n",
    "log.info(f\"[UMAP] Saved BEST-by-silhouette: {BEST_SIL_PNG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2aac2-bf41-4f76-b36b-26844884adab",
   "metadata": {},
   "source": [
    "## END OF CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c564c5-5fe0-4de7-92ad-7d60e8485a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d7684-ab0f-4989-b60a-8445a3874b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_ultra_new)",
   "language": "python",
   "name": "pytorch_ultra_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
