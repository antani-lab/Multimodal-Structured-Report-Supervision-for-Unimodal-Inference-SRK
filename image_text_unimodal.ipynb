{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfa17a-41f9-45e4-b307-ea7b0689d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217e84a-1894-44ee-ab4c-043131d6f4d4",
   "metadata": {},
   "source": [
    "## image-only / text-only training + validation + test-inference stack\n",
    "\n",
    "Supports MOD.img and MOD.txt.\n",
    "\n",
    "Trains with class-balanced sampling for image data\n",
    "\n",
    "Saves the checkpoint named (best_<NETWORK>_<MODALITY>_val_loss.pt), selected by highest validation MCC.\n",
    "\n",
    "Writes best_*_val_loss_log.json that includes best_val_mcc, best_epoch, etc.\n",
    "\n",
    "Test inference recursively scans under cfg.out_dir for all model-aware checkpoints of the chosen stem, reads each run’s log to get best_val_mcc, selects the highest-MCC checkpoint (ties → newest file), prints why, and evaluates on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5188cc6-e26c-4f30-88f8-5c6e1cedfc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# =========================\n",
    "# Standard library imports\n",
    "# =========================\n",
    "import ast\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import dataclasses\n",
    "from collections import defaultdict\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# Third-party imports\n",
    "# =========================\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, WeightedRandomSampler\n",
    "from torchvision import transforms as T\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pytorch_grad_cam import (\n",
    "    AblationCAM,\n",
    "    EigenCAM,\n",
    "    EigenGradCAM,\n",
    "    GradCAM,\n",
    "    GradCAMPlusPlus,\n",
    "    HiResCAM,\n",
    "    LayerCAM,\n",
    "    ScoreCAM,\n",
    "    XGradCAM,\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Optional dependency\n",
    "try:\n",
    "    import timm\n",
    "    _HAS_TIMM = True\n",
    "except Exception:\n",
    "    timm = None\n",
    "    _HAS_TIMM = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd92204-b271-449c-93d2-7119887af892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set environmental variables\n",
    "\n",
    "print(torch.__version__)\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/gpfs/gsfs12/users/rajaramans2/.cache/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/gpfs/gsfs12/users/rajaramans2/.cache/huggingface/hub\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52879ba5-371b-4d6c-b605-af3319d00e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "class MOD(Enum):\n",
    "    img = auto()\n",
    "    txt = auto()\n",
    "\n",
    "class NETWORK(Enum):\n",
    "    # timm / torchvision families\n",
    "    dpn68 = auto()\n",
    "    coatnet0 = auto()\n",
    "    convnext_nano = auto()\n",
    "    hrnet32 = auto()\n",
    "    resnet18 = auto()\n",
    "    densenet121 = auto()\n",
    "    mobilenet_v2 = auto()\n",
    "    # NEW: VGG family\n",
    "    vgg11 = auto()\n",
    "    vgg13 = auto()\n",
    "    vgg16 = auto()\n",
    "    vgg19 = auto()\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    modality: MOD\n",
    "    network: NETWORK\n",
    "    n_classes: int = 2\n",
    "    hidden_dim: int = 256\n",
    "    epochs: int = 64\n",
    "    lr: float = 5e-5\n",
    "    weight_decay: float = 1e-4\n",
    "    img_size: int = 224\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 2\n",
    "    pin_memory: bool = True\n",
    "\n",
    "    # class names\n",
    "    class_names: Tuple[str, ...] = (\"normal\", \"tb\")\n",
    "\n",
    "    # data locations\n",
    "    dataset_root: str = \"/dataset\"\n",
    "    csv_train: Optional[str] = None\n",
    "    csv_valid: Optional[str] = None\n",
    "    csv_test:  Optional[str] = None\n",
    "    images_subdir: str = \"images\"\n",
    "    reports_subdir: str = \"reports\"\n",
    "    out_dir: str = \"/models\"\n",
    "\n",
    "    # text/HF (offline-first)\n",
    "    hf_offline: bool = True\n",
    "    hf_local_dir: Optional[str] = \"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\"\n",
    "    hf_text_model_name: str = \"microsoft/BiomedVLP-CXR-BERT-general\"\n",
    "    max_tokens: int = 512\n",
    "    hf_tokenizer_local_dir: Optional[str] = \"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.csv_train is None:\n",
    "            self.csv_train = os.path.join(self.dataset_root, \"label_train.csv\")\n",
    "        if self.csv_valid is None:\n",
    "            self.csv_valid = os.path.join(self.dataset_root, \"label_valid.csv\")\n",
    "        if self.csv_test is None:\n",
    "            self.csv_test = os.path.join(self.dataset_root, \"label_test.csv\")\n",
    "\n",
    "    @property\n",
    "    def images_dir(self) -> str:\n",
    "        return os.path.join(self.dataset_root, self.images_subdir)\n",
    "\n",
    "    @property\n",
    "    def reports_dir(self) -> str:\n",
    "        return os.path.join(self.dataset_root, self.reports_subdir)\n",
    "\n",
    "def ckpt_stem(cfg: RunConfig) -> str:\n",
    "    return f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "\n",
    "def ckpt_paths(cfg: RunConfig) -> Dict[str, str]:\n",
    "    stem = ckpt_stem(cfg)\n",
    "    base = cfg.out_dir\n",
    "    return {\n",
    "        \"best_pt\":         os.path.join(base, f\"best_{stem}_val_loss.pt\"),\n",
    "        \"curves_png\":      os.path.join(base, f\"best_{stem}_val_loss_curves.png\"),\n",
    "        \"curves_csv\":      os.path.join(base, f\"best_{stem}_val_loss_history.csv\"),\n",
    "        \"curves_png_full\": os.path.join(base, f\"best_{stem}_val_loss_curves_full.png\"),\n",
    "        \"curves_csv_full\": os.path.join(base, f\"best_{stem}_val_loss_history_full.csv\"),\n",
    "        \"log_json\":        os.path.join(base, f\"best_{stem}_val_loss_log.json\"),\n",
    "        \"mcc_png\":         os.path.join(base, f\"best_{stem}_val_mcc_curve.png\"),\n",
    "    }\n",
    "\n",
    "# running configuration\n",
    "cfg = RunConfig(\n",
    "    modality=MOD.img, # or MOD.txt\n",
    "    network=NETWORK.vgg11,\n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-4,\n",
    "    img_size=224,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset_root=\"/dataset\",\n",
    "    csv_train=\"/label_train.csv\",\n",
    "    csv_valid=\"/label_valid.csv\",\n",
    "    csv_test =\"/label_test.csv\",\n",
    "    images_subdir=\"images\",\n",
    "    reports_subdir=\"reports\",\n",
    "    out_dir=\"/models\",\n",
    "    class_names=(\"normal\",\"tb\"),\n",
    "    hf_text_model_name=\"microsoft/BiomedVLP-CXR-BERT-general\",\n",
    "    hf_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "    hf_offline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c67641-de94-4b12-b060-f4c88a39a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dataset & loaders (Albumentations for image)\n",
    "\n",
    "def _read_csv_two_cols_no_header(path: str) -> Tuple[List[str], List[int]]:\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{path} must have at least 2 columns (img,label)\")\n",
    "    imgs = df.iloc[:, 0].astype(str).tolist()\n",
    "    labs = df.iloc[:, 1].astype(int).tolist()\n",
    "    return imgs, labs\n",
    "\n",
    "def _abs_image_path(dataset_root: str, images_subdir: str, fname: str) -> Path:\n",
    "    return Path(dataset_root) / images_subdir / fname\n",
    "\n",
    "def _abs_report_path(dataset_root: str, reports_subdir: str, fname: str) -> Path:\n",
    "    # prefer <stem>.txt inside reports_subdir, else a direct filename\n",
    "    p = Path(dataset_root) / reports_subdir / (Path(fname).stem + \".txt\")\n",
    "    if p.exists(): return p\n",
    "    q = Path(dataset_root) / reports_subdir / fname\n",
    "    return q\n",
    "\n",
    "def _one_hot(idx: int, n_classes: int) -> torch.Tensor:\n",
    "    y = torch.zeros(n_classes, dtype=torch.float32)\n",
    "    if 0 <= idx < n_classes: y[idx] = 1.0\n",
    "    return y\n",
    "\n",
    "class CSVImageOrTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - For MOD.img: reads grayscale → RGB with Albumentations, Normalizes (ImageNet), returns {\"image\", \"y_onehot\", \"filename\"}\n",
    "    - For MOD.txt: reads report text and tokenizes, returns {\"text\": {\"input_ids\",\"attention_mask\"}, \"y_onehot\", \"filename\"}\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: RunConfig, which: str, modality: MOD,\n",
    "                 n_classes: int, tokenizer: Optional[AutoTokenizer] = None,\n",
    "                 max_len: int = 192) -> None:\n",
    "        assert modality in (MOD.img, MOD.txt), \"This dataset only supports image-only OR text-only.\"\n",
    "        self.cfg = cfg\n",
    "        self.modality = modality\n",
    "        self.n_classes = int(n_classes)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = int(max_len)\n",
    "        csv_path = {\"train\": cfg.csv_train, \"valid\": cfg.csv_valid, \"test\": cfg.csv_test}[which]\n",
    "        img_names, labels = _read_csv_two_cols_no_header(csv_path)\n",
    "\n",
    "        self.items: List[Tuple[Path, int, Optional[Path]]] = []\n",
    "        for fn, y in zip(img_names, labels):\n",
    "            ip = _abs_image_path(cfg.dataset_root, cfg.images_subdir, fn)\n",
    "            rp = _abs_report_path(cfg.dataset_root, cfg.reports_subdir, fn)\n",
    "            if modality == MOD.img and not ip.exists():\n",
    "                continue\n",
    "            if modality == MOD.txt and not rp.exists():\n",
    "                continue\n",
    "            self.items.append((ip, int(y), rp if rp.exists() else None))\n",
    "\n",
    "        size = int(cfg.img_size)\n",
    "        self.train_tfm = A.Compose([\n",
    "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "            A.Resize(size, size, interpolation=cv2.INTER_AREA),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.eval_tfm = A.Compose([\n",
    "            A.Resize(size, size, interpolation=cv2.INTER_AREA),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.tfm = self.train_tfm if which == \"train\" else self.eval_tfm\n",
    "        self.return_paths = (which == \"test\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def _read_img(self, p: Path) -> torch.Tensor:\n",
    "        img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            img = np.zeros((self.cfg.img_size, self.cfg.img_size), dtype=np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        img = self.tfm(image=img)[\"image\"]\n",
    "        return img\n",
    "\n",
    "    def _read_text(self, rp: Optional[Path]) -> Dict[str, torch.Tensor]:\n",
    "        if self.tokenizer is None:\n",
    "            raise RuntimeError(\"Tokenizer is None but MOD.txt was requested.\")\n",
    "        text = \"\"\n",
    "        if rp is not None:\n",
    "            try:\n",
    "                text = rp.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "            except Exception:\n",
    "                text = \"\"\n",
    "        if not text:\n",
    "            text = \"[EMPTY]\"\n",
    "        tok = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\"input_ids\": tok[\"input_ids\"][0], \"attention_mask\": tok[\"attention_mask\"][0]}\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        ip, y_idx, rp = self.items[idx]\n",
    "        y = _one_hot(y_idx, self.n_classes)\n",
    "        out: Dict[str, torch.Tensor] = {\"y_onehot\": y, \"filename\": ip.name}\n",
    "        if self.modality == MOD.img:\n",
    "            out[\"image\"] = self._read_img(ip)\n",
    "        else:  # MOD.txt\n",
    "            out[\"text\"]  = self._read_text(rp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a590f-7cb8-4a77-a581-a90326c6eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer, loader builders, imbalanced sampler\n",
    "\n",
    "# --- robust CSV reader (headerless, keeps first data row) ---\n",
    "def _read_split_csv_headerless(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a split CSV that may or may not have a header.\n",
    "    Returns DataFrame with columns ['img','label'] and integer labels.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
    "\n",
    "    # Always header=None so we never drop the first row\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None, encoding=\"utf-8-sig\")\n",
    "\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{os.path.basename(path)} must have ≥2 columns (filename, label).\")\n",
    "\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = [\"img\", \"label\"]\n",
    "\n",
    "    # Detect and drop a header row if present\n",
    "    def _is_hdr_token(s: str) -> bool:\n",
    "        s = (str(s) or \"\").strip().lower()\n",
    "        return s in {\"img\", \"image\", \"filename\", \"file\", \"path\", \"label\", \"class\", \"target\", \"y\"} or s.startswith(\"img\")\n",
    "\n",
    "    if _is_hdr_token(df.iloc[0, 0]) and _is_hdr_token(df.iloc[0, 1]):\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    df[\"img\"] = df[\"img\"].astype(str).str.strip()\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# compact per-split summary\n",
    "def _print_split_summary(cfg: \"RunConfig\", split: str, *, modality: \"MOD\") -> None:\n",
    "    csv_path = {\n",
    "        \"train\": getattr(cfg, \"csv_train\", None),\n",
    "        \"valid\": getattr(cfg, \"csv_valid\", None),\n",
    "        \"test\":  getattr(cfg, \"csv_test\",  None),\n",
    "    }[split]\n",
    "    if not csv_path:\n",
    "        print(f\"[{split}] CSV path not provided.\", flush=True)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = _read_split_csv_headerless(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[{split}] ERROR reading CSV: {e}\", flush=True)\n",
    "        return\n",
    "\n",
    "    total_csv = len(df)\n",
    "\n",
    "    # Build absolute paths and check existence depending on modality\n",
    "    img_dir = os.path.join(cfg.dataset_root, getattr(cfg, \"images_subdir\", \"images\"))\n",
    "    txt_dir = os.path.join(cfg.dataset_root, getattr(cfg, \"reports_subdir\", \"reports\"))\n",
    "\n",
    "    df[\"img_path\"] = df[\"img\"].apply(lambda x: os.path.join(img_dir, x))\n",
    "    df[\"txt_path\"] = df[\"img\"].apply(lambda x: os.path.join(txt_dir, os.path.splitext(x)[0] + \".txt\"))\n",
    "\n",
    "    if modality == MOD.img:\n",
    "        mask = df[\"img_path\"].apply(os.path.isfile)\n",
    "        kept = int(mask.sum()); missing_img = int((~mask).sum())\n",
    "        df_use = df[mask].copy()\n",
    "        mod_str = \"images only\"\n",
    "        miss_str = f\"Missing images={missing_img}\"\n",
    "    elif modality == MOD.txt:\n",
    "        mask = df[\"txt_path\"].apply(os.path.isfile)\n",
    "        kept = int(mask.sum()); missing_txt = int((~mask).sum())\n",
    "        df_use = df[mask].copy()\n",
    "        mod_str = \"texts only\"\n",
    "        miss_str = f\"Missing texts={missing_txt}\"\n",
    "    else:\n",
    "        mask = (df[\"img_path\"].apply(os.path.isfile)) & (df[\"txt_path\"].apply(os.path.isfile))\n",
    "        kept = int(mask.sum())\n",
    "        missing_img = int((~df[\"img_path\"].apply(os.path.isfile)).sum())\n",
    "        missing_txt = int((~df[\"txt_path\"].apply(os.path.isfile)).sum())\n",
    "        df_use = df[mask].copy()\n",
    "        mod_str = \"require image & text\"\n",
    "        miss_str = f\"Missing images={missing_img}; Missing texts={missing_txt}\"\n",
    "\n",
    "    # Class counts (in the usable subset)\n",
    "    cls_counts: Dict[int, int] = df_use[\"label\"].value_counts().sort_index().to_dict()\n",
    "    cls_str = \" \".join([f\"{k}={v}\" for k, v in cls_counts.items()]) if cls_counts else \"(none)\"\n",
    "\n",
    "    # Show a few filename→label pairs to verify mapping\n",
    "    head_n = min(5, kept)\n",
    "    if head_n > 0:\n",
    "        examples = \"\\n\".join([f\"      • {r['img']}  →  {int(r['label'])}\"\n",
    "                              for _, r in df_use.head(head_n).iterrows()])\n",
    "    else:\n",
    "        examples = \"(no usable rows)\"\n",
    "\n",
    "    print(f\"[{split}] CSV rows={total_csv} | kept={kept} ({mod_str}). {miss_str}.\", flush=True)\n",
    "    print(f\"[{split}] label counts: {cls_str}\", flush=True)\n",
    "    print(f\"[{split}] first {head_n} examples:\\n{examples}\\n\", flush=True)\n",
    "\n",
    "def build_tokenizer(cfg: \"RunConfig\") -> Optional[AutoTokenizer]:\n",
    "    if cfg.modality != MOD.txt:\n",
    "        return None\n",
    "    offline = bool(getattr(cfg, \"hf_offline\", True))\n",
    "    local_tok = getattr(cfg, \"hf_tokenizer_local_dir\", None)\n",
    "    local_mod = getattr(cfg, \"hf_local_dir\", None)\n",
    "    model_id = getattr(cfg, \"hf_text_model_name\", \"microsoft/BiomedVLP-CXR-BERT-general\")\n",
    "\n",
    "    def _try_dir(p):\n",
    "        if p and os.path.isdir(p):\n",
    "            try:\n",
    "                return AutoTokenizer.from_pretrained(p, local_files_only=True, use_fast=True)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    tok = _try_dir(local_tok) or _try_dir(local_mod)\n",
    "    if tok is not None:\n",
    "        return tok\n",
    "    try:\n",
    "        return AutoTokenizer.from_pretrained(model_id, local_files_only=True, use_fast=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if offline:\n",
    "        raise RuntimeError(\"Offline and no local tokenizer snapshot found.\")\n",
    "    return AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "class ImbalancedDatasetSampler(Sampler[int]):\n",
    "    def __init__(self, labels: torch.Tensor):\n",
    "        self.labels = labels.clone().cpu().long()\n",
    "        k = int(self.labels.max().item()) + 1 if self.labels.numel() > 0 else 2\n",
    "        counts = torch.bincount(self.labels, minlength=k).clamp_min(1)\n",
    "        weights = (1.0 / counts)[self.labels]\n",
    "        self.sample_weights = weights.double()\n",
    "        self.num_samples = len(self.labels)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx = torch.multinomial(self.sample_weights, num_samples=self.num_samples, replacement=True)\n",
    "        return iter(idx.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def make_loaders(cfg: \"RunConfig\") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Print summaries *before* constructing datasets\n",
    "    _print_split_summary(cfg, \"train\", modality=cfg.modality)\n",
    "    _print_split_summary(cfg, \"valid\", modality=cfg.modality)\n",
    "    _print_split_summary(cfg, \"test\",  modality=cfg.modality)\n",
    "\n",
    "    # Build tokenizer if needed\n",
    "    tok = build_tokenizer(cfg)\n",
    "\n",
    "    # Build datasets\n",
    "    ds_tr = CSVImageOrTextDataset(cfg, \"train\", cfg.modality, cfg.n_classes, \n",
    "                                  tokenizer=tok, max_len=int(cfg.max_tokens))\n",
    "    ds_va = CSVImageOrTextDataset(cfg, \"valid\", cfg.modality, cfg.n_classes, \n",
    "                                  tokenizer=tok, max_len=int(cfg.max_tokens))\n",
    "    ds_te = CSVImageOrTextDataset(cfg, \"test\",  cfg.modality, cfg.n_classes, \n",
    "                                  tokenizer=tok, max_len=int(cfg.max_tokens))\n",
    "\n",
    "    # Build loaders (class-balanced sampling for image branch)\n",
    "    if cfg.modality == MOD.img:\n",
    "        df_tr = _read_split_csv_headerless(cfg.csv_train)\n",
    "        labels_tr = torch.as_tensor(df_tr[\"label\"].values, dtype=torch.long)\n",
    "        sampler = ImbalancedDatasetSampler(labels_tr)\n",
    "        dl_tr = DataLoader(\n",
    "            ds_tr, batch_size=int(cfg.batch_size), sampler=sampler, shuffle=False,\n",
    "            num_workers=int(cfg.num_workers), pin_memory=bool(cfg.pin_memory),\n",
    "            drop_last=True, persistent_workers=bool(int(cfg.num_workers) > 0)\n",
    "        )\n",
    "    else:\n",
    "        dl_tr = DataLoader(\n",
    "            ds_tr, batch_size=int(cfg.batch_size), shuffle=True,\n",
    "            num_workers=int(cfg.num_workers), pin_memory=bool(cfg.pin_memory),\n",
    "            drop_last=False, persistent_workers=bool(int(cfg.num_workers) > 0)\n",
    "        )\n",
    "\n",
    "    dl_va = DataLoader(\n",
    "        ds_va, batch_size=int(cfg.batch_size), shuffle=False,\n",
    "        num_workers=int(cfg.num_workers), pin_memory=bool(cfg.pin_memory),\n",
    "        drop_last=False, persistent_workers=bool(int(cfg.num_workers) > 0)\n",
    "    )\n",
    "    dl_te = DataLoader(\n",
    "        ds_te, batch_size=int(cfg.batch_size), shuffle=False,\n",
    "        num_workers=int(cfg.num_workers), pin_memory=bool(cfg.pin_memory),\n",
    "        drop_last=False, persistent_workers=bool(int(cfg.num_workers) > 0)\n",
    "    )\n",
    "\n",
    "    # Also print batch counts to sanity check batch sizing\n",
    "    def _n_batches(n_items: int, bs: int, drop_last: bool) -> int:\n",
    "        if drop_last:\n",
    "            return max(0, n_items // max(1, bs))\n",
    "        return (n_items + max(1, bs) - 1) // max(1, bs)\n",
    "\n",
    "    try:\n",
    "        print(f\"[batches] train={_n_batches(len(ds_tr), int(cfg.batch_size), True)}, \"\n",
    "              f\"valid={_n_batches(len(ds_va), int(cfg.batch_size), False)}, \"\n",
    "              f\"test={_n_batches(len(ds_te), int(cfg.batch_size), False)}\",\n",
    "              flush=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dl_tr, dl_va, dl_te\n",
    "\n",
    "# Make sure these are imported/defined from the stack\n",
    "train_loader, valid_loader, test_loader = make_loaders(cfg)  # prints split details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec39d6e-b210-4690-b9fc-a03023dbe196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (Torch/Timm image backbones; optional post-3×3; Text encoder)\n",
    "\n",
    "# timm (optional import)\n",
    "try:\n",
    "    import timm\n",
    "    _HAS_TIMM = True\n",
    "except Exception:\n",
    "    timm = None\n",
    "    _HAS_TIMM = False\n",
    "\n",
    "def _net_name_like(net_obj) -> str:\n",
    "    \"\"\"\n",
    "    Robustly resolve a backbone name:\n",
    "      - NETWORK enum with `.name`\n",
    "      - or string\n",
    "      - or fallback to str(obj)\n",
    "    \"\"\"\n",
    "    if isinstance(net_obj, str):\n",
    "        return net_obj.lower()\n",
    "    name = getattr(net_obj, \"name\", None)\n",
    "    if name is not None:\n",
    "        return str(name).lower()\n",
    "    return str(net_obj).lower()\n",
    "\n",
    "# -------- timm IDs\n",
    "TIMM_NAME_MAP: Dict[str, str] = {\n",
    "    \"dpn68\":         \"dpn68.mx_in1k\",\n",
    "    \"coatnet0\":      \"coatnet_0_rw_224.sw_in1k\",\n",
    "    \"convnext_nano\": \"convnext_nano.in12k\",\n",
    "    \"hrnet32\":       \"hrnet_w32.ms_in1k\",\n",
    "}\n",
    "\n",
    "# -------- helpers --------\n",
    "def _ends_with_3x3(module: nn.Module) -> bool:\n",
    "    last_conv = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_conv = m\n",
    "    if last_conv is None:\n",
    "        return False\n",
    "    k = getattr(last_conv, \"kernel_size\", (0, 0))\n",
    "    if not isinstance(k, tuple):\n",
    "        k = (k, k)\n",
    "    return tuple(k) == (3, 3)\n",
    "\n",
    "def _insert_post3x3(ch: int) -> nn.Sequential:\n",
    "    # Conv(3×3) + BN + SiLU\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(ch),\n",
    "        nn.SiLU(inplace=True),\n",
    "    )\n",
    "\n",
    "def _resolve_hf_local_dir(local_dir: Optional[str]) -> Optional[str]:\n",
    "    if not local_dir or not os.path.isdir(local_dir):\n",
    "        return None\n",
    "    needed = {\"config.json\", \"pytorch_model.bin\", \"model.safetensors\", \"rust_model.ot\"}\n",
    "    files = set(os.listdir(local_dir))\n",
    "    if files & needed:\n",
    "        return local_dir\n",
    "    snaps = os.path.join(local_dir, \"snapshots\")\n",
    "    refs  = os.path.join(local_dir, \"refs\", \"main\")\n",
    "    if os.path.isdir(snaps):\n",
    "        if os.path.isfile(refs):\n",
    "            with open(refs, \"r\") as f:\n",
    "                commit = f.read().strip()\n",
    "            cand = os.path.join(snaps, commit)\n",
    "            if os.path.isdir(cand):\n",
    "                return cand\n",
    "        subdirs = [\n",
    "            os.path.join(snaps, d) for d in os.listdir(snaps)\n",
    "            if os.path.isdir(os.path.join(snaps, d))\n",
    "        ]\n",
    "        if subdirs:\n",
    "            subdirs.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return subdirs[0]\n",
    "    return None\n",
    "\n",
    "# Torchvision image backbones\n",
    "class TorchvisionBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18 / DenseNet121 / MobileNetV2\n",
    "    encoder (features) → [optional post-3×3] → GAP → Dropout → (B, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, which: \"NETWORK\", p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        # --- encoder selection\n",
    "        if which == NETWORK.resnet18:\n",
    "            try: m = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            except: m = tvm.resnet18(weights=None)\n",
    "            self.encoder = nn.Sequential(*(list(m.children())[:-2]))  # (B, C, H, W)\n",
    "            ch = m.fc.in_features\n",
    "        elif which == NETWORK.densenet121:\n",
    "            try: m = tvm.densenet121(weights=tvm.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "            except: m = tvm.densenet121(weights=None)\n",
    "            self.encoder = m.features  # (B, C, H, W)\n",
    "            ch = m.classifier.in_features\n",
    "        elif which == NETWORK.mobilenet_v2:\n",
    "            try: m = tvm.mobilenet_v2(weights=tvm.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "            except: m = tvm.mobilenet_v2(weights=None)\n",
    "            self.encoder = m.features  # (B, C, H, W)\n",
    "            ch = m.classifier[1].in_features\n",
    "        else:\n",
    "            raise ValueError(which)\n",
    "\n",
    "        # ---- RIGHT order: post-3×3 → GAP → Dropout\n",
    "        self.post3x3 = _insert_post3x3(ch) if not _ends_with_3x3(self.encoder) else nn.Identity()\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = ch\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.encoder(x)           # (B, C, H, W)\n",
    "        f = self.post3x3(f)           # (B, C, H, W)\n",
    "        f = self.gap(f).flatten(1)    # (B, C)\n",
    "        f = self.drop(f)              # (B, C)\n",
    "        return f\n",
    "\n",
    "class TorchvisionVGGBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG11/13/16/19 (BN)\n",
    "    encoder (features) → [optional post-3×3] → GAP → Dropout → (B, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, which: \"NETWORK\", p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        vgg_map = {\n",
    "            NETWORK.vgg11: (tvm.vgg11_bn,  getattr(tvm, \"VGG11_BN_Weights\", None)),\n",
    "            NETWORK.vgg13: (tvm.vgg13_bn,  getattr(tvm, \"VGG13_BN_Weights\", None)),\n",
    "            NETWORK.vgg16: (tvm.vgg16_bn,  getattr(tvm, \"VGG16_BN_Weights\", None)),\n",
    "            NETWORK.vgg19: (tvm.vgg19_bn,  getattr(tvm, \"VGG19_BN_Weights\", None)),\n",
    "        }\n",
    "        ctor, weights_enum = vgg_map[which]\n",
    "        try:\n",
    "            if weights_enum is not None:\n",
    "                weights = getattr(weights_enum, \"IMAGENET1K_V1\")\n",
    "                m = ctor(weights=weights)\n",
    "            else:\n",
    "                m = ctor(weights=None)\n",
    "        except Exception:\n",
    "            m = ctor(weights=None)\n",
    "\n",
    "        self.encoder = m.features  # (B, C, H, W)\n",
    "        ch = None\n",
    "        for mod in self.encoder.modules():\n",
    "            if isinstance(mod, nn.Conv2d):\n",
    "                ch = mod.out_channels\n",
    "        if ch is None:\n",
    "            raise RuntimeError(\"VGG out channels not found\")\n",
    "\n",
    "        self.post3x3 = _insert_post3x3(ch) if not _ends_with_3x3(self.encoder) else nn.Identity()\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = ch\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.encoder(x)\n",
    "        f = self.post3x3(f)\n",
    "        f = self.gap(f).flatten(1)\n",
    "        f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "# timm backbones\n",
    "def _maybe_create_timm(timm_name: str, img_size: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a timm model safely:\n",
    "      - Don’t pass img_size to models that don’t accept it (e.g., DPN).\n",
    "      - num_classes=0 + global_pool='' to expose raw features.\n",
    "    \"\"\"\n",
    "    if not _HAS_TIMM:\n",
    "        raise RuntimeError(\"timm is not available\")\n",
    "\n",
    "    base_kwargs = dict(num_classes=0, global_pool=\"\")\n",
    "\n",
    "    # Some models accept img_size, others don't — try with, then without.\n",
    "    try:\n",
    "        return timm.create_model(timm_name, pretrained=True, img_size=img_size, **base_kwargs)\n",
    "    except TypeError:\n",
    "        # Retry without img_size\n",
    "        try:\n",
    "            return timm.create_model(timm_name, pretrained=True, **base_kwargs)\n",
    "        except Exception:\n",
    "            # Fallback to pretrained=False\n",
    "            return timm.create_model(timm_name, pretrained=False, **base_kwargs)\n",
    "\n",
    "class TimmBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    timm forward_features → tensor\n",
    "      - If 4D: encoder → [optional post-3×3] → GAP → Dropout → (B, C)\n",
    "      - If 3D: token/CLS path → Dropout → (B, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, key_name: str, img_size: int, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        if key_name not in TIMM_NAME_MAP:\n",
    "            raise ValueError(f\"Unknown timm backbone key '{key_name}'. Options: {list(TIMM_NAME_MAP.keys())}\")\n",
    "        timm_name = TIMM_NAME_MAP[key_name]\n",
    "\n",
    "        self.m = _maybe_create_timm(timm_name, img_size=img_size)\n",
    "\n",
    "        # Probe feature shape to configure heads\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, img_size, img_size)\n",
    "            f = self.m.forward_features(dummy)\n",
    "\n",
    "        if f.ndim == 4:\n",
    "            ch = int(f.shape[1])\n",
    "            # RIGHT ORDER: post-3×3 → GAP → Dropout\n",
    "            needs_post = not _ends_with_3x3(self.m)\n",
    "            self.post3x3 = _insert_post3x3(ch) if needs_post else nn.Identity()\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.drop = nn.Dropout(p=p_drop)\n",
    "            self.kind = \"map4d\"\n",
    "            self.out_dim = ch\n",
    "        elif f.ndim == 3:\n",
    "            # ViT-like: CLS/token dim\n",
    "            self.post3x3 = nn.Identity()\n",
    "            self.gap = nn.Identity()\n",
    "            self.drop = nn.Dropout(p=p_drop)\n",
    "            self.kind = \"tokens\"\n",
    "            self.out_dim = int(f.shape[-1])\n",
    "        else:\n",
    "            raise RuntimeError(\"Unexpected feature shape from timm model\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.m.forward_features(x)\n",
    "        if self.kind == \"map4d\":\n",
    "            f = self.post3x3(f)          # (B, C, H, W)\n",
    "            f = self.gap(f).flatten(1)   # (B, C)\n",
    "            f = self.drop(f)             # (B, C)\n",
    "            return f\n",
    "        else:\n",
    "            # tokens / CLS at index 0\n",
    "            f = f[:, 0]                   # (B, C)\n",
    "            f = self.drop(f)\n",
    "            return f\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Text fine-tuning policy (copied semantically from multimodal)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class TextFTPolicy:\n",
    "    \"\"\"\n",
    "    Text encoder fine-tuning policy:\n",
    "      - authors_default: freeze embeddings + pooler, finetune last encoder block only\n",
    "      - freeze_all:      no text encoder params train\n",
    "      - last_n:          finetune last `last_n` encoder blocks\n",
    "      - train_all:       finetune all text encoder params\n",
    "    \"\"\"\n",
    "    mode: Literal[\"authors_default\", \"freeze_all\", \"last_n\", \"train_all\"] = \"authors_default\"\n",
    "    last_n: int = 1\n",
    "\n",
    "\n",
    "def configure_text_finetune(bert: nn.Module, policy: TextFTPolicy) -> None:\n",
    "    \"\"\"Set requires_grad on HF text encoder parameters according to policy.\"\"\"\n",
    "    def set_all(req: bool):\n",
    "        for p in bert.parameters():\n",
    "            p.requires_grad = req\n",
    "\n",
    "    if policy.mode == \"authors_default\":\n",
    "        # Freeze embeddings + pooler, finetune only final encoder layer\n",
    "        if hasattr(bert, \"embeddings\"):\n",
    "            for p in bert.embeddings.parameters():\n",
    "                p.requires_grad = False\n",
    "        if hasattr(bert, \"pooler\"):\n",
    "            for p in bert.pooler.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        enc_layers = getattr(getattr(bert, \"encoder\", None), \"layer\", [])\n",
    "        for i, layer in enumerate(enc_layers):\n",
    "            req = (i == len(enc_layers) - 1)\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = req\n",
    "        return\n",
    "\n",
    "    if policy.mode == \"freeze_all\":\n",
    "        set_all(False)\n",
    "        return\n",
    "\n",
    "    if policy.mode == \"train_all\":\n",
    "        set_all(True)\n",
    "        return\n",
    "\n",
    "    if policy.mode == \"last_n\":\n",
    "        # Freeze embeddings + pooler, finetune last `last_n` encoder layers\n",
    "        if hasattr(bert, \"embeddings\"):\n",
    "            for p in bert.embeddings.parameters():\n",
    "                p.requires_grad = False\n",
    "        if hasattr(bert, \"pooler\"):\n",
    "            for p in bert.pooler.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        enc = getattr(bert, \"encoder\", None)\n",
    "        total = len(enc.layer)\n",
    "        cutoff = max(0, total - int(policy.last_n))\n",
    "        for i, layer in enumerate(enc.layer):\n",
    "            req = (i >= cutoff)\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = req\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown TextFTPolicy.mode: {policy.mode}\")\n",
    "\n",
    "# Unimodal classifier (Img-only OR Txt-only)\n",
    "class ImgOrTxtClassifier(nn.Module):\n",
    "    \"\"\"   \n",
    "      - Image: backbone → [optional post-3×3] → GAP → Dropout → Linear → ReLU → Linear\n",
    "      - Text : HF encoder (pooled/CLS) → Dropout → Linear → ReLU → Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: \"RunConfig\", text_ft: Optional[TextFTPolicy] = None):\n",
    "        super().__init__()\n",
    "        self.modality = cfg.modality\n",
    "        self.n_classes = int(cfg.n_classes)\n",
    "        self.hidden = int(cfg.hidden_dim)\n",
    "        self.img_size = int(getattr(cfg, \"img_size\", 224))\n",
    "        if text_ft is None:\n",
    "            text_ft = TextFTPolicy(mode=\"freeze_all\")\n",
    "\n",
    "        # Image branch\n",
    "        self.img_enc = None\n",
    "        img_out = 0\n",
    "        if self.modality == MOD.img:\n",
    "            net_name = _net_name_like(cfg.network).lower()\n",
    "\n",
    "            if _HAS_TIMM and (net_name in TIMM_NAME_MAP):\n",
    "                self.img_enc = TimmBackbone(net_name, img_size=self.img_size, p_drop=0.3)\n",
    "            else:\n",
    "                if net_name in (\"resnet18\", \"densenet121\", \"mobilenet_v2\"):\n",
    "                    self.img_enc = TorchvisionBackbone(cfg.network, p_drop=0.3)\n",
    "                elif net_name in (\"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\"):\n",
    "                    self.img_enc = TorchvisionVGGBackbone(cfg.network, p_drop=0.3)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported backbone: {cfg.network} (name='{net_name}')\")\n",
    "\n",
    "            img_out = int(self.img_enc.out_dim)\n",
    "\n",
    "        # Text branch\n",
    "        self.txt_enc = None\n",
    "        txt_hid = 0\n",
    "        if self.modality == MOD.txt:\n",
    "            resolved = _resolve_hf_local_dir(getattr(cfg, \"hf_local_dir\", None))\n",
    "            offline = bool(getattr(cfg, \"hf_offline\", True))\n",
    "\n",
    "            if offline:\n",
    "                if not resolved:\n",
    "                    raise RuntimeError(\"Offline mode but no valid local HF snapshot found.\")\n",
    "                self.txt_enc = AutoModel.from_pretrained(resolved, local_files_only=True)\n",
    "            else:\n",
    "                model_id = getattr(cfg, \"hf_text_model_name\", \"microsoft/BiomedVLP-CXR-BERT-general\")\n",
    "                self.txt_enc = AutoModel.from_pretrained(resolved or model_id)\n",
    "\n",
    "            # Apply multimodal-style fine-tuning policy\n",
    "            configure_text_finetune(self.txt_enc, text_ft)\n",
    "\n",
    "            txt_hid = int(self.txt_enc.config.hidden_size)\n",
    "            self.txt_drop = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Shared head\n",
    "        feat_in = img_out if self.modality == MOD.img else txt_hid\n",
    "        self.proj = nn.Linear(feat_in, self.hidden)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.mid = nn.Linear(self.hidden, self.hidden)\n",
    "        self.cls = nn.Linear(self.hidden, self.n_classes)\n",
    "\n",
    "    # Image-only path\n",
    "    def forward_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.img_enc(x)                  # (B, C)\n",
    "        z = self.mid(self.act(self.proj(f)))\n",
    "        return self.cls(z)                   # logits\n",
    "\n",
    "    # Text-only path\n",
    "    def forward_text(self, t: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        out = self.txt_enc(input_ids=t[\"input_ids\"], attention_mask=t[\"attention_mask\"])\n",
    "        pooled = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state[:, 0]\n",
    "        pooled = self.txt_drop(pooled)\n",
    "        z = self.mid(self.act(self.proj(pooled)))\n",
    "        return self.cls(z)\n",
    "\n",
    "    def forward(self, image=None, text=None) -> torch.Tensor:\n",
    "        if self.modality == MOD.img:\n",
    "            return self.forward_image(image)\n",
    "        if self.modality == MOD.txt:\n",
    "            return self.forward_text(text)\n",
    "        raise ValueError(f\"ImgOrTxtClassifier supports only MOD.img or MOD.txt, got {self.modality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a03d0-95db-471f-8c77-eb423dbc3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss (CE only) for either image or text, and related metrics\n",
    "\n",
    "_ce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "def onehot_to_idx(y_onehot: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.argmax(y_onehot, dim=1)\n",
    "\n",
    "def compute_total_loss(logits: torch.Tensor, y_onehot: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    y_idx = onehot_to_idx(y_onehot)\n",
    "    ce = _ce(logits, y_idx)\n",
    "    return ce, {\"total\": float(ce.detach().item()), \"ce\": float(ce.detach().item())}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_dataloader_classifier(model: nn.Module, loader: DataLoader, modality: MOD) -> Dict[str, float]:\n",
    "    device = next(model.parameters()).device\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    for batch in loader:\n",
    "        if batch[\"y_onehot\"].numel() == 0: continue\n",
    "        if modality == MOD.img:\n",
    "            img = batch[\"image\"].to(device, non_blocking=True)\n",
    "            logits = model.forward_image(img)\n",
    "        else:\n",
    "            txt = {k: v.to(device, non_blocking=True) for k, v in batch[\"text\"].items()}\n",
    "            logits = model.forward_text(txt)\n",
    "        probs1 = F.softmax(logits.float(), dim=1)[:,1].detach().cpu().numpy()\n",
    "        preds  = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        y_idx  = torch.argmax(batch[\"y_onehot\"], dim=1).detach().cpu().numpy()\n",
    "        y_true.extend(y_idx.tolist()); y_pred.extend(preds.tolist()); y_prob.extend(probs1.tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "    TP = int(((y_true==1)&(y_pred==1)).sum())\n",
    "    FP = int(((y_true==0)&(y_pred==1)).sum())\n",
    "    FN = int(((y_true==1)&(y_pred==0)).sum())\n",
    "    TN = int(((y_true==0)&(y_pred==0)).sum())\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred) if y_true.size else 0.0\n",
    "    sens = recall_score(y_true, y_pred, pos_label=1) if y_true.size else 0.0\n",
    "    spec = TN / (TN + FP) if (TN + FP) else 0.0\n",
    "    prec = precision_score(y_true, y_pred, pos_label=1) if (TP + FP) else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) else 0.0\n",
    "    f1s = f1_score(y_true, y_pred, pos_label=1) if y_true.size else 0.0\n",
    "    mcc = matthews_corrcoef(y_true, y_pred) if y_true.size else 0.0\n",
    "    kappa = cohen_kappa_score(y_true, y_pred) if y_true.size else 0.0\n",
    "    try: roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError: roc_auc = float(\"nan\")\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "        \"balanced_accuracy\": float(bal_acc), \"sensitivity\": float(sens), \"specificity\": float(spec),\n",
    "        \"precision\": float(prec), \"NPV\": float(npv), \"F1_score\": float(f1s),\n",
    "        \"MCC\": float(mcc), \"Cohen_Kappa\": float(kappa), \"ROC_AUC\": float(roc_auc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebc79b-f544-4e8e-8da0-66bc9e7baa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop (save best by highest validation MCC)\n",
    "\n",
    "def _count_params(m: nn.Module) -> Tuple[int, int]:\n",
    "    tot = sum(p.numel() for p in m.parameters())\n",
    "    trn = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return tot, trn\n",
    "\n",
    "def _format_params(n: int) -> str:\n",
    "    if n >= 1e6: return f\"{n/1e6:.2f}M\"\n",
    "    if n >= 1e3: return f\"{n/1e3:.2f}k\"\n",
    "    return str(n)\n",
    "\n",
    "def _save_curves_png_csv(history: Dict[str, List[float]], out_png: str, out_csv: str) -> None:\n",
    "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "    df = pd.DataFrame(history); df.to_csv(out_csv, index=False)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    if \"train_total\" in df and \"val_total\" in df:\n",
    "        ax.plot(df[\"epoch\"], df[\"train_total\"], label=\"train_total\", linewidth=2)\n",
    "        ax.plot(df[\"epoch\"], df[\"val_total\"],   label=\"val_total\",   linewidth=2, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss\"); ax.set_title(\"Training / Validation Loss\")\n",
    "    ax.legend(loc=\"upper right\"); ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout(); fig.savefig(out_png, dpi=200); plt.close(fig)\n",
    "\n",
    "def _save_mcc_curve(history: Dict[str, List[float]], out_png: str) -> None:\n",
    "    try:\n",
    "        epochs = history[\"epoch\"]; val_mcc = history[\"val_MCC\"]\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        ax.plot(epochs, val_mcc, linewidth=2)\n",
    "        ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Validation MCC\"); ax.set_title(\"Validation MCC\")\n",
    "        ax.grid(True, alpha=0.3); fig.tight_layout(); fig.savefig(out_png, dpi=200); plt.close(fig)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _print_model_overview(model: nn.Module, cfg: \"RunConfig\") -> None:\n",
    "    # Full architecture + parameter counts + useful heads info\n",
    "    print(\"\\n========== Model Architecture (FULL) ==========\", flush=True)\n",
    "    print(f\"Modality: {cfg.modality.name}\", flush=True)\n",
    "    print(f\"Backbone: {cfg.network.name}\", flush=True)\n",
    "    print(f\"Hidden dim: {cfg.hidden_dim} | Classes: {cfg.n_classes}\", flush=True)\n",
    "    if hasattr(model, \"img_enc\") and (model.img_enc is not None):\n",
    "        img_out = getattr(model.img_enc, \"out_dim\", None)\n",
    "        print(f\"[Image] out_dim={img_out}\", flush=True)\n",
    "        # identify if post3x3 exists and is active\n",
    "        if hasattr(model.img_enc, \"post3x3\"):\n",
    "            print(f\"[Image] post3x3: {model.img_enc.post3x3.__class__.__name__}\", flush=True)\n",
    "    if hasattr(model, \"txt_enc\") and (model.txt_enc is not None):\n",
    "        hid = getattr(getattr(model.txt_enc, \"config\", None), \"hidden_size\", None)\n",
    "        print(f\"[Text ] hidden_size={hid}\", flush=True)\n",
    "\n",
    "    tot, trn = _count_params(model)\n",
    "    frz = tot - trn\n",
    "    print(f\"Parameters: total={_format_params(tot)}, trainable={_format_params(trn)}, frozen={_format_params(frz)}\", flush=True)\n",
    "    print(\"-------------- BEGIN torch.nn print(model) --------------\", flush=True)\n",
    "    print(model, flush=True)\n",
    "    print(\"--------------- END torch.nn print(model) ---------------\\n\", flush=True)\n",
    "\n",
    "@dataclass\n",
    "class TrainRuntimeCfg:\n",
    "    amp: bool = True\n",
    "    patience: int = 10\n",
    "    grad_clip_norm: Optional[float] = 1.0\n",
    "    amp_dtype: torch.dtype = torch.float16\n",
    "    log_every: int = 1\n",
    "\n",
    "def run_epoch(model: nn.Module, loader: DataLoader, cfg: \"RunConfig\",\n",
    "              *, train: bool, optim: Optional[torch.optim.Optimizer],\n",
    "              trcfg: TrainRuntimeCfg) -> Dict[str, float]:\n",
    "    device = next(model.parameters()).device\n",
    "    use_amp = trcfg.amp and (device.type == \"cuda\")\n",
    "    model.train(mode=train)\n",
    "    sums = {\"total\": 0.0, \"ce\": 0.0}\n",
    "    n_samples = 0\n",
    "    # scaler selection for torch versions\n",
    "    scaler = (torch.amp.GradScaler(\"cuda\") if (train and use_amp and hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"))\n",
    "              else (torch.cuda.amp.GradScaler() if (train and use_amp and hasattr(torch.cuda, \"amp\")) else None))\n",
    "    for batch in loader:\n",
    "        if batch[\"y_onehot\"].numel() == 0: continue\n",
    "        bsz = batch[\"y_onehot\"].size(0); n_samples += bsz\n",
    "        if cfg.modality == MOD.img:\n",
    "            x = batch[\"image\"].to(device, non_blocking=True)\n",
    "            target = batch[\"y_onehot\"].to(device, non_blocking=True)\n",
    "            autocast_ctx = (torch.autocast(\"cuda\", dtype=trcfg.amp_dtype) if use_amp else nullcontext())\n",
    "            with torch.set_grad_enabled(train), autocast_ctx:\n",
    "                logits = model.forward_image(x)\n",
    "                loss, scalars = compute_total_loss(logits, target)\n",
    "        else:\n",
    "            t = {k: v.to(device, non_blocking=True) for k, v in batch[\"text\"].items()}\n",
    "            target = batch[\"y_onehot\"].to(device, non_blocking=True)\n",
    "            autocast_ctx = (torch.autocast(\"cuda\", dtype=trcfg.amp_dtype) if use_amp else nullcontext())\n",
    "            with torch.set_grad_enabled(train), autocast_ctx:\n",
    "                logits = model.forward_text(t)\n",
    "                loss, scalars = compute_total_loss(logits, target)\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                if trcfg.grad_clip_norm is not None:\n",
    "                    scaler.unscale_(optim); nn.utils.clip_grad_norm_(model.parameters(), trcfg.grad_clip_norm)\n",
    "                scaler.step(optim); scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if trcfg.grad_clip_norm is not None:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), trcfg.grad_clip_norm)\n",
    "                optim.step()\n",
    "        for k in sums.keys():\n",
    "            sums[k] += scalars.get(k, 0.0) * bsz\n",
    "    eps = 1e-12\n",
    "    return {k: (v / max(n_samples, eps)) for k, v in sums.items()}\n",
    "\n",
    "def fit_img_or_txt_with_best_mcc_checkpoint(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    cfg: \"RunConfig\",\n",
    "    trcfg: TrainRuntimeCfg = TrainRuntimeCfg(),\n",
    ") -> Dict[str, object]:\n",
    "\n",
    "    assert cfg.modality in (MOD.img, MOD.txt), \"Training here supports image-only or text-only.\"\n",
    "\n",
    "    # Print full summary **before** training begins\n",
    "    _print_model_overview(model, cfg)\n",
    "\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    paths = ckpt_paths(cfg)\n",
    "\n",
    "    # Backward-compat fallback for mcc_png if missing in ckpt_paths\n",
    "    if \"mcc_png\" not in paths:\n",
    "        stem = f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "        paths[\"mcc_png\"] = os.path.join(cfg.out_dir, f\"best_{stem}_val_loss_mcc.png\")\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    history = {\"epoch\": [], \"train_total\": [], \"val_total\": [], \"val_MCC\": []}\n",
    "\n",
    "    best_mcc = -1.0\n",
    "    best_epoch = 0\n",
    "    bad_epochs = 0\n",
    "    improvements = 0\n",
    "    stopped_early = False\n",
    "    last_va_total = None\n",
    "    last_va_mcc = None\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        tr = run_epoch(model, train_loader, cfg, train=True,  optim=optim, trcfg=trcfg)\n",
    "        va = run_epoch(model, valid_loader, cfg, train=False, optim=None,  trcfg=trcfg)\n",
    "        last_va_total = va[\"total\"]\n",
    "\n",
    "        # compute validation metrics → MCC\n",
    "        val_metrics = evaluate_dataloader_classifier(model, valid_loader, cfg.modality)\n",
    "        last_va_mcc = val_metrics[\"MCC\"]\n",
    "\n",
    "        best_str = (\"-∞\" if best_mcc < 0 else f\"{best_mcc:.4f}\")\n",
    "        streak_str = f\"{bad_epochs}/{trcfg.patience}\"\n",
    "        print(f\"[{ep:03d}/{cfg.epochs} | best_val_MCC={best_str} | no_improve={streak_str}] \"\n",
    "              f\"Train: total={tr['total']:.4f}  |  Valid: total={va['total']:.4f}  MCC={last_va_mcc:.4f}\")\n",
    "\n",
    "        history[\"epoch\"].append(ep)\n",
    "        history[\"train_total\"].append(tr[\"total\"]); history[\"val_total\"].append(va[\"total\"]); history[\"val_MCC\"].append(last_va_mcc)\n",
    "\n",
    "        # selection criterion: **maximize validation MCC**\n",
    "        if last_va_mcc > best_mcc + 1e-8:\n",
    "            print(f\"  ↑ val_MCC improved: {best_mcc:.6f} → {last_va_mcc:.6f}  (saving checkpoint; reset no_improve=0)\")\n",
    "            best_mcc = last_va_mcc; best_epoch = ep; bad_epochs = 0; improvements += 1\n",
    "            torch.save(model.state_dict(), paths[\"best_pt\"])\n",
    "            print(f\"  ↳ saved: {paths['best_pt']}\")\n",
    "            with open(paths[\"log_json\"], \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"best_val_mcc\": float(best_mcc),\n",
    "                    \"best_epoch\": int(best_epoch),\n",
    "                    \"epochs_run\": int(ep)\n",
    "                }, f, indent=2)\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            print(f\"  ↳ no MCC improvement (no_improve={bad_epochs}/{trcfg.patience})\")\n",
    "            if bad_epochs >= trcfg.patience:\n",
    "                print(f\"EARLY STOP: validation MCC did not improve for {bad_epochs} consecutive epochs \"\n",
    "                      f\"(patience={trcfg.patience}). Stopping at epoch {ep}.\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "    # Curves\n",
    "    _save_curves_png_csv(history, paths[\"curves_png\"], paths[\"curves_csv\"])\n",
    "    _save_curves_png_csv(history, paths[\"curves_png_full\"], paths[\"curves_csv_full\"])\n",
    "    _save_mcc_curve(history, paths[\"mcc_png\"])\n",
    "\n",
    "    # Restore best checkpoint\n",
    "    try:\n",
    "        state = torch.load(paths[\"best_pt\"], map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=True)\n",
    "        print(f\"Restored best (by MCC) from {paths['best_pt']}  |  best_epoch={best_epoch}  best_val_MCC={best_mcc:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not reload best checkpoint ({e}).\")\n",
    "\n",
    "    return {\n",
    "        \"best_val_MCC\": float(best_mcc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"improvements\": int(improvements),\n",
    "        \"early_stopped\": bool(stopped_early),\n",
    "        \"last_epoch_val_total\": float(last_va_total) if last_va_total is not None else None,\n",
    "        \"last_epoch_val_MCC\": float(last_va_mcc) if last_va_mcc is not None else None,\n",
    "        \"paths\": paths,\n",
    "        \"history\": history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e686610-7421-4f20-bc54-87a8d4857f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING RUN (pick MOD.img OR MOD.txt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Choose one modality: MOD.img or MOD.txt\n",
    "    cfg = RunConfig(\n",
    "        modality=MOD.img,    # <-- change to MOD.txt for text-only\n",
    "        network=NETWORK.vgg11, # choose backbone (dpn68 / coatnet0, etc)\n",
    "        n_classes=2,\n",
    "        hidden_dim=256,\n",
    "        epochs=64,\n",
    "        lr=5e-5,\n",
    "        weight_decay=1e-4,\n",
    "        img_size=224,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        dataset_root=\"/dataset\",\n",
    "        csv_train=\"/label_train.csv\",\n",
    "        csv_valid=\"/label_valid.csv\",\n",
    "        csv_test =\"/label_test.csv\",\n",
    "        images_subdir=\"images\",\n",
    "        reports_subdir=\"reports\",\n",
    "        out_dir=\"/models\",\n",
    "        class_names=(\"normal\",\"tb\"),\n",
    "        hf_offline=True,\n",
    "        hf_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "        hf_tokenizer_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "    )\n",
    "\n",
    "    # Build loaders\n",
    "    train_loader, valid_loader, test_loader = make_loaders(cfg)\n",
    "\n",
    "    # Device + model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImgOrTxtClassifier(cfg).to(device)\n",
    "\n",
    "    # Print model summary BEFORE training (full architecture + param counts)\n",
    "    _print_model_overview(model, cfg)\n",
    "\n",
    "    # Train (select best by validation MCC; filename unchanged)\n",
    "    trcfg = TrainRuntimeCfg(amp=True, patience=10, grad_clip_norm=1.0, amp_dtype=torch.float16)\n",
    "    out = fit_img_or_txt_with_best_mcc_checkpoint(model, train_loader, valid_loader, cfg, trcfg=trcfg)\n",
    "\n",
    "    # Evaluate best on VALID and TEST\n",
    "    valid_metrics = evaluate_dataloader_classifier(model, valid_loader, cfg.modality)\n",
    "    test_metrics = evaluate_dataloader_classifier(model, test_loader, cfg.modality)\n",
    "\n",
    "    print(\"\\n[BEST CHECKPOINT (by MCC)]\", flush=True)\n",
    "    print(f\"checkpoint : {out['paths']['best_pt']}\", flush=True)\n",
    "    print(f\"validation MCC : {valid_metrics['MCC']:.6f}\", flush=True)\n",
    "    print(f\"test MCC  : {test_metrics['MCC']:.6f}\", flush=True)\n",
    "    print(f\"valid metrics  : {valid_metrics}\", flush=True)\n",
    "    print(f\"test metrics  : {test_metrics}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722f96f-f97f-4af2-9e9d-132937123c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference (reuse training loaders; pick highest val-MCC)\n",
    "\n",
    "# ---------- model-aware checkpoint naming ----------\n",
    "def _ckpt_stem(cfg: \"RunConfig\") -> str:\n",
    "    # e.g., \"vgg16_img\"\n",
    "    return f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "\n",
    "def _ckpt_paths(cfg: \"RunConfig\") -> Dict[str, str]:\n",
    "    stem = _ckpt_stem(cfg)\n",
    "    base = cfg.out_dir\n",
    "    return {\n",
    "        \"best_pt\":         os.path.join(base, f\"best_{stem}_val_loss.pt\"),\n",
    "        \"curves_png\":      os.path.join(base, f\"best_{stem}_val_loss_curves.png\"),\n",
    "        \"curves_csv\":      os.path.join(base, f\"best_{stem}_val_loss_history.csv\"),\n",
    "        \"curves_png_full\": os.path.join(base, f\"best_{stem}_val_loss_curves_full.png\"),\n",
    "        \"curves_csv_full\": os.path.join(base, f\"best_{stem}_val_loss_history_full.csv\"),\n",
    "        \"log_json\":        os.path.join(base, f\"best_{stem}_val_loss_log.json\"),\n",
    "    }\n",
    "\n",
    "# robust best-MCC reader\n",
    "def _coerce_float(x: Any) -> Optional[float]:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _read_best_mcc_from_log(log_json_path: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Robust to capitalization and placements:\n",
    "      - best_val_mcc / best_val_MCC / val_mcc / val_MCC / best_mcc / best_MCC\n",
    "      - history: { val_mcc: [...], val_MCC: [...] } → max\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_json_path, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if not isinstance(meta, dict):\n",
    "        return None\n",
    "\n",
    "    for k in (\"best_val_mcc\", \"best_val_MCC\", \"val_mcc\", \"val_MCC\", \"best_mcc\", \"best_MCC\"):\n",
    "        v = _coerce_float(meta.get(k, None))\n",
    "        if v is not None:\n",
    "            return v\n",
    "\n",
    "    hist = meta.get(\"history\", None)\n",
    "    if isinstance(hist, dict):\n",
    "        for hk in (\"val_mcc\", \"val_MCC\"):\n",
    "            arr = hist.get(hk, None)\n",
    "            if isinstance(arr, list) and len(arr) > 0:\n",
    "                try:\n",
    "                    return float(np.nanmax(np.asarray(arr, dtype=float)))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "# scan out_dir and select best by MCC\n",
    "def scan_runs_and_select_best(cfg: \"RunConfig\") -> Dict[str, str]:\n",
    "    stem = _ckpt_stem(cfg)\n",
    "    pattern = os.path.join(cfg.out_dir, \"**\", f\"best_{stem}_val_loss.pt\")\n",
    "    paths = glob.glob(pattern, recursive=True)\n",
    "    rows = []\n",
    "    for pt in paths:\n",
    "        subdir = os.path.dirname(pt)\n",
    "        log_json = os.path.join(subdir, f\"best_{stem}_val_loss_log.json\")\n",
    "        mtime = os.path.getmtime(pt)\n",
    "        mcc = _read_best_mcc_from_log(log_json) if os.path.isfile(log_json) else None\n",
    "        rows.append({\n",
    "            \"ckpt_path\": pt,\n",
    "            \"log_json\": log_json if os.path.isfile(log_json) else None,\n",
    "            \"mtime\": mtime,\n",
    "            \"val_mcc\": (None if mcc is None else float(mcc)),\n",
    "            \"subdir\": subdir\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"No checkpoints found under '{cfg.out_dir}' for stem 'best_{stem}_val_loss.pt'.\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"val_mcc\"] = pd.to_numeric(df[\"val_mcc\"], errors=\"coerce\")\n",
    "    df = df.sort_values(by=[\"val_mcc\", \"mtime\"], ascending=[False, False], na_position=\"last\")\n",
    "    return df.iloc[0].to_dict()\n",
    "\n",
    "# load image-only trained model; modify for loading text-only trained model\n",
    "@torch.no_grad()\n",
    "def load_img_model(cfg_mod: \"RunConfig\", ckpt_path: str) -> nn.Module:\n",
    "    assert cfg_mod.modality == MOD.img, \"This loader is for image-only models.\"\n",
    "    model = ImgOrTxtClassifier(cfg_mod).to(device).eval()\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    return model\n",
    "\n",
    "# print model name, arch, and parameter counts\n",
    "def _print_model_summary_for_test(model: nn.Module, cfg_mod: \"RunConfig\", ckpt_path: str) -> None:\n",
    "    # Detect backbone class name (timm or torchvision wrapper)\n",
    "    try:\n",
    "        if hasattr(model, \"img_enc\") and hasattr(model.img_enc, \"m\"):       # timm-backed wrapper\n",
    "            backbone_detected = model.img_enc.m.__class__.__name__\n",
    "        elif hasattr(model, \"img_enc\"):                                     # torchvision-backed wrapper\n",
    "            backbone_detected = model.img_enc.__class__.__name__\n",
    "        else:\n",
    "            backbone_detected = model.__class__.__name__\n",
    "    except Exception:\n",
    "        backbone_detected = model.__class__.__name__\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(\"\\n========== Model Loaded for Test ==========\", flush=True)\n",
    "    print(f\"Checkpoint           : {ckpt_path}\", flush=True)\n",
    "    print(f\"Backbone (cfg)       : {getattr(cfg_mod.network, 'name', str(cfg_mod.network))}\", flush=True)\n",
    "    print(f\"Backbone (detected)  : {backbone_detected}\", flush=True)\n",
    "    print(f\"Modality             : {getattr(cfg_mod.modality, 'name', str(cfg_mod.modality))}\", flush=True)\n",
    "    print(f\"Device               : {device.type}\", flush=True)\n",
    "    print(f\"Total params         : {total_params:,}\", flush=True)\n",
    "    print(f\"Trainable params     : {trainable_params:,}\", flush=True)\n",
    "    print(f\"Frozen params        : {frozen_params:,}\", flush=True)\n",
    "    print(\"--------------- Full Architecture ---------------\", flush=True)\n",
    "    print(model, flush=True)\n",
    "    print(\"=================================================\\n\", flush=True)\n",
    "\n",
    "\n",
    "# evaluate using the SAME function as training\n",
    "@torch.inference_mode()\n",
    "def evaluate_image_only_with_training_stack(\n",
    "    cfg_mod: \"RunConfig\",\n",
    "    ckpt_path: str,\n",
    "    save_subdir: str = \"test_infer_unimodal_shenzhen_new\",\n",
    ") -> Dict[str, float]:\n",
    "\n",
    "    assert cfg_mod.modality == MOD.img, \"Use MOD.img for image-only evaluation.\"\n",
    "    # Reuse loaders to get identical transforms & batching\n",
    "    _, _, test_loader = make_loaders(cfg_mod)\n",
    "\n",
    "    # Load the best-MCC checkpoint and immediately print summary\n",
    "    model = load_img_model(cfg_mod, ckpt_path)\n",
    "    model.eval()  # ensure eval mode\n",
    "    _print_model_summary_for_test(model, cfg_mod, ckpt_path)\n",
    "\n",
    "    # Evaluate with the exact same helper used during training\n",
    "    metrics = evaluate_dataloader_classifier(model, test_loader, cfg_mod.modality)\n",
    "\n",
    "    # Save metrics next to the checkpoint\n",
    "    save_dir = os.path.join(os.path.dirname(ckpt_path), save_subdir)\n",
    "    os.makedirs(save_dir, exist_ok=True)   \n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(save_dir, \"test_metrics.csv\"), index=False)\n",
    "\n",
    "    # APPEND: save per-sample softmax predictions\n",
    "    rows: list[dict] = []\n",
    "    for batch in test_loader:\n",
    "        # Strictly follow image-only evaluator: use \"image\" and \"y_onehot\"\n",
    "        if not isinstance(batch, dict) or (\"image\" not in batch) or (\"y_onehot\" not in batch):\n",
    "            continue\n",
    "        if batch[\"y_onehot\"].numel() == 0:\n",
    "            continue\n",
    "\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        logits = model.forward_image(x)  # image-only path\n",
    "        probs = F.softmax(logits.float(), dim=1).detach().cpu().numpy()  # (B,2)\n",
    "        y_idx = torch.argmax(batch[\"y_onehot\"], dim=1).detach().cpu().numpy().astype(int)\n",
    "        pred = probs.argmax(axis=1).astype(int)\n",
    "\n",
    "        # Filenames: prefer 'filename'; else basename of 'image_path'; else idx_i fallback\n",
    "        B = probs.shape[0]\n",
    "        # Gather names from potential keys without tensor truthiness\n",
    "        def _to_list(v):\n",
    "            if v is None: return None\n",
    "            if isinstance(v, (list, tuple, np.ndarray)):\n",
    "                return [os.path.basename(str(s)) for s in list(v)]\n",
    "            return [os.path.basename(str(v))] * B\n",
    "        names = None\n",
    "        if \"filename\" in batch:\n",
    "            names = _to_list(batch[\"filename\"])\n",
    "        if (names is None) and (\"image_path\" in batch):\n",
    "            names = _to_list(batch[\"image_path\"])\n",
    "        if (names is None) or (len(names) != B):\n",
    "            names = [f\"idx_{i}\" for i in range(B)]\n",
    "        for i in range(B):\n",
    "            rows.append({\n",
    "                \"img\": names[i],\n",
    "                \"true_label\": int(y_idx[i]),\n",
    "                \"prob_0\": float(probs[i, 0]),\n",
    "                \"prob_1\": float(probs[i, 1]),\n",
    "                \"predicted_label\": int(pred[i]),\n",
    "            })\n",
    "\n",
    "    out_csv = os.path.join(save_dir, \"softmax_preds.csv\")\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f\"[SOFTMAX] Saved per-sample probabilities to: {out_csv}\")\n",
    "    return metrics\n",
    "\n",
    "# runner\n",
    "if __name__ == \"__main__\":\n",
    "    # SAME modality/backbone/out_dir as image-only training run\n",
    "    cfg_eval = RunConfig(\n",
    "        modality=MOD.img,\n",
    "        network=NETWORK.vgg11,\n",
    "        n_classes=2,\n",
    "        hidden_dim=256,\n",
    "        img_size=224,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        dataset_root=\"/dataset\",\n",
    "        csv_test =\"/label_test.csv\",\n",
    "        images_subdir=\"images\",\n",
    "        out_dir=\"/models\",\n",
    "        hf_offline=True\n",
    "    )\n",
    "\n",
    "    # OPTIONAL: set this if you want to force a specific checkpoint\n",
    "    # MANUAL_CKPT = \"/path/to/your/best_model.pt\"\n",
    "\n",
    "    if MANUAL_CKPT is None:\n",
    "        # Scan runs and list them (for visibility)\n",
    "        def _fmt(x):\n",
    "            if x is None or (isinstance(x, float) and (math.isnan(x))): return \"—\"\n",
    "            return f\"{x:.4f}\" if isinstance(x, float) else str(x)\n",
    "\n",
    "        print(\"\\n================ Available runs for this stem ================\")\n",
    "        stem = _ckpt_stem(cfg_eval)\n",
    "        for pt in glob.glob(os.path.join(cfg_eval.out_dir, \"**\", f\"best_{stem}_val_loss.pt\"), recursive=True):\n",
    "            logp = os.path.join(os.path.dirname(pt), f\"best_{stem}_val_loss_log.json\")\n",
    "            mcc = _read_best_mcc_from_log(logp) if os.path.isfile(logp) else None\n",
    "            print(f\"• {pt}\\n    val_MCC={_fmt(mcc)}\")\n",
    "        print(\"==============================================================\\n\")\n",
    "\n",
    "        best_row = scan_runs_and_select_best(cfg_eval)\n",
    "        ckpt_path = best_row[\"ckpt_path\"]\n",
    "        print(\">>> Selecting checkpoint:\")\n",
    "        print(f\"    {ckpt_path}\")\n",
    "        reason = f\"highest validation MCC={_fmt(best_row['val_mcc'])}\"\n",
    "        if best_row['val_mcc'] is None or (isinstance(best_row['val_mcc'], float) and math.isnan(best_row['val_mcc'])):\n",
    "            reason = \"no MCC recorded; selecting most recent checkpoint\"\n",
    "        print(f\"    Because: {reason}\\n\")\n",
    "    else:\n",
    "        ckpt_path = MANUAL_CKPT\n",
    "        print(\">>> Using manual checkpoint:\")\n",
    "        print(f\"    {ckpt_path}\\n\")\n",
    "\n",
    "    # Evaluate on test set using the *same* evaluation helper as training\n",
    "    save_subdir = f\"test_infer_unimodal_tbx11k_external_new1_{_ckpt_stem(cfg_eval)}\"\n",
    "    res = evaluate_image_only_with_training_stack(cfg_eval, ckpt_path, save_subdir=save_subdir)\n",
    "\n",
    "    print(f\"\\n=== Test results (selected run) [{cfg_eval.network.name}/{cfg_eval.modality.name}] ===\")\n",
    "    for k, v in res.items():\n",
    "        print(f\"{k:>18s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d717fa8-3594-4de0-864a-85612f544703",
   "metadata": {},
   "source": [
    "### GRAD-CAM VISUALIZATION USING THE SHENZHEN TEST SET\n",
    "\n",
    "1. Scans all runs under cfg.out_dir, reads each run’s log to get validation MCC, and selects the highest MCC.\n",
    "\n",
    "2. Builds YOLO GT txts from lung masks (normalized to the original image size), then scales GT boxes to 1024×1024 for drawing.\n",
    "\n",
    "3. Runs Grad-CAM on the test images and saves: the base 1024 image, heatmaps (with red GT boxes), contours (with red GT boxes), and bboxes (blue model boxes + red GT boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4b0f9-ad9f-46bb-8bf6-3c4785a62525",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"gradcam-img-gt\")\n",
    "\n",
    "# Reproducibility\n",
    "def set_deterministic(seed: int = 1337) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # no-op on CPU\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_deterministic(1337)\n",
    "\n",
    "# User config (Unimodal)\n",
    "    modality=MOD.img,\n",
    "    network=NETWORK.vgg11,\n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-4,\n",
    "    img_size=224,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset_root=\"/dataset\",\n",
    "    csv_test =\"/dataset/label_test.csv\",\n",
    "    images_subdir=\"images\",\n",
    "    reports_subdir=\"reports\", # unused here\n",
    "    out_dir=\"/unimodal/models_img_vgg11\",\n",
    "    class_names=(\"normal\",\"tb\"),\n",
    "    hf_offline=True,\n",
    "    hf_local_dir=None,\n",
    ")\n",
    "\n",
    "# Original images (cropped) + masks (cropped) to derive GT from\n",
    "ORIG_DIR = os.path.join(cfg.dataset_root, \"shen_orig_crop\")\n",
    "MASK_DIR = os.path.join(cfg.dataset_root, \"shen_mask_crop\")\n",
    "YOLO_TXT_DIR = os.path.join(cfg.dataset_root, \"shen_mask_crop_yolo_from_mask\")  # will be created\n",
    "RESIZED_1024_DIR = os.path.join(cfg.dataset_root, \"images\")  # 1024×1024 viz base\n",
    "\n",
    "# Output folder name under the chosen run\n",
    "SAVE_ROOT_NAME = f\"gradcam_{cfg.network.name}_{cfg.modality.name}_internal_overlap\"\n",
    "\n",
    "# Grad-CAM + drawing params\n",
    "CAM_METHOD  = \"gradcam\" # \"gradcam\", \"gradcam++\", \"xgradcam\", ...\n",
    "HEATMAP_ALPHA = 0.5\n",
    "BIN_THR = 0.4         # for contour extraction\n",
    "\n",
    "# Colors / thickness (BGR)\n",
    "CONTOUR_COLOR = (0,0,255) # red for contours\n",
    "CONTOUR_THICK = 3\n",
    "BB_MODEL_COLOR = (255,0,0) # blue for model boxes\n",
    "BB_GT_COLOR = (0,0,255) # red  for GT boxes\n",
    "BB_THICK = 4\n",
    "\n",
    "TARGET_CLASS_IDX = 1 # \"TB\" class\n",
    "RESIZE_TO = 1024 # visualization side\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "AUTO_CLEAN = False  # set True to purge junk files (never touches valid images)\n",
    "\n",
    "# Path helpers & junk filtering\n",
    "JUNK_DIR_TOKENS = {\"__pycache__\", \".ipynb_checkpoints\", \".git\", \".svn\", \".DS_Store\"}\n",
    "JUNK_FILE_BASENAMES = {\"thumbs.db\", \"desktop.ini\"}\n",
    "_CHECKPOINT_RE = re.compile(r\"(?i)(?:^|[^A-Za-z0-9])checkpoint(?:$|[^A-Za-z0-9])\")\n",
    "_DOTFILE_RE    = re.compile(r\"^\\.\")\n",
    "\n",
    "def _is_img(path_or_name: str) -> bool:\n",
    "    return os.path.splitext(path_or_name)[1].lower() in IMG_EXTS\n",
    "\n",
    "def _basename_no_ext(path: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "def _is_junk_path(path: str) -> bool:\n",
    "    p = pathlib.Path(path)\n",
    "    for part in p.parts:\n",
    "        if part in JUNK_DIR_TOKENS or _DOTFILE_RE.search(part):\n",
    "            return True\n",
    "    base = p.name\n",
    "    if base.lower() in JUNK_FILE_BASENAMES:\n",
    "        return True\n",
    "    if _DOTFILE_RE.match(base) or _CHECKPOINT_RE.search(os.path.splitext(base)[0]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _list_images(dir_path: str) -> List[str]:\n",
    "    files: List[str] = []\n",
    "    for root, dirs, fns in os.walk(dir_path):\n",
    "        dirs[:] = [d for d in dirs if not _is_junk_path(os.path.join(root, d))]\n",
    "        for fn in fns:\n",
    "            fp = os.path.join(root, fn)\n",
    "            if _is_junk_path(fp): continue\n",
    "            if _is_img(fn): files.append(fp)\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def _ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _safe_remove(path: str) -> bool:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def clean_tree_of_junk(root_dir: str) -> Dict[str, int]:\n",
    "    removed = 0\n",
    "    skipped = 0\n",
    "    for r, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            fp = os.path.join(r, f)\n",
    "            junk = _is_junk_path(fp)\n",
    "            if not junk and _is_img(fp):\n",
    "                base_woext = os.path.splitext(os.path.basename(fp))[0]\n",
    "                if _CHECKPOINT_RE.search(base_woext):\n",
    "                    junk = True\n",
    "            if junk:\n",
    "                if _safe_remove(fp): removed += 1\n",
    "                else: skipped += 1\n",
    "    return {\"removed\": removed, \"skipped\": skipped}\n",
    "\n",
    "# Checkpoint naming & scanner (IMG ONLY)\n",
    "def _ckpt_stem(cfg_scan: \"RunConfig\") -> str:\n",
    "    return f\"{cfg_scan.network.name}_{cfg_scan.modality.name}\"  # e.g., vgg11_img\n",
    "\n",
    "def _read_val_mcc_from_log(log_json_path: str) -> Optional[float]:\n",
    "    try:\n",
    "        with open(log_json_path, \"r\") as f: meta = json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "    # common keys\n",
    "    for k in (\"best_val_mcc\", \"best_val_MCC\", \"val_mcc\", \"val_MCC\", \"best_mcc\", \"best_MCC\"):\n",
    "        v = meta.get(k, None)\n",
    "        if isinstance(v, (int, float)): return float(v)\n",
    "    # nested dicts\n",
    "    for nk in (\"valid_metrics\",\"validation_metrics\",\"val_metrics\",\"best_metrics\",\"best_valid_metrics\"):\n",
    "        d = meta.get(nk, None)\n",
    "        if isinstance(d, dict):\n",
    "            for cand in (\"MCC\",\"mcc\",\"val_MCC\",\"val_mcc\"):\n",
    "                v = d.get(cand, None)\n",
    "                if isinstance(v, (int, float)): return float(v)\n",
    "    # history → max\n",
    "    hist = meta.get(\"history\", None)\n",
    "    if isinstance(hist, dict):\n",
    "        for hk in (\"val_mcc\",\"val_MCC\"):\n",
    "            arr = hist.get(hk, None)\n",
    "            if isinstance(arr, list) and len(arr) > 0:\n",
    "                try: return float(np.nanmax(np.asarray(arr, dtype=float)))\n",
    "                except Exception: pass\n",
    "    return None\n",
    "\n",
    "def _guess_log_for_checkpoint(pt_path: str, stem: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Map:\n",
    "      best_{stem}_val_loss.pt                  -> best_{stem}_val_loss_log.json\n",
    "      best_{stem}_val_loss_dimension_256.pt    -> best_{stem}_val_loss_log_dimension_256.json\n",
    "    \"\"\"\n",
    "    subdir = os.path.dirname(pt_path)\n",
    "    cand1  = re.sub(r\"(_val_loss)(.*)\\.pt$\", r\"\\1_log\\2.json\", pt_path)\n",
    "    cand2  = os.path.join(subdir, f\"best_{stem}_val_loss_log.json\")\n",
    "    for c in (cand1, cand2):\n",
    "        if c and os.path.isfile(c): return c\n",
    "    return None\n",
    "\n",
    "def scan_img_checkpoints_by_mcc(cfg_scan: \"RunConfig\") -> pd.DataFrame:\n",
    "    assert cfg_scan.modality == MOD.img, \"scan_img_checkpoints_by_mcc expects MOD.img\"\n",
    "    stem = _ckpt_stem(cfg_scan)\n",
    "    patterns = [\n",
    "        os.path.join(cfg_scan.out_dir, \"**\", f\"best_{stem}_val_loss.pt\"), # legacy (preferred now)\n",
    "        os.path.join(cfg_scan.out_dir, \"**\", f\"best_{stem}_val_loss_dimension_*.pt\"), # older runs\n",
    "    ]\n",
    "    paths: List[str] = []\n",
    "    for pat in patterns:\n",
    "        paths.extend(glob.glob(pat, recursive=True))\n",
    "    paths = sorted(set(paths))\n",
    "\n",
    "    rows = []\n",
    "    for pt in paths:\n",
    "        subdir   = os.path.dirname(pt)\n",
    "        log_json = _guess_log_for_checkpoint(pt, stem)\n",
    "        val_mcc  = _read_val_mcc_from_log(log_json) if log_json else None\n",
    "        mtime    = os.path.getmtime(pt) if os.path.isfile(pt) else 0.0\n",
    "        rows.append({\n",
    "            \"val_mcc\": (None if val_mcc is None else float(val_mcc)),\n",
    "            \"ckpt_path\": pt, \"log_json\": log_json,\n",
    "            \"mtime\": mtime, \"subdir\": subdir\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No checkpoints found under '{cfg_scan.out_dir}'. Tried: {patterns}\"\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def select_best_by_mcc(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Highest non-negative MCC; ties → newest mtime.\n",
    "    If none have a valid MCC, fallback to newest mtime.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[\"val_mcc\"] = pd.to_numeric(df2[\"val_mcc\"], errors=\"coerce\")\n",
    "    ok = df2[df2[\"val_mcc\"].notna() & (df2[\"val_mcc\"] >= 0.0)].copy()\n",
    "    if len(ok) > 0:\n",
    "        ok = ok.sort_values(by=[\"val_mcc\",\"mtime\"], ascending=[False, False])\n",
    "        return ok.iloc[0]\n",
    "    return df2.sort_values(by=[\"mtime\"], ascending=[False]).iloc[0]\n",
    "\n",
    "# -------------------- Model utilities (IMG ONLY) -------------------- #\n",
    "def print_model_overview_for_cam(model: nn.Module, img_size: int = 224) -> None:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(\"Grad-CAM — Model Overview\")\n",
    "    log.info(f\"Device: {device.type}\")\n",
    "    log.info(f\"Input size (C,H,W): (3, {img_size}, {img_size})\")\n",
    "    log.info(f\"Total params: {total:,}\")\n",
    "    log.info(f\"Trainable params: {train:,}\")\n",
    "    log.info(f\"Frozen params: {total-train:,}\")\n",
    "\n",
    "def _kernel_tuple(m: nn.Conv2d) -> Tuple[int,int]:\n",
    "    k = m.kernel_size\n",
    "    return (k if isinstance(k, tuple) else (k, k))\n",
    "\n",
    "def _last_conv_kgt1(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    last_any = None; last_kgt1 = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_any = m\n",
    "            if max(_kernel_tuple(m)) > 1:\n",
    "                last_kgt1 = m\n",
    "    return last_kgt1 if last_kgt1 is not None else last_any\n",
    "\n",
    "def _vit_patch_embed_conv(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    target = None\n",
    "    for name, m in module.named_modules():\n",
    "        if isinstance(m, nn.Conv2d) and (\"patch_embed\" in name or \"patch\" in name):\n",
    "            target = m\n",
    "    return target\n",
    "\n",
    "def _qualname_of_module(root: nn.Module, target: nn.Module) -> str:\n",
    "    for n, m in root.named_modules():\n",
    "        if m is target:\n",
    "            return n or \"<root>\"\n",
    "    return \"<unknown>\"\n",
    "\n",
    "def select_target_layer_for_cam(model: nn.Module) -> nn.Conv2d:\n",
    "    enc = getattr(model, \"img_enc\", None)\n",
    "    if enc is not None:\n",
    "        p3 = getattr(enc, \"post3x3\", None)\n",
    "        if isinstance(p3, nn.Sequential):\n",
    "            for m in p3.modules():\n",
    "                if isinstance(m, nn.Conv2d) and _kernel_tuple(m) == (3,3):\n",
    "                    return m\n",
    "        elif isinstance(p3, nn.Conv2d) and _kernel_tuple(p3) == (3,3):\n",
    "            return p3\n",
    "        tgt = _last_conv_kgt1(enc)\n",
    "        if isinstance(tgt, nn.Conv2d): return tgt\n",
    "        vitp = _vit_patch_embed_conv(enc)\n",
    "        if isinstance(vitp, nn.Conv2d): return vitp\n",
    "    tgt = _last_conv_kgt1(model)\n",
    "    if isinstance(tgt, nn.Conv2d): return tgt\n",
    "    raise RuntimeError(\"No suitable Conv2d layer found for Grad-CAM.\")\n",
    "\n",
    "class CamImageWrapper(nn.Module):\n",
    "    def __init__(self, model_img_only: nn.Module):\n",
    "        super().__init__()\n",
    "        self.mm = model_img_only\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mm.forward_image(x)\n",
    "\n",
    "def build_preprocess(size: int) -> T.Compose:\n",
    "    return T.Compose([\n",
    "        T.ToPILImage(), T.Resize((size, size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_ckpt_model(cfg_load: \"RunConfig\", ckpt_path: str) -> \"ImgOrTxtClassifier\":\n",
    "    m = ImgOrTxtClassifier(cfg_load).to(device).eval()\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    return m\n",
    "\n",
    "def _init_cam(CAMClass, model, target_layers):\n",
    "    sig = inspect.signature(CAMClass.__init__)\n",
    "    if \"use_cuda\" in sig.parameters:\n",
    "        return CAMClass(model=model, target_layers=target_layers, use_cuda=(device.type == \"cuda\"))\n",
    "    return CAMClass(model=model, target_layers=target_layers)\n",
    "\n",
    "# CSV Reader\n",
    "def _is_hdr(s: str) -> bool:\n",
    "    s = (str(s) or \"\").strip().lower()\n",
    "    return s in {\"img\",\"image\",\"filename\",\"file\",\"path\",\"label\",\"class\",\"target\",\"y\"} or s.startswith(\"img\")\n",
    "\n",
    "def _read_label_csv(csv_path: str, tag: str) -> pd.DataFrame:\n",
    "    if not os.path.isfile(csv_path):\n",
    "        log.warning(f\"[CSV] Missing {tag} CSV at: {csv_path}\")\n",
    "        return pd.DataFrame(columns=[\"img\", \"label\"])\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, header=None, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{tag} CSV must have >= 2 columns (filename, label): {csv_path}\")\n",
    "    df = df.iloc[:, :2].copy(); df.columns = [\"img\", \"label\"]\n",
    "    if len(df) and _is_hdr(df.iloc[0,0]) and _is_hdr(df.iloc[0,1]):\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "    df[\"img\"]   = df[\"img\"].astype(str).map(lambda s: os.path.basename(s.strip()))\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"img\"].map(_is_img)].reset_index(drop=True)\n",
    "    log.info(f\"[CSV] {tag}: {len(df)} rows after clean → {csv_path}\")\n",
    "    return df\n",
    "\n",
    "def _read_splits(test_csv: str) -> pd.DataFrame:\n",
    "    dfe = _read_label_csv(test_csv,  \"test\")\n",
    "    before = len(dfe)\n",
    "    dfe = dfe.drop_duplicates(subset=[\"img\"], keep=\"first\").reset_index(drop=True)\n",
    "    after = len(dfe)\n",
    "    if after < before:\n",
    "        log.info(f\"[CSV] Dedup: {before} → {after} unique basenames\")\n",
    "    return dfe\n",
    "    \n",
    "# Build YOLO txts from grayscale masks\n",
    "def _compute_yolo_from_mask(mask_gray: np.ndarray, W: int, H: int,\n",
    "                            min_area_px: int = 4) -> List[Tuple[float,float,float,float]]:\n",
    "    if mask_gray.max() > 1:\n",
    "        _, binm = cv2.threshold(mask_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    else:\n",
    "        binm = (mask_gray > 0).astype(np.uint8) * 255\n",
    "    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "    binm = cv2.morphologyEx(binm, cv2.MORPH_OPEN,  k, iterations=1)\n",
    "    binm = cv2.morphologyEx(binm, cv2.MORPH_CLOSE, k, iterations=1)\n",
    "    cnts, _ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes: List[Tuple[float,float,float,float]] = []\n",
    "    for c in cnts:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if w*h < max(min_area_px, int(0.00005 * W * H)):\n",
    "            continue\n",
    "        cx = (x + w/2.0) / float(W)\n",
    "        cy = (y + h/2.0) / float(H)\n",
    "        ww =  w / float(W)\n",
    "        hh =  h / float(H)\n",
    "        cx = min(max(cx, 0.0), 1.0); cy = min(max(cy, 0.0), 1.0)\n",
    "        ww = min(max(ww, 0.0), 1.0); hh = min(max(hh, 0.0), 1.0)\n",
    "        if ww > 0 and hh > 0:\n",
    "            boxes.append((cx, cy, ww, hh))\n",
    "    return boxes\n",
    "\n",
    "def build_yolo_txts_from_masks(orig_dir: str, mask_dir: str, out_dir: str, strict_match: bool = True) -> None:\n",
    "    _ensure_dir(out_dir)\n",
    "    orig_files = _list_images(orig_dir)\n",
    "    mask_files = _list_images(mask_dir)\n",
    "    log.info(f\"[YOLO-from-mask] Candidates — orig={len(orig_files)} | mask={len(mask_files)} (after junk filtering)\")\n",
    "\n",
    "    def _to_base_map(paths: List[str], tag: str) -> Dict[str, str]:\n",
    "        m: Dict[str, str] = {}\n",
    "        dups: Dict[str, List[str]] = {}\n",
    "        for p in paths:\n",
    "            b = _basename_no_ext(p)\n",
    "            if b in m:\n",
    "                dups.setdefault(b, []).append(p)\n",
    "                continue\n",
    "            m[b] = p\n",
    "        if dups:\n",
    "            ex = [(k, [os.path.basename(x) for x in v[:2]]) for k, v in list(dups.items())[:3]]\n",
    "            log.warning(f\"[{tag}] {len(dups)} duplicate basenames detected (keeping first). Examples: {ex}\")\n",
    "        return m\n",
    "\n",
    "    orig_by_base: Dict[str, str] = { _basename_no_ext(p): os.path.basename(p) for p in orig_files }\n",
    "    mask_by_base: Dict[str, str] = _to_base_map(mask_files, tag=\"MASK\")\n",
    "\n",
    "    orig_bases = set(orig_by_base.keys())\n",
    "    mask_bases = set(mask_by_base.keys())\n",
    "    missing_in_mask = sorted(orig_bases - mask_bases)\n",
    "    missing_in_orig = sorted(mask_bases - orig_bases)\n",
    "\n",
    "    if missing_in_mask or missing_in_orig:\n",
    "        msg_parts = []\n",
    "        if missing_in_mask:\n",
    "            msg_parts.append(f\"Missing masks for {len(missing_in_mask)} images (e.g., {missing_in_mask[:5]})\")\n",
    "        if missing_in_orig:\n",
    "            msg_parts.append(f\"Extra masks without originals: {len(missing_in_orig)} (e.g., {missing_in_orig[:5]})\")\n",
    "        msg = \" | \".join(msg_parts)\n",
    "        if strict_match:\n",
    "            raise ValueError(f\"[YOLO-from-mask] Filename sets must match after junk-filtering. {msg}\")\n",
    "        else:\n",
    "            log.warning(f\"[YOLO-from-mask] {msg} — proceeding with intersection.\")\n",
    "\n",
    "    bases = sorted(orig_bases & mask_bases) if (missing_in_mask or missing_in_orig) else sorted(orig_bases)\n",
    "\n",
    "    n_written, n_empty = 0, 0\n",
    "    for b in bases:\n",
    "        img_name = orig_by_base[b]  # keep extension; write \"<name>.<ext>.txt\"\n",
    "        img_path = os.path.join(orig_dir, img_name)\n",
    "        mask_path = mask_by_base[b]\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            log.warning(f\"[YOLO-from-mask] Skipping (orig not readable): {img_path}\")\n",
    "            continue\n",
    "        H, W = img.shape[:2]\n",
    "\n",
    "        m = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if m is None:\n",
    "            log.warning(f\"[YOLO-from-mask] Skipping (mask not readable): {mask_path}\")\n",
    "            continue\n",
    "\n",
    "        boxes = _compute_yolo_from_mask(m, W, H)\n",
    "        out_txt = os.path.join(out_dir, img_name + \".txt\")\n",
    "        with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            if not boxes:\n",
    "                n_empty += 1\n",
    "            for (cx, cy, ww, hh) in boxes:\n",
    "                f.write(f\"0 {cx:.6f} {cy:.6f} {ww:.6f} {hh:.6f}\\n\")\n",
    "        n_written += 1\n",
    "\n",
    "    log.info(f\"[YOLO-from-mask] Wrote {n_written} .txt files to: {out_dir}  (empty={n_empty})\")\n",
    "\n",
    "# Phase B: Load-scaled GT boxes (to 1024)\n",
    "def _load_original_size(name: str) -> Tuple[np.ndarray, int, int]:\n",
    "    p = os.path.join(ORIG_DIR, name)\n",
    "    bgr = cv2.imread(p)\n",
    "    if bgr is None:\n",
    "        raise FileNotFoundError(f\"Original image not found: {p}\")\n",
    "    H, W = bgr.shape[:2]\n",
    "    return bgr, W, H\n",
    "\n",
    "def _load_resized_1024(name: str) -> Tuple[Optional[np.ndarray], bool]:\n",
    "    p = os.path.join(RESIZED_1024_DIR, name)\n",
    "    bgr = cv2.imread(p)\n",
    "    if bgr is None:\n",
    "        return None, False\n",
    "    return bgr, True\n",
    "\n",
    "def _ensure_1024_base_image(name: str) -> Tuple[np.ndarray, int, int, int, int]:\n",
    "    bgr_1024, ok = _load_resized_1024(name)\n",
    "    orig_bgr, orig_W, orig_H = _load_original_size(name)\n",
    "    if not ok:\n",
    "        base_bgr = cv2.resize(orig_bgr, (RESIZE_TO, RESIZE_TO), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        H1, W1 = bgr_1024.shape[:2]\n",
    "        base_bgr = bgr_1024 if (H1 == RESIZE_TO and W1 == RESIZE_TO) else cv2.resize(bgr_1024, (RESIZE_TO, RESIZE_TO), interpolation=cv2.INTER_AREA)\n",
    "    return base_bgr, RESIZE_TO, RESIZE_TO, orig_W, orig_H\n",
    "\n",
    "def _read_yolo_gt_scaled_to_1024(name: str, orig_W: int, orig_H: int) -> List[Tuple[int,int,int,int]]:\n",
    "    \"\"\"\n",
    "    Read YOLO txt created from masks (normalized to ORIGINAL WxH) and scale to 1024×1024 pixels.\n",
    "    The txt filename is \"<original_image_name>.<ext>.txt\".\n",
    "    \"\"\"\n",
    "    txt_path = os.path.join(YOLO_TXT_DIR, name + \".txt\")\n",
    "    if not os.path.isfile(txt_path):\n",
    "        return []\n",
    "    boxes = []\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            toks = re.split(r\"[,\\s]+\", line)\n",
    "            if len(toks) < 5: continue\n",
    "            try:\n",
    "                _ = int(float(toks[0]))  # class id (unused)\n",
    "                cx = float(toks[1]); cy = float(toks[2])\n",
    "                w  = float(toks[3]); h  = float(toks[4])\n",
    "            except Exception:\n",
    "                continue\n",
    "            x1 = (cx - w/2.0) * orig_W\n",
    "            y1 = (cy - h/2.0) * orig_H\n",
    "            x2 = (cx + w/2.0) * orig_W\n",
    "            y2 = (cy + h/2.0) * orig_H\n",
    "            sx = RESIZE_TO / float(orig_W)\n",
    "            sy = RESIZE_TO / float(orig_H)\n",
    "            X1 = int(max(0, min(RESIZE_TO-1, round(x1 * sx))))\n",
    "            Y1 = int(max(0, min(RESIZE_TO-1, round(y1 * sy))))\n",
    "            X2 = int(max(0, min(RESIZE_TO-1, round(x2 * sx))))\n",
    "            Y2 = int(max(0, min(RESIZE_TO-1, round(y2 * sy))))\n",
    "            if X2 > X1 and Y2 > Y1:\n",
    "                boxes.append((X1, Y1, X2, Y2))\n",
    "    return boxes\n",
    "\n",
    "# Main Grad-CAM (image-only; GT overlay)\n",
    "def run_gradcam_img_with_gt(\n",
    "    cfg_cam: \"RunConfig\",\n",
    "    ckpt_path: str,\n",
    "    dfe: pd.DataFrame,\n",
    "    save_root_overlap: str,\n",
    "    cam_method: str = CAM_METHOD,\n",
    "    heatmap_alpha: float = HEATMAP_ALPHA,\n",
    "    bin_thr: float = BIN_THR,\n",
    ") -> None:\n",
    "    assert os.path.isfile(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    os.makedirs(save_root_overlap, exist_ok=True)\n",
    "    out_dirs = {\n",
    "        \"images\":   os.path.join(save_root_overlap, \"images\"),\n",
    "        \"heatmaps\": os.path.join(save_root_overlap, \"heatmaps\"),\n",
    "        \"contours\": os.path.join(save_root_overlap, \"contours\"),\n",
    "        \"bboxes\":   os.path.join(save_root_overlap, \"bboxes\"),\n",
    "    }\n",
    "    for d in out_dirs.values():\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Load model & CAM\n",
    "    model_img = _load_ckpt_model(cfg_cam, ckpt_path)\n",
    "    print_model_overview_for_cam(model_img, img_size=int(getattr(cfg_cam, \"img_size\", 224)))\n",
    "    cam_model = CamImageWrapper(model_img).to(device).eval()\n",
    "    target_layer = select_target_layer_for_cam(model_img)\n",
    "    for p in target_layer.parameters():\n",
    "        p.requires_grad_(True)\n",
    "    tl_name = _qualname_of_module(model_img, target_layer)\n",
    "    log.info(f\"[CAM] Target layer: {tl_name}  kernel={getattr(target_layer, 'kernel_size', None)}\")\n",
    "\n",
    "    methods = {\n",
    "        \"gradcam\": GradCAM, \"gradcam++\": GradCAMPlusPlus, \"hirescam\": HiResCAM,\n",
    "        \"xgradcam\": XGradCAM, \"layercam\": LayerCAM, \"eigencam\": EigenCAM,\n",
    "        \"eigengradcam\": EigenGradCAM, \"scorecam\": ScoreCAM, \"ablationcam\": AblationCAM\n",
    "    }\n",
    "    CAMClass = methods[cam_method.lower()]\n",
    "    cam = _init_cam(CAMClass, model=cam_model, target_layers=[target_layer])\n",
    "    try: cam.batch_size = 1\n",
    "    except Exception: pass\n",
    "\n",
    "    preprocess = build_preprocess(int(getattr(cfg_cam, \"img_size\", 224)))\n",
    "\n",
    "    # Iterate over UNION of CSVs (all unique images)\n",
    "    for _, row in dfe.iterrows():\n",
    "        name = os.path.basename(str(row[\"img\"]).strip())\n",
    "        if not _is_img(name): continue\n",
    "\n",
    "        # Build 1024 base + get original size for GT scaling\n",
    "        try:\n",
    "            base_bgr, W_vis, H_vis, orig_W, orig_H = _ensure_1024_base_image(name)\n",
    "        except FileNotFoundError:\n",
    "            log.warning(f\"[CAM] Skipping (missing original and/or 1024 file): {name}\")\n",
    "            continue\n",
    "\n",
    "        # Model input from 1024 base\n",
    "        rgb = cv2.cvtColor(base_bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = preprocess(rgb).unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward Grad-CAM\n",
    "        cam_model.zero_grad(set_to_none=True)\n",
    "        with torch.enable_grad():\n",
    "            if cam_method.lower() == \"eigencam\":\n",
    "                cam_mask = cam(input_tensor=x)[0]\n",
    "            else:\n",
    "                cam_mask = cam(\n",
    "                    input_tensor=x,\n",
    "                    targets=[ClassifierOutputTarget(int(TARGET_CLASS_IDX))],\n",
    "                    aug_smooth=True, eigen_smooth=True\n",
    "                )[0]\n",
    "\n",
    "        # Normalize & upsample to 1024\n",
    "        mmin, mmax = float(np.min(cam_mask)), float(np.max(cam_mask))\n",
    "        cam_mask = (cam_mask - mmin) / (mmax - mmin + 1e-8)\n",
    "        mask_u8 = (np.clip(cv2.resize(cam_mask, (W_vis, H_vis), interpolation=cv2.INTER_NEAREST), 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        # Heatmap overlay\n",
    "        heat = cv2.applyColorMap(mask_u8, cv2.COLORMAP_HOT)\n",
    "        heat_overlay = cv2.addWeighted(heat, float(heatmap_alpha), base_bgr, 1.0 - float(heatmap_alpha), 0.0)\n",
    "\n",
    "        # Contours (model areas)\n",
    "        thr = int(255 * float(bin_thr))\n",
    "        _, binm = cv2.threshold(mask_u8, thr, 255, cv2.THRESH_BINARY)\n",
    "        contours,_ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Contour image (red)\n",
    "        cont_img = base_bgr.copy()\n",
    "        if len(contours) > 0:\n",
    "            cv2.drawContours(cont_img, contours, -1, CONTOUR_COLOR, CONTOUR_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # Model boxes (blue)\n",
    "        box_img = base_bgr.copy()\n",
    "        model_boxes: List[Tuple[int,int,int,int]] = []\n",
    "        for c in contours:\n",
    "            x0, y0, w, h = cv2.boundingRect(c)\n",
    "            x1, y1, x2, y2 = x0, y0, x0 + w, y0 + h\n",
    "            model_boxes.append((x1, y1, x2, y2))\n",
    "            cv2.rectangle(box_img, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # GT boxes scaled to 1024 (red)\n",
    "        gt_boxes_1024 = _read_yolo_gt_scaled_to_1024(name, orig_W, orig_H)\n",
    "\n",
    "        # Save base image\n",
    "        stem = os.path.splitext(name)[0]\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"images\"], f\"{stem}.png\"), base_bgr)\n",
    "\n",
    "        # Heatmap + GT\n",
    "        heat_ov = heat_overlay.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes_1024:\n",
    "            cv2.rectangle(heat_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"heatmaps\"], f\"{stem}__{cfg_cam.network.name}__{cam_method}.png\"), heat_ov)\n",
    "\n",
    "        # Contours + GT\n",
    "        cont_ov = cont_img.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes_1024:\n",
    "            cv2.rectangle(cont_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"contours\"], f\"{stem}__{cfg_cam.network.name}__{cam_method}.png\"), cont_ov)\n",
    "\n",
    "        # BBoxes view: model (blue) + GT (red)\n",
    "        box_ov = base_bgr.copy()\n",
    "        for (x1, y1, x2, y2) in model_boxes:\n",
    "            cv2.rectangle(box_ov, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes_1024:\n",
    "            cv2.rectangle(box_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"bboxes\"], f\"{stem}__{cfg_cam.network.name}__{cam_method}.png\"), box_ov)\n",
    "\n",
    "    # Cleanup\n",
    "    try:\n",
    "        if hasattr(cam, \"activations_and_grads\") and (cam.activations_and_grads is not None):\n",
    "            cam.activations_and_grads.release()\n",
    "    except Exception:\n",
    "        pass\n",
    "    del cam, cam_model, model_img\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    log.info(f\"[CAM] Saved to: {save_root_overlap}\")\n",
    "\n",
    "# Scan → Select best (by val-MCC) → Build GT → Run CAM \n",
    "# 1) Scan and list image-only candidates\n",
    "cfg_scan = cfg  # already MOD.img\n",
    "df_found = scan_img_checkpoints_by_mcc(cfg_scan)\n",
    "\n",
    "def _fmt(x):\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)): return \"—\"\n",
    "    return f\"{x:.4f}\" if isinstance(x, float) else str(x)\n",
    "\n",
    "# Only show non-negative, valid MCCs\n",
    "df_show = df_found.copy()\n",
    "df_show[\"val_mcc\"] = pd.to_numeric(df_show[\"val_mcc\"], errors=\"coerce\")\n",
    "df_show = df_show[df_show[\"val_mcc\"].notna() & (df_show[\"val_mcc\"] >= 0.0)]\n",
    "\n",
    "print(\"\\n================ Available image-only checkpoints (sorted by folder) ================\")\n",
    "if len(df_show) == 0:\n",
    "    print(\"• (no checkpoints with a valid non-negative MCC found; selection will fallback to newest mtime)\")\n",
    "else:\n",
    "    for _, r in df_show.sort_values(\"subdir\").iterrows():\n",
    "        print(f\"• {r['subdir']}\")\n",
    "        print(f\"    val_MCC={_fmt(float(r['val_mcc']))}\")\n",
    "        print(f\"    ckpt={r['ckpt_path']}\")\n",
    "        if r['log_json']: print(f\"    log ={r['log_json']}\")\n",
    "print(\"======================================================================================\\n\")\n",
    "\n",
    "row_best = select_best_by_mcc(df_found)\n",
    "best_ckpt = row_best[\"ckpt_path\"]\n",
    "best_subdir = row_best[\"subdir\"]\n",
    "best_mcc = row_best[\"val_mcc\"]\n",
    "\n",
    "reason = f\"highest validation MCC={_fmt(best_mcc)}\"\n",
    "if best_mcc is None or (isinstance(best_mcc, float) and math.isnan(best_mcc)):\n",
    "    reason = \"no MCC recorded; selecting most recent checkpoint\"\n",
    "\n",
    "print(\">>> Selecting checkpoint:\")\n",
    "print(f\" {best_ckpt}\")\n",
    "print(f\" Because: {reason}\\n\")\n",
    "\n",
    "# 2) Build YOLO GT txts from masks (once)\n",
    "log.info(\"================ Precompute YOLO from Masks ================\")\n",
    "log.info(f\"Original dir : {ORIG_DIR}\")\n",
    "log.info(f\"Mask dir     : {MASK_DIR}\")\n",
    "log.info(f\"YOLO out dir : {YOLO_TXT_DIR}\")\n",
    "if AUTO_CLEAN:\n",
    "    stats1 = clean_tree_of_junk(ORIG_DIR)\n",
    "    stats2 = clean_tree_of_junk(MASK_DIR)\n",
    "    log.info(f\"[Clean ORIG] removed={stats1['removed']} skipped={stats1['skipped']}\")\n",
    "    log.info(f\"[Clean MASK] removed={stats2['removed']} skipped={stats2['skipped']}\")\n",
    "\n",
    "build_yolo_txts_from_masks(ORIG_DIR, MASK_DIR, YOLO_TXT_DIR, strict_match=True)\n",
    "\n",
    "# 3) Read all splits for CAM\n",
    "dfe = _read_splits(cfg.csv_test)\n",
    "log.info(f\"[CSV] test: {len(dfe)} unique images\")\n",
    "\n",
    "# 4) Run Grad-CAM with GT overlay; outputs beside the chosen run\n",
    "save_root_overlap = os.path.join(best_subdir, SAVE_ROOT_NAME)\n",
    "run_gradcam_img_with_gt(\n",
    "    cfg_cam=cfg,\n",
    "    ckpt_path=best_ckpt,\n",
    "    dfe=dfe,\n",
    "    save_root_overlap=save_root_overlap,\n",
    "    cam_method=CAM_METHOD,\n",
    "    heatmap_alpha=HEATMAP_ALPHA,\n",
    "    bin_thr=BIN_THR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e124452-b2fd-40e1-99ff-883aed974ec3",
   "metadata": {},
   "source": [
    "### Computing GRAD-CAM visualizations with the external TBX11K test set\n",
    "\n",
    "1. Annotations (512×512) → pre-crop (256×256); Scale the authors’ ground truth boxes by 0.5.\n",
    "2. Pre-crop (256×256) → lung-crop: Use the provided 256×256 mask to recover the actual crop ROI (the tight bounding rectangle of non-zero mask, optionally with a small margin). Intersect the pre-crop box with this ROI and translate it to the crop coordinate frame.\n",
    "3. Lung-crop → final saved TB image (256×256): The crop ROI is then resized to 256×256 when you saved the lung-cropped images. So scale the translated box by: sx = 256 / (roi_w),  sy = 256 / (roi_h)\n",
    "4. Use floor for x1,y1 and ceil for x2,y2 to preserve coverage; clip to [0,255].\n",
    "5. Run Grad-CAM on those TB images and overlays blue model CAM boxes + red GT boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033865b-ab15-49e0-bbec-67b7800cbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"gradcam-tbx11k-img\")\n",
    "\n",
    "# Reproducibility\n",
    "def set_deterministic(seed: int = 1337) -> None:\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)  # no-op on CPU\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_deterministic(1337)\n",
    "\n",
    "# User config\n",
    "cfg = RunConfig(\n",
    "    modality=MOD.img,                 \n",
    "    network=NETWORK.vgg11,           \n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-4,\n",
    "    img_size=224,                 \n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset_root=\"/dataset\",   \n",
    "    csv_test=\"/dataset/label_test_tbx11k.csv\",  # TBX11K test CSV\n",
    "    images_subdir=\"tbx11k/all\",  \n",
    "    out_dir=\"/unimodal/models_img_vgg11\",  # image-only runs root\n",
    "    class_names=(\"normal\",\"tb\"),\n",
    "    hf_offline=True,\n",
    "    hf_local_dir=None,\n",
    ")\n",
    "\n",
    "# TBX11K dataset specific paths\n",
    "# Final lung-cropped TB images (256×256)\n",
    "TB_256_DIR = \"/dataset/tbx11k/tb\"\n",
    "# 256×256 masks aligned to the pre-crop 256×256 space\n",
    "TB_MASKS_256_DIR = \"/dataset/tbx11k/masks/tb\"\n",
    "# Authors' lesion boxes (512×512 coords)\n",
    "BBOX_CSV_PATH = \"/dataset/data_tbx11k.csv\"\n",
    "# Output folder name under the chosen run\n",
    "SAVE_ROOT_NAME   = f\"gradcam_tbx11k_{cfg.network.name}_{cfg.modality.name}_test_overlap\"\n",
    "# Grad-CAM & drawing params\n",
    "CAM_METHOD  = \"gradcam\"   # \"gradcam\", \"gradcam++\", \"xgradcam\", \"layercam\", ...\n",
    "HEATMAP_ALPHA = 0.5\n",
    "BIN_THR = 0.4         # CAM threshold for contours/bboxes\n",
    "\n",
    "# Colors / thickness (BGR)\n",
    "CONTOUR_COLOR = (0, 0, 255) # red contours\n",
    "CONTOUR_THICK = 2\n",
    "BB_MODEL_COLOR = (255, 0, 0) # blue model boxes\n",
    "BB_GT_COLOR = (0, 0, 255) # red GT boxes\n",
    "BB_THICK = 3\n",
    "TARGET_CLASS_IDX = 1\n",
    "FINAL_SIZE = 256\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "# Optional crop behavior \n",
    "ROI_MARGIN_FRAC = 0.00        # e.g., 0.02 to add 2% per side\n",
    "ROI_ENFORCE_SQUARE = False    # set True if padded to square before resizing\n",
    "\n",
    "# Utility: paths, junk filtering\n",
    "JUNK_DIR_TOKENS = {\"__pycache__\", \".ipynb_checkpoints\", \".git\", \".svn\", \".DS_Store\"}\n",
    "JUNK_FILE_BASENAMES = {\"thumbs.db\", \"desktop.ini\"}\n",
    "_CHECKPOINT_RE = re.compile(r\"(?i)(?:^|[^A-Za-z0-9])checkpoint(?:$|[^A-Za-z0-9])\")\n",
    "_DOTFILE_RE = re.compile(r\"^\\.\")\n",
    "\n",
    "def _is_img(path_or_name: str) -> bool:\n",
    "    return os.path.splitext(path_or_name)[1].lower() in IMG_EXTS\n",
    "\n",
    "def _is_junk_path(path: str) -> bool:\n",
    "    p = pathlib.Path(path)\n",
    "    for part in p.parts:\n",
    "        if part in JUNK_DIR_TOKENS or _DOTFILE_RE.search(part):\n",
    "            return True\n",
    "    base = p.name\n",
    "    if base.lower() in JUNK_FILE_BASENAMES:\n",
    "        return True\n",
    "    if _DOTFILE_RE.match(base) or _CHECKPOINT_RE.search(os.path.splitext(base)[0]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _list_images(dir_path: str) -> List[str]:\n",
    "    out = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        dirs[:] = [d for d in dirs if not _is_junk_path(os.path.join(root, d))]\n",
    "        for fn in files:\n",
    "            fp = os.path.join(root, fn)\n",
    "            if not _is_junk_path(fp) and _is_img(fn):\n",
    "                out.append(fp)\n",
    "    return sorted(out)\n",
    "\n",
    "def _find_by_basename(dir_path: str, basename: str) -> Optional[str]:\n",
    "    base, _ = os.path.splitext(basename)\n",
    "    for ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"]:\n",
    "        p = os.path.join(dir_path, base + ext)\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# Checkpoint scanning \n",
    "def _ckpt_stem(cfg_scan: \"RunConfig\") -> str:\n",
    "    return f\"{cfg_scan.network.name}_{cfg_scan.modality.name}\"  # e.g., vgg11_img\n",
    "\n",
    "def _read_val_mcc_from_log(log_json_path: str) -> Optional[float]:\n",
    "    try:\n",
    "        with open(log_json_path, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "    # direct keys (also allow *_MCC)\n",
    "    for k in (\"best_val_mcc\",\"best_val_MCC\",\"val_mcc\",\"val_MCC\",\"best_mcc\",\"best_MCC\"):\n",
    "        v = meta.get(k, None)\n",
    "        if isinstance(v, (int, float)): return float(v)\n",
    "    # nested dicts\n",
    "    for nk in (\"valid_metrics\",\"validation_metrics\",\"val_metrics\",\"best_metrics\",\"best_valid_metrics\"):\n",
    "        d = meta.get(nk, None)\n",
    "        if isinstance(d, dict):\n",
    "            for cand in (\"MCC\",\"mcc\",\"val_MCC\",\"val_mcc\"):\n",
    "                v = d.get(cand, None)\n",
    "                if isinstance(v, (int, float)): return float(v)\n",
    "    # history → max\n",
    "    hist = meta.get(\"history\", None)\n",
    "    if isinstance(hist, dict):\n",
    "        for hk in (\"val_mcc\",\"val_MCC\"):\n",
    "            arr = hist.get(hk, None)\n",
    "            if isinstance(arr, list) and len(arr) > 0:\n",
    "                try: return float(np.nanmax(np.asarray(arr, dtype=float)))\n",
    "                except Exception: pass\n",
    "    return None\n",
    "\n",
    "def _guess_log_for_checkpoint(pt_path: str, stem: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Map:\n",
    "      best_{stem}_val_loss.pt               -> best_{stem}_val_loss_log.json\n",
    "      best_{stem}_val_loss_dimension_256.pt -> best_{stem}_val_loss_log_dimension_256.json\n",
    "    \"\"\"\n",
    "    subdir = os.path.dirname(pt_path)\n",
    "    cand1  = re.sub(r\"(_val_loss)(.*)\\.pt$\", r\"\\1_log\\2.json\", pt_path)\n",
    "    cand2  = os.path.join(subdir, f\"best_{stem}_val_loss_log.json\")\n",
    "    for c in (cand1, cand2):\n",
    "        if c and os.path.isfile(c):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def scan_img_checkpoints_by_mcc(cfg_scan: \"RunConfig\") -> pd.DataFrame:\n",
    "    assert cfg_scan.modality == MOD.img, \"scan_img_checkpoints_by_mcc expects MOD.img\"\n",
    "    stem = _ckpt_stem(cfg_scan)\n",
    "    patterns = [\n",
    "        os.path.join(cfg_scan.out_dir, \"**\", f\"best_{stem}_val_loss.pt\"), \n",
    "        os.path.join(cfg_scan.out_dir, \"**\", f\"best_{stem}_val_loss_dimension_*.pt\"), \n",
    "    ]\n",
    "    paths: List[str] = []\n",
    "    for pat in patterns:\n",
    "        paths.extend(glob.glob(pat, recursive=True))\n",
    "    paths = sorted(set(paths))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No checkpoints found under '{cfg_scan.out_dir}'. Tried: {patterns}\")\n",
    "\n",
    "    rows = []\n",
    "    for pt in paths:\n",
    "        subdir = os.path.dirname(pt)\n",
    "        log_json = _guess_log_for_checkpoint(pt, stem)\n",
    "        val_mcc = _read_val_mcc_from_log(log_json) if log_json else None\n",
    "        mtime = os.path.getmtime(pt) if os.path.isfile(pt) else 0.0\n",
    "        rows.append({\n",
    "            \"val_mcc\": (None if val_mcc is None else float(val_mcc)),\n",
    "            \"ckpt_path\": pt, \"log_json\": log_json,\n",
    "            \"mtime\": mtime, \"subdir\": subdir\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def select_best_by_mcc(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Highest non-negative MCC; ties → newest mtime.\n",
    "    If none valid → newest by mtime.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[\"val_mcc\"] = pd.to_numeric(df2[\"val_mcc\"], errors=\"coerce\")\n",
    "    ok = df2[df2[\"val_mcc\"].notna() & (df2[\"val_mcc\"] >= 0.0)].copy()\n",
    "    if len(ok) > 0:\n",
    "        ok = ok.sort_values(by=[\"val_mcc\",\"mtime\"], ascending=[False, False])\n",
    "        return ok.iloc[0]\n",
    "    return df2.sort_values(by=[\"mtime\"], ascending=[False]).iloc[0]\n",
    "\n",
    "# TBX11K CSV helpers\n",
    "def _read_test_csv(csv_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, header=None, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"Test CSV must have >= 2 columns (filename, label).\")\n",
    "    df = df.iloc[:, :2]; df.columns = [\"img\", \"label\"]\n",
    "\n",
    "    def _is_hdr(s: str) -> bool:\n",
    "        s = str(s).strip().lower()\n",
    "        return s in {\"img\",\"image\",\"filename\",\"file\",\"path\",\"label\",\"class\",\"target\",\"y\"} or s.startswith(\"img\")\n",
    "\n",
    "    if _is_hdr(df.iloc[0,0]) and _is_hdr(df.iloc[0,1]):\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    df[\"img\"]   = df[\"img\"].astype(str).map(lambda s: os.path.basename(s.strip()))\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"img\"].map(_is_img)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def crosscheck_tb(csv_tb_names: List[str], tb_dir: str) -> List[str]:\n",
    "    csv_tb_set = {n for n in csv_tb_names if n.lower().startswith(\"tb\")}\n",
    "    dir_tb_set = {os.path.basename(p) for p in _list_images(tb_dir) if os.path.basename(p).lower().startswith(\"tb\")}\n",
    "    inter = sorted(csv_tb_set & dir_tb_set)\n",
    "    log.info(\"[TB CHECK] CSV tb count : %d\", len(csv_tb_set))\n",
    "    log.info(\"[TB CHECK] Dir tb count : %d\", len(dir_tb_set))\n",
    "    log.info(\"[TB CHECK] Match count  : %d\", len(inter))\n",
    "    only_csv = sorted(csv_tb_set - dir_tb_set)\n",
    "    only_dir = sorted(dir_tb_set - csv_tb_set)\n",
    "    if only_csv or only_dir:\n",
    "        log.warning(\"[TB CHECK] Mismatch — only_in_csv=%d, only_in_dir=%d\", len(only_csv), len(only_dir))\n",
    "        if only_csv[:5]: log.warning(\"  e.g., only_in_csv: %s\", only_csv[:5])\n",
    "        if only_dir[:5]: log.warning(\"  e.g., only_in_dir: %s\", only_dir[:5])\n",
    "    else:\n",
    "        log.info(\"[TB CHECK] Filename sets MATCH ✅\")\n",
    "    return inter\n",
    "\n",
    "# GT mapping: authors (512×512) → final 256×256 via mask ROI\n",
    "def _parse_bbox_field(braw: Any) -> List[Dict[str, float]]:\n",
    "    \"\"\"'bbox' can be list[dict] or dict-as-string; return list of dicts with xmin,ymin,width,height.\"\"\"\n",
    "    if braw is None or (isinstance(braw, float) and math.isnan(braw)): return []\n",
    "    try:\n",
    "        b = ast.literal_eval(braw) if isinstance(braw, str) else braw\n",
    "    except Exception:\n",
    "        return []\n",
    "    if isinstance(b, dict):  return [b]\n",
    "    if isinstance(b, list):  return [x for x in b if isinstance(x, dict)]\n",
    "    return []\n",
    "\n",
    "def _mask_to_roi(mask_256: np.ndarray, margin_frac: float = ROI_MARGIN_FRAC,\n",
    "                 enforce_square: bool = ROI_ENFORCE_SQUARE) -> Optional[Tuple[int,int,int,int]]:\n",
    "    \"\"\"Return (x1,y1,x2,y2) tight ROI of non-zero mask with optional margin/square padding.\"\"\"\n",
    "    if mask_256 is None: return None\n",
    "    if mask_256.max() > 1:\n",
    "        _, binm = cv2.threshold(mask_256, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    else:\n",
    "        binm = (mask_256 > 0).astype(np.uint8)*255\n",
    "    cnts, _ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts: \n",
    "        return None\n",
    "    x1 = min(cv2.boundingRect(c)[0] for c in cnts)\n",
    "    y1 = min(cv2.boundingRect(c)[1] for c in cnts)\n",
    "    x2 = max(cv2.boundingRect(c)[0] + cv2.boundingRect(c)[2] for c in cnts)\n",
    "    y2 = max(cv2.boundingRect(c)[1] + cv2.boundingRect(c)[3] for c in cnts)\n",
    "    # margin\n",
    "    if margin_frac and margin_frac > 0:\n",
    "        w = x2 - x1; h = y2 - y1\n",
    "        mx = int(round(w * margin_frac)); my = int(round(h * margin_frac))\n",
    "        x1 -= mx; y1 -= my; x2 += mx; y2 += my\n",
    "    # enforce square (optional)\n",
    "    if enforce_square:\n",
    "        w = x2 - x1; h = y2 - y1\n",
    "        side = max(w, h)\n",
    "        cx = (x1 + x2) // 2; cy = (y1 + y2) // 2\n",
    "        x1 = cx - side//2; y1 = cy - side//2; x2 = x1 + side; y2 = y1 + side\n",
    "    # clip to [0,255]\n",
    "    x1 = max(0, min(255, x1)); y1 = max(0, min(255, y1))\n",
    "    x2 = max(1, min(256, x2)); y2 = max(1, min(256, y2))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def _scale_512_to_256_box(xmin: float, ymin: float, width: float, height: float,\n",
    "                          src_w: int = 512, src_h: int = 512) -> Tuple[float,float,float,float]:\n",
    "    \"\"\"Return (x1,y1,x2,y2) in pre-crop 256×256 space.\"\"\"\n",
    "    sx = 256.0 / float(src_w); sy = 256.0 / float(src_h)\n",
    "    x1 = (xmin) * sx; y1 = (ymin) * sy\n",
    "    x2 = (xmin + width) * sx; y2 = (ymin + height) * sy\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def _map_box_pre256_to_final256_via_roi(box_pre: Tuple[float,float,float,float],\n",
    "                                        roi: Tuple[int,int,int,int]) -> Optional[Tuple[int,int,int,int]]:\n",
    "    \"\"\"Apply ROI crop (intersect) then scale to 256 final image; floor/ceil to preserve coverage.\"\"\"\n",
    "    x1, y1, x2, y2 = box_pre\n",
    "    rx1, ry1, rx2, ry2 = roi\n",
    "    ix1 = max(x1, rx1); iy1 = max(y1, ry1)\n",
    "    ix2 = min(x2, rx2); iy2 = min(y2, ry2)\n",
    "    if ix2 <= ix1 or iy2 <= iy1:\n",
    "        return None\n",
    "    # translate to crop coords\n",
    "    cx1 = ix1 - rx1; cy1 = iy1 - ry1\n",
    "    cx2 = ix2 - rx1; cy2 = iy2 - ry1\n",
    "    rw = max(1, rx2 - rx1); rh = max(1, ry2 - ry1)\n",
    "    sx = 256.0 / float(rw); sy = 256.0 / float(rh)\n",
    "    fx1 = math.floor(cx1 * sx); fy1 = math.floor(cy1 * sy)\n",
    "    fx2 = math.ceil (cx2 * sx); fy2 = math.ceil (cy2 * sy)\n",
    "    # clip to [0,255], enforce non-zero size\n",
    "    fx1 = max(0, min(255, fx1)); fy1 = max(0, min(255, fy1))\n",
    "    fx2 = max(fx1+1, min(256, fx2)); fy2 = max(fy1+1, min(256, fy2))\n",
    "    return (int(fx1), int(fy1), int(fx2), int(fy2))\n",
    "\n",
    "def build_gt_box_map_final256(bbox_csv_path: str, masks_dir_256: str,\n",
    "                              names_final_256: List[str]) -> Dict[str, List[Tuple[int,int,int,int]]]:\n",
    "    \"\"\"\n",
    "    For each TB image name in names_final_256:\n",
    "      • Read its mask (256×256) to derive the crop ROI\n",
    "      • Load authors’ boxes (512×512), scale to pre-crop 256×256\n",
    "      • Apply ROI crop+resize mapping to final 256×256\n",
    "    Output: dict[name] -> list of (x1,y1,x2,y2) in final 256×256 coords\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(bbox_csv_path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    needed = [\"fname\",\"image_height\",\"image_width\",\"bbox\",\"target\",\"image_type\"]\n",
    "    for k in needed:\n",
    "        if k not in cols: raise ValueError(f\"Column '{k}' missing in {bbox_csv_path}\")\n",
    "\n",
    "    df = df[\n",
    "        (df[cols[\"image_type\"]].astype(str).str.lower() == \"tb\") &\n",
    "        (df[cols[\"target\"]].astype(str).str.lower() == \"tb\")\n",
    "    ].copy()\n",
    "\n",
    "    # Collect authors' boxes keyed by basename\n",
    "    authors_boxes_512: Dict[str, List[Tuple[float,float,float,float,int,int]]] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        name = os.path.basename(str(r[cols[\"fname\"]]).strip())\n",
    "        boxes = _parse_bbox_field(r[cols[\"bbox\"]])\n",
    "        if not boxes: \n",
    "            continue\n",
    "        W = int(r[cols[\"image_width\"]]) if not pd.isna(r[cols[\"image_width\"]]) else 512\n",
    "        H = int(r[cols[\"image_height\"]]) if not pd.isna(r[cols[\"image_height\"]]) else 512\n",
    "        W = 512 if W <= 0 else W; H = 512 if H <= 0 else H\n",
    "        pre_list = authors_boxes_512.setdefault(name, [])\n",
    "        for b in boxes:\n",
    "            try:\n",
    "                xmin = float(b.get(\"xmin\",0.0)); ymin = float(b.get(\"ymin\",0.0))\n",
    "                width= float(b.get(\"width\",0.0)); height= float(b.get(\"height\",0.0))\n",
    "                if width <= 0 or height <= 0: continue\n",
    "                pre_list.append((xmin, ymin, width, height, W, H))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # Map to final 256×256 using the 256×256 mask ROI\n",
    "    out: Dict[str, List[Tuple[int,int,int,int]]] = {}\n",
    "    for name in names_final_256:\n",
    "        base = os.path.splitext(name)[0]\n",
    "        mask_path = _find_by_basename(masks_dir_256, name)\n",
    "        if mask_path is None:\n",
    "            log.warning(\"[GT MAP] Missing mask for %s — skipping GT boxes.\", name)\n",
    "            continue\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            log.warning(\"[GT MAP] Unreadable mask for %s — skipping.\", name)\n",
    "            continue\n",
    "        roi = _mask_to_roi(mask, ROI_MARGIN_FRAC, ROI_ENFORCE_SQUARE)\n",
    "        if roi is None:\n",
    "            log.warning(\"[GT MAP] Empty ROI from mask for %s — skipping.\", name)\n",
    "            continue\n",
    "\n",
    "        # Try exact filename; if not found, try any key with same stem (different ext)\n",
    "        auth_list = authors_boxes_512.get(name, [])\n",
    "        if not auth_list:\n",
    "            for k, v in authors_boxes_512.items():\n",
    "                if os.path.splitext(k)[0].lower() == base.lower():\n",
    "                    auth_list = v\n",
    "                    break\n",
    "        if not auth_list:\n",
    "            # no GT for this image — allowed\n",
    "            continue\n",
    "\n",
    "        final_boxes = []\n",
    "        for (xmin, ymin, width, height, W, H) in auth_list:\n",
    "            x1p, y1p, x2p, y2p = _scale_512_to_256_box(xmin, ymin, width, height, src_w=W, src_h=H)\n",
    "            mapped = _map_box_pre256_to_final256_via_roi((x1p, y1p, x2p, y2p), roi)\n",
    "            if mapped is not None:\n",
    "                final_boxes.append(mapped)\n",
    "        if final_boxes:\n",
    "            out[name] = final_boxes\n",
    "    return out\n",
    "\n",
    "# Model / CAM helpers\n",
    "def print_model_overview_for_cam(model: nn.Module, img_size: int = 224) -> None:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(\"Grad-CAM — Model Overview\")\n",
    "    log.info(f\"Device: {device.type}\")\n",
    "    log.info(f\"Input size (C,H,W): (3, {img_size}, {img_size})\")\n",
    "    log.info(f\"Total params: {total:,}\")\n",
    "    log.info(f\"Trainable params: {train:,}\")\n",
    "    log.info(f\"Frozen params: {total-train:,}\")\n",
    "\n",
    "def _kernel_tuple(m: nn.Conv2d) -> Tuple[int,int]:\n",
    "    k = m.kernel_size\n",
    "    return (k if isinstance(k, tuple) else (k, k))\n",
    "\n",
    "def _last_conv_kgt1(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    last_any = None; last_kgt1 = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_any = m\n",
    "            if max(_kernel_tuple(m)) > 1:\n",
    "                last_kgt1 = m\n",
    "    return last_kgt1 if last_kgt1 is not None else last_any\n",
    "\n",
    "def _vit_patch_embed_conv(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    target = None\n",
    "    for name, m in module.named_modules():\n",
    "        if isinstance(m, nn.Conv2d) and (\"patch_embed\" in name or \"patch\" in name):\n",
    "            target = m\n",
    "    return target\n",
    "\n",
    "def _qualname_of_module(root: nn.Module, target: nn.Module) -> str:\n",
    "    for n, m in root.named_modules():\n",
    "        if m is target:\n",
    "            return n or \"<root>\"\n",
    "    return \"<unknown>\"\n",
    "\n",
    "def select_target_layer_for_cam(model: nn.Module) -> nn.Conv2d:\n",
    "    enc = getattr(model, \"img_enc\", None)\n",
    "    if enc is not None:\n",
    "        p3 = getattr(enc, \"post3x3\", None)\n",
    "        if isinstance(p3, nn.Sequential):\n",
    "            for m in p3.modules():\n",
    "                if isinstance(m, nn.Conv2d) and _kernel_tuple(m) == (3,3):\n",
    "                    return m\n",
    "        elif isinstance(p3, nn.Conv2d) and _kernel_tuple(p3) == (3,3):\n",
    "            return p3\n",
    "        tgt = _last_conv_kgt1(enc)\n",
    "        if isinstance(tgt, nn.Conv2d): return tgt\n",
    "        vitp = _vit_patch_embed_conv(enc)\n",
    "        if isinstance(vitp, nn.Conv2d): return vitp\n",
    "    tgt = _last_conv_kgt1(model)\n",
    "    if isinstance(tgt, nn.Conv2d): return tgt\n",
    "    raise RuntimeError(\"No suitable Conv2d layer found for Grad-CAM.\")\n",
    "\n",
    "class CamImageWrapper(nn.Module):\n",
    "    def __init__(self, model_img_only: nn.Module):\n",
    "        super().__init__()\n",
    "        self.mm = model_img_only\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mm.forward_image(x)\n",
    "\n",
    "def build_preprocess(size: int) -> T.Compose:\n",
    "    return T.Compose([\n",
    "        T.ToPILImage(), T.Resize((size, size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_ckpt_model(cfg_load: \"RunConfig\", ckpt_path: str) -> \"ImgOrTxtClassifier\":\n",
    "    m = ImgOrTxtClassifier(cfg_load).to(device).eval()\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    return m\n",
    "\n",
    "def _init_cam(CAMClass, model, target_layers):\n",
    "    sig = inspect.signature(CAMClass.__init__)\n",
    "    if \"use_cuda\" in sig.parameters:\n",
    "        return CAMClass(model=model, target_layers=target_layers, use_cuda=(device.type == \"cuda\"))\n",
    "    return CAMClass(model=model, target_layers=target_layers)\n",
    "\n",
    "# Grad-CAM main (TBX11K with GT overlay)\n",
    "def run_gradcam_tbx11k_img_with_gt(\n",
    "    cfg_cam: \"RunConfig\",\n",
    "    ckpt_path: str,\n",
    "    csv_test_path: str,\n",
    "    tb_256_dir: str,\n",
    "    masks_256_dir: str,\n",
    "    bbox_csv_path: str,\n",
    "    save_root: str,\n",
    "    cam_method: str = CAM_METHOD,\n",
    "    heatmap_alpha: float = HEATMAP_ALPHA,\n",
    "    bin_thr: float = BIN_THR,\n",
    ") -> None:\n",
    "    assert os.path.isfile(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    for p in [csv_test_path, tb_256_dir, masks_256_dir, bbox_csv_path]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Required path missing: {p}\")\n",
    "\n",
    "    # Outputs\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "    out_dirs = {\n",
    "        \"images\": os.path.join(save_root, \"images\"),\n",
    "        \"heatmaps\": os.path.join(save_root, \"heatmaps\"),\n",
    "        \"contours\": os.path.join(save_root, \"contours\"),\n",
    "        \"bboxes\": os.path.join(save_root, \"bboxes\"),\n",
    "    }\n",
    "    for d in out_dirs.values(): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Model & CAM\n",
    "    model_img = _load_ckpt_model(cfg_cam, ckpt_path)\n",
    "    print_model_overview_for_cam(model_img, img_size=int(getattr(cfg_cam, \"img_size\", 224)))\n",
    "    cam_model = CamImageWrapper(model_img).to(device).eval()\n",
    "    target_layer = select_target_layer_for_cam(model_img)\n",
    "    for p in target_layer.parameters(): p.requires_grad_(True)\n",
    "    tl_name = _qualname_of_module(model_img, target_layer)\n",
    "    log.info(f\"[CAM] Target layer: {tl_name}  kernel={getattr(target_layer, 'kernel_size', None)}\")\n",
    "\n",
    "    methods = {\n",
    "        \"gradcam\": GradCAM, \"gradcam++\": GradCAMPlusPlus, \"hirescam\": HiResCAM,\n",
    "        \"xgradcam\": XGradCAM, \"layercam\": LayerCAM, \"eigencam\": EigenCAM,\n",
    "        \"eigengradcam\": EigenGradCAM, \"scorecam\": ScoreCAM, \"ablationcam\": AblationCAM\n",
    "    }\n",
    "    mkey = cam_method.lower()\n",
    "    if mkey not in methods:\n",
    "        raise ValueError(f\"Unknown CAM method '{cam_method}'\")\n",
    "    cam = _init_cam(methods[mkey], model=cam_model, target_layers=[target_layer])\n",
    "    try: cam.batch_size = 1\n",
    "    except Exception: pass\n",
    "    preprocess = build_preprocess(int(getattr(cfg_cam, \"img_size\", 224)))\n",
    "\n",
    "    # TB selection via CSV + directory cross-check\n",
    "    df_all = _read_test_csv(csv_test_path)\n",
    "    df_tb  = df_all[df_all[\"img\"].str.lower().str.startswith(\"tb\")].copy()\n",
    "    tb_names = crosscheck_tb(sorted(df_tb[\"img\"].astype(str).unique().tolist()), tb_256_dir)\n",
    "\n",
    "    # Build GT box map: final 256×256 coords\n",
    "    gt_boxes_map = build_gt_box_map_final256(bbox_csv_path, masks_256_dir, tb_names)\n",
    "\n",
    "    # Iterate TB images\n",
    "    for name in tb_names:\n",
    "        img_path = _find_by_basename(tb_256_dir, name) or os.path.join(tb_256_dir, name)\n",
    "        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            log.warning(\"[CAM] Skipping unreadable: %s\", img_path)\n",
    "            continue\n",
    "        H, W = bgr.shape[:2]\n",
    "        if (H, W) != (FINAL_SIZE, FINAL_SIZE):\n",
    "            bgr = cv2.resize(bgr, (FINAL_SIZE, FINAL_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Save base 256 image\n",
    "        stem = os.path.splitext(os.path.basename(name))[0]\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"images\"], f\"{stem}.png\"), bgr)\n",
    "\n",
    "        # Model input\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = preprocess(rgb).unsqueeze(0).to(device)\n",
    "\n",
    "        # Grad-CAM\n",
    "        cam_model.zero_grad(set_to_none=True)\n",
    "        with torch.enable_grad():\n",
    "            if mkey == \"eigencam\":\n",
    "                mask = cam(input_tensor=x)[0]\n",
    "            else:\n",
    "                mask = cam(\n",
    "                    input_tensor=x,\n",
    "                    targets=[ClassifierOutputTarget(int(TARGET_CLASS_IDX))],\n",
    "                    aug_smooth=True, eigen_smooth=True\n",
    "                )[0]\n",
    "\n",
    "        # Normalize & resize to 256\n",
    "        mmin, mmax = float(np.min(mask)), float(np.max(mask))\n",
    "        mask = (mask - mmin) / (mmax - mmin + 1e-8)\n",
    "        mask_u8 = (np.clip(cv2.resize(mask, (FINAL_SIZE, FINAL_SIZE), interpolation=cv2.INTER_NEAREST), 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        # Heatmap overlay\n",
    "        heat = cv2.applyColorMap(mask_u8, cv2.COLORMAP_HOT)\n",
    "        heat_overlay = cv2.addWeighted(heat, float(heatmap_alpha), bgr, 1.0 - float(heatmap_alpha), 0.0)\n",
    "\n",
    "        # Contours & model boxes\n",
    "        thr = int(255 * float(bin_thr))\n",
    "        _, binm = cv2.threshold(mask_u8, thr, 255, cv2.THRESH_BINARY)\n",
    "        contours,_ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        cont_img = bgr.copy()\n",
    "        if contours:\n",
    "            cv2.drawContours(cont_img, contours, -1, CONTOUR_COLOR, CONTOUR_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        box_img = bgr.copy()\n",
    "        model_boxes: List[Tuple[int,int,int,int]] = []\n",
    "        for c in contours:\n",
    "            x0, y0, w0, h0 = cv2.boundingRect(c)\n",
    "            x1, y1, x2, y2 = x0, y0, x0 + w0, y0 + h0\n",
    "            model_boxes.append((x1, y1, x2, y2))\n",
    "            cv2.rectangle(box_img, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # GT boxes in final 256×256 space\n",
    "        gt_boxes = gt_boxes_map.get(name, gt_boxes_map.get(os.path.basename(name), []))\n",
    "\n",
    "        # Save outputs with GT overlay (red)\n",
    "        # 1) Heatmaps + GT\n",
    "        heat_ov = heat_overlay.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "            cv2.rectangle(heat_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"heatmaps\"], f\"{stem}__{cfg_cam.network.name}__{mkey}.png\"), heat_ov)\n",
    "\n",
    "        # 2) Contours + GT\n",
    "        cont_ov = cont_img.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "            cv2.rectangle(cont_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"contours\"], f\"{stem}__{cfg_cam.network.name}__{mkey}.png\"), cont_ov)\n",
    "\n",
    "        # 3) BBoxes view: model (blue) + GT (red)\n",
    "        box_ov = bgr.copy()\n",
    "        for (x1, y1, x2, y2) in model_boxes:\n",
    "            cv2.rectangle(box_ov, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "            cv2.rectangle(box_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"bboxes\"], f\"{stem}__{cfg_cam.network.name}__{mkey}.png\"), box_ov)\n",
    "\n",
    "    # Cleanup\n",
    "    try:\n",
    "        if hasattr(cam, \"activations_and_grads\") and (cam.activations_and_grads is not None):\n",
    "            cam.activations_and_grads.release()\n",
    "    except Exception:\n",
    "        pass\n",
    "    del cam, cam_model, model_img\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    log.info(\"[CAM] Saved to: %s\", save_root)\n",
    "\n",
    "# Scan → select best (by val-MCC) → run CAM on TBX11K\n",
    "cfg_scan = dataclasses.replace(cfg, modality=MOD.img)\n",
    "df_found = scan_img_checkpoints_by_mcc(cfg_scan)\n",
    "\n",
    "def _fmt(x):\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)): return \"—\"\n",
    "    return f\"{x:.4f}\" if isinstance(x, float) else str(x)\n",
    "\n",
    "# Print only valid, non-negative MCC entries\n",
    "df_show = df_found.copy()\n",
    "df_show[\"val_mcc\"] = pd.to_numeric(df_show[\"val_mcc\"], errors=\"coerce\")\n",
    "df_show = df_show[df_show[\"val_mcc\"].notna() & (df_show[\"val_mcc\"] >= 0.0)]\n",
    "\n",
    "print(\"\\n================ Available image-only checkpoints (sorted by folder) ================\")\n",
    "if len(df_show) == 0:\n",
    "    print(\"• (no checkpoints with a valid non-negative MCC found; selection will fallback to newest mtime)\")\n",
    "else:\n",
    "    for _, r in df_show.sort_values(\"subdir\").iterrows():\n",
    "        print(f\"• {r['subdir']}\")\n",
    "        print(f\" val_MCC={_fmt(float(r['val_mcc']))}\")\n",
    "        print(f\" ckpt={r['ckpt_path']}\")\n",
    "        if r['log_json']: print(f\" log ={r['log_json']}\")\n",
    "print(\"======================================================================================\\n\")\n",
    "\n",
    "row_best = select_best_by_mcc(df_found)\n",
    "best_ckpt = row_best[\"ckpt_path\"]\n",
    "best_subdir = row_best[\"subdir\"]\n",
    "best_mcc = row_best[\"val_mcc\"]\n",
    "reason = f\"highest validation MCC={_fmt(best_mcc)}\"\n",
    "if best_mcc is None or (isinstance(best_mcc, float) and math.isnan(best_mcc)):\n",
    "    reason = \"no MCC recorded; selecting most recent checkpoint\"\n",
    "print(\">>> Selecting checkpoint:\")\n",
    "print(f\" {best_ckpt}\")\n",
    "print(f\" Because: {reason}\\n\")\n",
    "# Build save root under the chosen run folder (no _dimension_ tags written)\n",
    "save_root = os.path.join(best_subdir, SAVE_ROOT_NAME)\n",
    "\n",
    "log.info(\"================ Grad-CAM (TBX11K, IMG-ONLY; mask-mapped GT) ================\")\n",
    "log.info(f\"Checkpoint : {best_ckpt}\")\n",
    "log.info(f\"Backbone   : {cfg.network.name}\")\n",
    "log.info(f\"Modality   : {cfg.modality.name}\")\n",
    "log.info(f\"csv_test   : {cfg.csv_test}\")\n",
    "log.info(f\"TB dir     : {TB_256_DIR} (final lung-cropped 256×256)\")\n",
    "log.info(f\"Masks dir  : {TB_MASKS_256_DIR} (pre-crop masks 256×256)\")\n",
    "log.info(f\"BBoxes csv : {BBOX_CSV_PATH} (authors' boxes @512×512)\")\n",
    "log.info(f\"Save root  : {save_root}\")\n",
    "log.info(\"===========================================================================\")\n",
    "\n",
    "# Run Grad-CAM with GT overlay\n",
    "run_gradcam_tbx11k_img_with_gt(\n",
    "    cfg_cam=cfg,\n",
    "    ckpt_path=best_ckpt,\n",
    "    csv_test_path=cfg.csv_test,\n",
    "    tb_256_dir=TB_256_DIR,\n",
    "    masks_256_dir=TB_MASKS_256_DIR,\n",
    "    bbox_csv_path=BBOX_CSV_PATH,\n",
    "    save_root=save_root,\n",
    "    cam_method=CAM_METHOD,\n",
    "    heatmap_alpha=HEATMAP_ALPHA,\n",
    "    bin_thr=BIN_THR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5fcab-c098-415b-bd09-6908c259694d",
   "metadata": {},
   "source": [
    "## END OF CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e03f7e-8463-4955-b637-db6856f8ae86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_class_py39)",
   "language": "python",
   "name": "pytorch_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
