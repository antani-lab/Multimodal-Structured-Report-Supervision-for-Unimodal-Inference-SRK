{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfa17a-41f9-45e4-b307-ea7b0689d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5188cc6-e26c-4f30-88f8-5c6e1cedfc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# =========================\n",
    "# Standard library imports\n",
    "# =========================\n",
    "import ast\n",
    "import glob\n",
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import dataclasses\n",
    "from collections import defaultdict\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# Third-party imports\n",
    "# =========================\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, WeightedRandomSampler\n",
    "from torchvision import transforms as T\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pytorch_grad_cam import (\n",
    "    AblationCAM,\n",
    "    EigenCAM,\n",
    "    EigenGradCAM,\n",
    "    GradCAM,\n",
    "    GradCAMPlusPlus,\n",
    "    HiResCAM,\n",
    "    LayerCAM,\n",
    "    ScoreCAM,\n",
    "    XGradCAM,\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd92204-b271-449c-93d2-7119887af892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set environmental variables\n",
    "\n",
    "print(torch.__version__)\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/gpfs/gsfs12/users/rajaramans2/.cache/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/gpfs/gsfs12/users/rajaramans2/.cache/huggingface/hub\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9353d-7549-4789-ac3f-3b3c4dee4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "class MOD(Enum):\n",
    "    img         = auto()\n",
    "    txt         = auto()\n",
    "    multimodal  = auto()\n",
    "\n",
    "class NETWORK(Enum):\n",
    "    # timm / torchvision families\n",
    "    dpn68 = auto()\n",
    "    coatnet0 = auto()\n",
    "    convnext_nano = auto()\n",
    "    hrnet32 = auto()\n",
    "    resnet18 = auto()\n",
    "    densenet121 = auto()\n",
    "    mobilenet_v2 = auto()\n",
    "    # VGG family\n",
    "    vgg11 = auto()\n",
    "    vgg13 = auto()\n",
    "    vgg16 = auto()\n",
    "    vgg19 = auto()\n",
    " \n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    modality: MOD\n",
    "    network: NETWORK\n",
    "    n_classes: int = 2\n",
    "    hidden_dim: int = 256\n",
    "    epochs: int = 64\n",
    "    lr: float = 5e-5\n",
    "    weight_decay: float = 1e-4\n",
    "    img_size: int = 224\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 2\n",
    "    pin_memory: bool = True\n",
    "    \n",
    "    # classes\n",
    "    class_names: Tuple[str, ...] = (\"normal\", \"tb\")\n",
    "\n",
    "    # data locations\n",
    "    dataset_root: str = \"//dataset\"\n",
    "    csv_train: Optional[str] = None\n",
    "    csv_valid: Optional[str] = None\n",
    "    csv_test:  Optional[str] = None\n",
    "    images_subdir: str = \"images\"\n",
    "    reports_subdir: str = \"reports\"\n",
    "    out_dir: str = \"/models\"\n",
    "\n",
    "    # text/HF (offline-first)\n",
    "    hf_offline: bool = True\n",
    "    hf_local_dir: Optional[str] = \"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\"\n",
    "    hf_text_model_name: str = \"microsoft/BiomedVLP-CXR-BERT-general\"\n",
    "    max_tokens: int = 512\n",
    "    hf_tokenizer_local_dir: Optional[str] = \"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\"\n",
    "\n",
    "    # alignment losses (for MM only)\n",
    "    use_align_losses: bool = True\n",
    "    align_lambda: float = 0.5\n",
    "    contrastive_temperature: float = 0.07\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.csv_train is None:\n",
    "            self.csv_train = os.path.join(self.dataset_root, \"label_train.csv\")\n",
    "        if self.csv_valid is None:\n",
    "            self.csv_valid = os.path.join(self.dataset_root, \"label_valid.csv\")\n",
    "        if self.csv_test is None:\n",
    "            self.csv_test  = os.path.join(self.dataset_root, \"label_test.csv\")\n",
    "\n",
    "    @property\n",
    "    def images_dir(self) -> str:\n",
    "        return os.path.join(self.dataset_root, self.images_subdir)\n",
    "\n",
    "    @property\n",
    "    def reports_dir(self) -> str:\n",
    "        return os.path.join(self.dataset_root, self.reports_subdir)\n",
    "\n",
    "def ckpt_stem(cfg: RunConfig) -> str:\n",
    "    return f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "\n",
    "def ckpt_paths(cfg: RunConfig) -> Dict[str, str]:\n",
    "    stem = ckpt_stem(cfg)\n",
    "    base = cfg.out_dir\n",
    "    return {\n",
    "        \"best_pt\":         os.path.join(base, f\"best_{stem}_val_loss.pt\"),\n",
    "        \"curves_png\":      os.path.join(base, f\"best_{stem}_val_loss_curves.png\"),\n",
    "        \"curves_csv\":      os.path.join(base, f\"best_{stem}_val_loss_history.csv\"),\n",
    "        \"curves_png_full\": os.path.join(base, f\"best_{stem}_val_loss_curves_full.png\"),\n",
    "        \"curves_csv_full\": os.path.join(base, f\"best_{stem}_val_loss_history_full.csv\"),\n",
    "        \"log_json\":        os.path.join(base, f\"best_{stem}_val_loss_log.json\"),\n",
    "        \"mcc_png\":         os.path.join(base, f\"best_{stem}_val_mcc_curve.png\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad741e-c6c8-4fc3-87dc-f263e5ee7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dataset & loaders (Albumentations), loads the train, validation, and test CSV files; class labels: 0 for normal, and 1 for tb\n",
    "\n",
    "def _read_split_csv_headerless(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a split CSV with NO header into a DataFrame with canonical\n",
    "    columns: [\"filename\", \"label\"].    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None, names=[\"filename\", \"label\"])\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{path} must have ≥2 columns (filename, label)\")\n",
    "    df[\"filename\"] = df[\"filename\"].astype(str)\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def _read_csv_two_cols_no_header(path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper when you just need the two lists.\n",
    "    Used by the dataset constructor.\n",
    "    \"\"\"\n",
    "    df = _read_split_csv_headerless(path)\n",
    "    return df[\"filename\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "def _print_split_summary(cfg: \"RunConfig\", which: str, modality=None) -> None:\n",
    "    \"\"\"\n",
    "    Print a per-split summary similar to the unimodal stack.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : RunConfig\n",
    "        Must expose csv_train / csv_valid / csv_test, n_classes, and optionally class_names.\n",
    "    which : {\"train\",\"valid\",\"test\"}\n",
    "    modality : optional\n",
    "        Either an enum with .name or a string; used only for pretty-printing.\n",
    "    \"\"\"\n",
    "    csv_attr = f\"csv_{which}\"\n",
    "    if not hasattr(cfg, csv_attr):\n",
    "        print(f\"[WARN] _print_split_summary: cfg has no attribute '{csv_attr}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    csv_path = getattr(cfg, csv_attr)\n",
    "    if not csv_path or not os.path.isfile(csv_path):\n",
    "        print(f\"[WARN] _print_split_summary: '{csv_attr}'='{csv_path}' not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    df = _read_split_csv_headerless(csv_path)\n",
    "    total = len(df)\n",
    "    vc = df[\"label\"].value_counts().sort_index()\n",
    "\n",
    "    # Try to get a readable modality name\n",
    "    if modality is None:\n",
    "        mod_name = \"N/A\"\n",
    "    else:\n",
    "        mod_name = getattr(modality, \"name\", str(modality))\n",
    "\n",
    "    print(f\"\\n[Split summary] split='{which}' | modality={mod_name} | csv='{csv_path}'\", flush=True)\n",
    "    print(f\"  total rows: {total}\", flush=True)\n",
    "\n",
    "    class_names = getattr(cfg, \"class_names\", None)\n",
    "    for cls_idx, cnt in vc.items():\n",
    "        if class_names is not None and 0 <= cls_idx < len(class_names):\n",
    "            cname = class_names[cls_idx]\n",
    "        else:\n",
    "            cname = f\"class_{cls_idx}\"\n",
    "        print(f\"  label={cls_idx:2d} ({cname:>10s}) : n={int(cnt)}\", flush=True)\n",
    "    print(\"\", flush=True)\n",
    "\n",
    "# ============================\n",
    "# ==== Path & label helpers\n",
    "# ============================\n",
    "\n",
    "def _abs_image_path(dataset_root: str, images_subdir: str, fname: str) -> Path:\n",
    "    return Path(dataset_root) / images_subdir / fname\n",
    "\n",
    "def _abs_report_path(dataset_root: str, reports_subdir: str, fname: str) -> Path:\n",
    "    \"\"\"\n",
    "    For a given image filename 'xxx.ext', we first try '<stem>.txt' under\n",
    "    reports_subdir. If not present,\n",
    "    we fall back to trying the raw fname itself under reports_subdir.\n",
    "    \"\"\"\n",
    "    root = Path(dataset_root)\n",
    "    stem_txt = root / reports_subdir / (Path(fname).stem + \".txt\")\n",
    "    if stem_txt.exists():\n",
    "        return stem_txt\n",
    "    return root / reports_subdir / fname\n",
    "\n",
    "def _one_hot(idx: int, n_classes: int) -> torch.Tensor:\n",
    "    y = torch.zeros(n_classes, dtype=torch.float32)\n",
    "    if 0 <= idx < n_classes:\n",
    "        y[idx] = 1.0\n",
    "    return y\n",
    "\n",
    "# ============================\n",
    "# ==== Multimodal Dataset Class\n",
    "# ============================\n",
    "\n",
    "class CSVMMImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multimodal dataset for joint image + text training.\n",
    "\n",
    "    Returns dict:\n",
    "        {\n",
    "            \"image\":     CxHxW tensor (float32),\n",
    "            \"text\":      {\"input_ids\", \"attention_mask\"} (int64 tensors),\n",
    "            \"y_onehot\":  one-hot label (float32),\n",
    "            \"filename\":  original image filename (str)\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg: \"RunConfig\",\n",
    "                 which: str,\n",
    "                 n_classes: int,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 max_len: int = 192) -> None:\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.n_classes = int(n_classes)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = int(max_len)\n",
    "\n",
    "        # --- Read split CSV (headerless) ---\n",
    "        csv_path = {\"train\": cfg.csv_train,\n",
    "                    \"valid\": cfg.csv_valid,\n",
    "                    \"test\":  cfg.csv_test}[which]\n",
    "        img_names, labels = _read_csv_two_cols_no_header(csv_path)\n",
    "\n",
    "        # --- Build list of (image_path, label, report_path) where both exist ---\n",
    "        self.items: List[Tuple[Path, int, Path]] = []\n",
    "        n_missing_img = 0\n",
    "        n_missing_txt = 0\n",
    "\n",
    "        for fn, y in zip(img_names, labels):\n",
    "            ip = _abs_image_path(cfg.dataset_root, cfg.images_subdir, fn)\n",
    "            rp = _abs_report_path(cfg.dataset_root, cfg.reports_subdir, fn)\n",
    "            if not ip.exists():\n",
    "                n_missing_img += 1\n",
    "                continue\n",
    "            if not rp.exists():\n",
    "                n_missing_txt += 1\n",
    "                continue\n",
    "            self.items.append((ip, int(y), rp))\n",
    "\n",
    "        # --- Report what survived (multimodal pairing) ---\n",
    "        print(f\"[Multimodal {which}] paired (image+report) samples: {len(self.items)} \"\n",
    "              f\"(missing_img={n_missing_img}, missing_txt={n_missing_txt})\",\n",
    "              flush=True)\n",
    "\n",
    "        # --- Image transforms (CLAHE only for train) ---\n",
    "        size = int(cfg.img_size)\n",
    "        self.train_tfm = A.Compose(\n",
    "            [\n",
    "                A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "                A.Resize(size, size, interpolation=cv2.INTER_AREA),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                            std=(0.229, 0.224, 0.225)),\n",
    "                ToTensorV2()\n",
    "            ]\n",
    "        )\n",
    "        self.eval_tfm = A.Compose(\n",
    "            [\n",
    "                A.Resize(size, size, interpolation=cv2.INTER_AREA),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                            std=(0.229, 0.224, 0.225)),\n",
    "                ToTensorV2()\n",
    "            ]\n",
    "        )\n",
    "        self.tfm = self.train_tfm if which == \"train\" else self.eval_tfm\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    # --- internals: image & text reading ---\n",
    "\n",
    "    def _read_img(self, path: Path) -> torch.Tensor:\n",
    "        # Grayscale read → RGB → Albumentations → tensor\n",
    "        img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            # Failsafe black image\n",
    "            img = np.zeros((self.cfg.img_size, self.cfg.img_size), dtype=np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        img = self.tfm(image=img)[\"image\"]  # (C,H,W), float32\n",
    "        return img\n",
    "\n",
    "    def _read_text(self, path: Path) -> Dict[str, torch.Tensor]:\n",
    "        try:\n",
    "            text = path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "        if not text:\n",
    "            text = \"[EMPTY]\"\n",
    "        tok = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Strip batch dimension\n",
    "        return {\n",
    "            \"input_ids\": tok[\"input_ids\"][0],\n",
    "            \"attention_mask\": tok[\"attention_mask\"][0],\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        ip, y_idx, rp = self.items[idx]\n",
    "        y = _one_hot(y_idx, self.n_classes)\n",
    "        return {\n",
    "            \"image\": self._read_img(ip),\n",
    "            \"text\": self._read_text(rp),\n",
    "            \"y_onehot\": y,\n",
    "            \"filename\": ip.name,\n",
    "        }\n",
    "\n",
    "# ============================\n",
    "# ==== Tokenizer builder\n",
    "# ============================\n",
    "def build_tokenizer(cfg: \"RunConfig\") -> AutoTokenizer:\n",
    "    \"\"\"\n",
    "    Build HuggingFace tokenizer for the text branch.\n",
    "    Mirrors unimodal behavior; prefers local snapshot if provided.\n",
    "    \"\"\"\n",
    "    model_id = getattr(cfg, \"hf_text_model_name\",\n",
    "                       \"microsoft/BiomedVLP-CXR-BERT-general\")\n",
    "    offline   = bool(getattr(cfg, \"hf_offline\", True))\n",
    "    local_dir = getattr(cfg, \"hf_tokenizer_local_dir\", None)\n",
    "    # >>> NEW: read trust_remote_code flag <<<\n",
    "    trust_remote = bool(getattr(cfg, \"hf_trust_remote_code\", False))\n",
    "\n",
    "    try:\n",
    "        if local_dir and os.path.isdir(local_dir):\n",
    "            return AutoTokenizer.from_pretrained(\n",
    "                local_dir,\n",
    "                local_files_only=True,\n",
    "                use_fast=True,\n",
    "                trust_remote_code=trust_remote,  # <<< NEW\n",
    "            )\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            local_files_only=offline,\n",
    "            use_fast=True,\n",
    "            trust_remote_code=trust_remote,      # <<< NEW\n",
    "        )\n",
    "    except Exception:\n",
    "        if offline:\n",
    "            raise RuntimeError(\n",
    "                \"Offline mode enabled but no local tokenizer found at \"\n",
    "                f\"hf_tokenizer_local_dir={local_dir!r}\"\n",
    "            )\n",
    "        # Fallback to online fetch\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            use_fast=True,\n",
    "            trust_remote_code=trust_remote,      # <<< NEW\n",
    "        )\n",
    "\n",
    "# ============================\n",
    "# ==== Imbalanced sampler\n",
    "# ============================\n",
    "\n",
    "class ImbalancedDatasetSampler(Sampler[int]):\n",
    "    \"\"\"\n",
    "    Simple inverse-frequency sampler over class indices.    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels: torch.Tensor):\n",
    "        self.labels = labels.clone().cpu().long()\n",
    "        k = int(self.labels.max().item()) + 1 if self.labels.numel() > 0 else 2\n",
    "        counts = torch.bincount(self.labels, minlength=k).clamp_min(1)\n",
    "        weights = (1.0 / counts)[self.labels]\n",
    "        self.sample_weights = weights.double()\n",
    "        self.num_samples = len(self.labels)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx = torch.multinomial(\n",
    "            self.sample_weights,\n",
    "            num_samples=self.num_samples,\n",
    "            replacement=True,\n",
    "        )\n",
    "        return iter(idx.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "# ============================\n",
    "# ==== Loader creation (multimodal)\n",
    "# ============================\n",
    "\n",
    "def make_loaders(cfg: \"RunConfig\") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Build train/valid/test DataLoaders for multimodal training.\n",
    "\n",
    "    - Prints split summaries (like unimodal) using _print_split_summary.\n",
    "    - Uses CSVMMImageTextDataset for all splits.\n",
    "    - Uses ImbalancedDatasetSampler for the training set.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Split summaries *before* dataset construction (raw CSV)\n",
    "    _print_split_summary(cfg, \"train\", modality=getattr(cfg, \"modality\", None))\n",
    "    _print_split_summary(cfg, \"valid\", modality=getattr(cfg, \"modality\", None))\n",
    "    _print_split_summary(cfg, \"test\",  modality=getattr(cfg, \"modality\", None))\n",
    "\n",
    "    # 2) Tokenizer\n",
    "    tok = build_tokenizer(cfg)\n",
    "\n",
    "    # 3) Datasets (these will also print how many image+report pairs survive)\n",
    "    ds_tr = CSVMMImageTextDataset(cfg, \"train\", cfg.n_classes, tokenizer=tok, max_len=int(cfg.max_tokens))\n",
    "    ds_va = CSVMMImageTextDataset(cfg, \"valid\", cfg.n_classes, tokenizer=tok, max_len=int(cfg.max_tokens))\n",
    "    ds_te = CSVMMImageTextDataset(cfg, \"test\",  cfg.n_classes, tokenizer=tok, max_len=int(cfg.max_tokens))\n",
    "\n",
    "    # 4) Class-balanced sampler for training (based on CSV labels, as in unimodal)\n",
    "    df_tr = _read_split_csv_headerless(cfg.csv_train)\n",
    "    labels_tr = torch.as_tensor(df_tr[\"label\"].values, dtype=torch.long)\n",
    "    sampler = ImbalancedDatasetSampler(labels_tr)\n",
    "\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr,\n",
    "        batch_size=int(cfg.batch_size),\n",
    "        sampler=sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=int(cfg.num_workers),\n",
    "        pin_memory=bool(cfg.pin_memory),\n",
    "        drop_last=True,\n",
    "        persistent_workers=bool(int(cfg.num_workers) > 0),\n",
    "    )\n",
    "\n",
    "    dl_va = DataLoader(\n",
    "        ds_va,\n",
    "        batch_size=int(cfg.batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=int(cfg.num_workers),\n",
    "        pin_memory=bool(cfg.pin_memory),\n",
    "        drop_last=False,\n",
    "        persistent_workers=bool(int(cfg.num_workers) > 0),\n",
    "    )\n",
    "\n",
    "    dl_te = DataLoader(\n",
    "        ds_te,\n",
    "        batch_size=int(cfg.batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=int(cfg.num_workers),\n",
    "        pin_memory=bool(cfg.pin_memory),\n",
    "        drop_last=False,\n",
    "        persistent_workers=bool(int(cfg.num_workers) > 0),\n",
    "    )\n",
    "\n",
    "    # 5) Batch count sanity check\n",
    "    def _n_batches(n_items: int, bs: int, drop_last: bool) -> int:\n",
    "        if drop_last:\n",
    "            return max(0, n_items // max(1, bs))\n",
    "        return (n_items + max(1, bs) - 1) // max(1, bs)\n",
    "\n",
    "    print(\n",
    "        f\"[batches] train={_n_batches(len(ds_tr), int(cfg.batch_size), True)}, \"\n",
    "        f\"valid={_n_batches(len(ds_va), int(cfg.batch_size), False)}, \"\n",
    "        f\"test={_n_batches(len(ds_te), int(cfg.batch_size), False)}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    return dl_tr, dl_va, dl_te\n",
    "\n",
    "train_loader, valid_loader, test_loader = make_loaders(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a289e-b2da-4a28-a631-6754c0a78e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "# ------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _net_name_like(x) -> str:\n",
    "    \"\"\"\n",
    "    Return a lowercase string name for the image backbone:\n",
    "      - Enum with .name  -> .name.lower()\n",
    "      - str              -> .lower()\n",
    "      - anything else    -> str(x).lower()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(getattr(x, \"name\")).lower()\n",
    "    except Exception:\n",
    "        return str(x).lower()\n",
    "\n",
    "def _ends_with_3x3(module: nn.Module) -> bool:\n",
    "    \"\"\"True if the last Conv2d in `module` has a 3×3 kernel.\"\"\"\n",
    "    last_conv = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_conv = m\n",
    "    return (last_conv is not None) and (tuple(getattr(last_conv, \"kernel_size\", (0, 0))) == (3, 3))\n",
    "\n",
    "def _insert_post3x3_if_needed(ch: int) -> nn.Sequential:\n",
    "    \"\"\"Conv(3×3) + BN + SiLU block.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(ch),\n",
    "        nn.SiLU(inplace=True),\n",
    "    )\n",
    "\n",
    "# HF-first timm model IDs (we try these first, then the plain timm IDs)\n",
    "TIMM_NAME_MAP: _Dict[str, _List[str]] = {\n",
    "    \"dpn68\":         [\"dpn68.mx_in1k\", \"dpn68\"],\n",
    "    \"coatnet0\":      [\"coatnet_0_rw_224.sw_in1k\", \"coatnet_0_rw_224\"],\n",
    "    \"convnext_nano\": [\"convnext_nano.in12k_ft_in1k\", \"convnext_nano\"],\n",
    "    \"hrnet32\":       [\"hrnet_w32.ms_in1k\", \"hrnet_w32\"],\n",
    "}\n",
    "\n",
    "def _maybe_create_timm(candidates: _List[str]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Try to create a timm model using HF variants first, then base IDs.\n",
    "    We DO NOT pass 'img_size' here (some models don't accept it).\n",
    "    We request feature-only (num_classes=0, global_pool=\"\").\n",
    "    \"\"\"\n",
    "    if not _HAS_TIMM:\n",
    "        raise RuntimeError(\"timm is not installed but a timm backbone was requested.\")\n",
    "    kwargs = dict(num_classes=0, global_pool=\"\")\n",
    "    last_err = None\n",
    "    for model_name in candidates:\n",
    "        # 1) pretrained=True\n",
    "        try:\n",
    "            return timm.create_model(model_name, pretrained=True, **kwargs)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "        # 2) pretrained=False\n",
    "        try:\n",
    "            return timm.create_model(model_name, pretrained=False, **kwargs)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Could not create any of timm models {candidates}. Last error: {last_err}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Image backbones\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "class TimmBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    timm forward_features → (B,C,H,W) or (B,Tokens,C).\n",
    "    If 4D: [optional post-3×3] → GAP → Dropout → (B,C)\n",
    "    If 3D: take CLS/token 0 → Dropout → (B,C)\n",
    "    \"\"\"\n",
    "    def __init__(self, key_name: str, img_size: int, insert_post3x3: bool = True, p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        key_name = key_name.lower()\n",
    "        if key_name not in TIMM_NAME_MAP:\n",
    "            raise ValueError(f\"Unknown timm backbone key '{key_name}'. Options: {list(TIMM_NAME_MAP.keys())}\")\n",
    "\n",
    "        # Create the timm model (HF-first)\n",
    "        self.m = _maybe_create_timm(TIMM_NAME_MAP[key_name])\n",
    "\n",
    "        # Probe features\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, int(img_size), int(img_size))\n",
    "            f = self.m.forward_features(dummy)\n",
    "            self._feat_is_4d = (f.ndim == 4)\n",
    "            if self._feat_is_4d:\n",
    "                ch = int(f.shape[1])\n",
    "                # Register post-3×3 FIRST, so repr() shows the correct order\n",
    "                self.post3x3 = _insert_post3x3_if_needed(ch) if (insert_post3x3 and not _ends_with_3x3(self.m)) else nn.Identity()\n",
    "                # Now register GAP and Dropout (classification \"head\")\n",
    "                self.gap  = nn.AdaptiveAvgPool2d((1, 1))\n",
    "                self.drop = nn.Dropout(p=p_drop)\n",
    "                self.out_dim = ch\n",
    "            elif f.ndim == 3:\n",
    "                # Transformer-like features: no post-3×3, just Dropout on CLS\n",
    "                self.post3x3 = nn.Identity()\n",
    "                self.gap  = nn.Identity()   # not used for 3D features\n",
    "                self.drop = nn.Dropout(p=p_drop)\n",
    "                self.out_dim = int(f.shape[-1])\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected feature shape from timm model: {tuple(f.shape)}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.m.forward_features(x)\n",
    "        if self._feat_is_4d:\n",
    "            f = self.post3x3(f)\n",
    "            f = self.gap(f).flatten(1)\n",
    "            f = self.drop(f)\n",
    "            return f\n",
    "        # token features: take class token (index 0)\n",
    "        f = f[:, 0]\n",
    "        f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "class TorchvisionBackbone(nn.Module):\n",
    "    \"\"\"ResNet18 / DenseNet121 / MobileNetV2 with optional post-3×3 → GAP → Dropout.\"\"\"\n",
    "    def __init__(self, which: \"NETWORK\", p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        import torchvision.models as tvm\n",
    "\n",
    "        if which == NETWORK.resnet18:\n",
    "            try: m = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            except Exception: m = tvm.resnet18(weights=None)\n",
    "            self.encoder = nn.Sequential(*(list(m.children())[:-2])); ch = m.fc.in_features\n",
    "        elif which == NETWORK.densenet121:\n",
    "            try: m = tvm.densenet121(weights=tvm.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "            except Exception: m = tvm.densenet121(weights=None)\n",
    "            self.encoder = m.features; ch = m.classifier.in_features\n",
    "        elif which == NETWORK.mobilenet_v2:\n",
    "            try: m = tvm.mobilenet_v2(weights=tvm.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "            except Exception: m = tvm.mobilenet_v2(weights=None)\n",
    "            self.encoder = m.features; ch = m.classifier[1].in_features\n",
    "        else:\n",
    "            raise ValueError(which)\n",
    "\n",
    "        # Register post-3×3 FIRST (so it prints before GAP/Dropout)\n",
    "        self.post3x3 = _insert_post3x3_if_needed(ch) if not _ends_with_3x3(self.encoder) else nn.Identity()\n",
    "        # Classification head\n",
    "        self.gap  = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = int(ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.encoder(x)\n",
    "        f = self.post3x3(f)\n",
    "        f = self.gap(f).flatten(1)\n",
    "        f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "class TorchvisionVGGBackbone(nn.Module):\n",
    "    \"\"\"VGG11/13/16/19 (BN) features + optional post-3×3 → GAP → Dropout.\"\"\"\n",
    "    def __init__(self, which: \"NETWORK\", p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        import torchvision.models as tvm\n",
    "\n",
    "        vgg_map = {\n",
    "            NETWORK.vgg11: (tvm.vgg11_bn,  getattr(tvm, \"VGG11_BN_Weights\", None)),\n",
    "            NETWORK.vgg13: (tvm.vgg13_bn,  getattr(tvm, \"VGG13_BN_Weights\", None)),\n",
    "            NETWORK.vgg16: (tvm.vgg16_bn,  getattr(tvm, \"VGG16_BN_Weights\", None)),\n",
    "            NETWORK.vgg19: (tvm.vgg19_bn,  getattr(tvm, \"VGG19_BN_Weights\", None)),\n",
    "        }\n",
    "        ctor, weights_enum = vgg_map[which]\n",
    "        try:\n",
    "            if weights_enum is not None:\n",
    "                weights = getattr(weights_enum, \"IMAGENET1K_V1\")\n",
    "                m = ctor(weights=weights)\n",
    "            else:\n",
    "                m = ctor(weights=None)\n",
    "        except Exception:\n",
    "            m = ctor(weights=None)\n",
    "\n",
    "        self.encoder = m.features\n",
    "\n",
    "        # find output channels from last Conv2d in the features\n",
    "        ch = None\n",
    "        for mod in self.encoder.modules():\n",
    "            if isinstance(mod, nn.Conv2d):\n",
    "                ch = mod.out_channels\n",
    "        if ch is None:\n",
    "            raise RuntimeError(\"VGG out channels not found\")\n",
    "\n",
    "        # Register post-3×3 FIRST (so it prints before GAP/Dropout)\n",
    "        self.post3x3 = _insert_post3x3_if_needed(ch) if not _ends_with_3x3(self.encoder) else nn.Identity()\n",
    "        # Classification head\n",
    "        self.gap  = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = int(ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.encoder(x)\n",
    "        f = self.post3x3(f)\n",
    "        f = self.gap(f).flatten(1)\n",
    "        f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Text helpers\n",
    "# ------------------------------------------------------------------\n",
    "def _resolve_hf_local_dir(local_dir: _Optional[str]) -> _Optional[str]:\n",
    "    if not local_dir or not os.path.isdir(local_dir):\n",
    "        return None\n",
    "    needed = {\"config.json\", \"pytorch_model.bin\", \"model.safetensors\", \"rust_model.ot\"}\n",
    "    files = set(os.listdir(local_dir))\n",
    "    if files & needed:\n",
    "        return local_dir\n",
    "    snaps = os.path.join(local_dir, \"snapshots\")\n",
    "    refs  = os.path.join(local_dir, \"refs\", \"main\")\n",
    "    if os.path.isdir(snaps):\n",
    "        if os.path.isfile(refs):\n",
    "            with open(refs, \"r\") as f:\n",
    "                commit = f.read().strip()\n",
    "            cand = os.path.join(snaps, commit)\n",
    "            if os.path.isdir(cand):\n",
    "                return cand\n",
    "        subdirs = [os.path.join(snaps, d) for d in os.listdir(snaps) if os.path.isdir(os.path.join(snaps, d))]\n",
    "        if subdirs:\n",
    "            subdirs.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return subdirs[0]\n",
    "    return None\n",
    "\n",
    "@_dataclass\n",
    "class TextFTPolicy:\n",
    "    mode: Literal[\"authors_default\", \"freeze_all\", \"last_n\", \"train_all\"] = \"authors_default\"\n",
    "    last_n: int = 1\n",
    "\n",
    "def configure_text_finetune(bert: nn.Module, policy: TextFTPolicy) -> None:\n",
    "    def set_all(req: bool):\n",
    "        for p in bert.parameters():\n",
    "            p.requires_grad = req\n",
    "\n",
    "    if policy.mode == \"authors_default\":\n",
    "        if hasattr(bert, \"embeddings\"):\n",
    "            for p in bert.embeddings.parameters():\n",
    "                p.requires_grad = False\n",
    "        if hasattr(bert, \"pooler\"):\n",
    "            for p in bert.pooler.parameters():\n",
    "                p.requires_grad = False\n",
    "        enc = getattr(getattr(bert, \"encoder\", None), \"layer\", [])\n",
    "        for i, layer in enumerate(enc):\n",
    "            req = (i == len(enc) - 1)\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = req\n",
    "        return\n",
    "\n",
    "    if policy.mode == \"freeze_all\":\n",
    "        set_all(False); return\n",
    "    if policy.mode == \"train_all\":\n",
    "        set_all(True); return\n",
    "    if policy.mode == \"last_n\":\n",
    "        if hasattr(bert, \"embeddings\"):\n",
    "            for p in bert.embeddings.parameters():\n",
    "                p.requires_grad = False\n",
    "        if hasattr(bert, \"pooler\"):\n",
    "            for p in bert.pooler.parameters():\n",
    "                p.requires_grad = False\n",
    "        enc = getattr(bert, \"encoder\", None)\n",
    "        total = len(enc.layer)\n",
    "        cutoff = max(0, total - policy.last_n)\n",
    "        for i, layer in enumerate(enc.layer):\n",
    "            req = (i >= cutoff)\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = req\n",
    "        return\n",
    "    raise ValueError(policy)\n",
    "\n",
    "def build_text_encoder_from_cfg(cfg) -> _Optional[nn.Module]:\n",
    "    if cfg.modality not in (MOD.txt, MOD.multimodal):\n",
    "        return None\n",
    "    local_dir = getattr(cfg, \"hf_local_dir\", None)\n",
    "    offline   = bool(getattr(cfg, \"hf_offline\", True))\n",
    "    # >>> NEW: read trust_remote_code flag from cfg <<<\n",
    "    trust_remote = bool(getattr(cfg, \"hf_trust_remote_code\", False))\n",
    "\n",
    "    if local_dir:\n",
    "        resolved = _resolve_hf_local_dir(local_dir)\n",
    "        if resolved and os.path.isdir(resolved):\n",
    "            return AutoModel.from_pretrained(\n",
    "                resolved,\n",
    "                local_files_only=True,\n",
    "                trust_remote_code=trust_remote,   # <<< NEW\n",
    "            )\n",
    "        if offline:\n",
    "            raise RuntimeError(f\"cfg.hf_local_dir='{local_dir}' is not a valid HF snapshot.\")\n",
    "    if offline:\n",
    "        raise RuntimeError(\"Offline but no valid local text snapshot found.\")\n",
    "    return AutoModel.from_pretrained(\n",
    "        cfg.hf_text_model_name,\n",
    "        trust_remote_code=trust_remote,           # <<< NEW\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Multimodal model\n",
    "# ------------------------------------------------------------------\n",
    "class MultimodalNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Image + Text classifier:\n",
    "      - Image: (backbone → [optional post-3×3] → GAP → Dropout) → Linear → ReLU → Linear\n",
    "      - Text : HF encoder (pooled) → Linear → ReLU → Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: \"RunConfig\", text_ft: TextFTPolicy = TextFTPolicy()):\n",
    "        super().__init__()\n",
    "        self.modality  = cfg.modality\n",
    "        self.hidden    = int(cfg.hidden_dim)\n",
    "        self.n_classes = int(cfg.n_classes)\n",
    "        self.img_enc   = None\n",
    "        self.txt_enc   = None\n",
    "\n",
    "        # ----- Image encoder -----\n",
    "        img_out = 0\n",
    "        if cfg.modality in (MOD.img, MOD.multimodal):\n",
    "            net_name = _net_name_like(cfg.network)\n",
    "            if net_name in (\"resnet18\", \"densenet121\", \"mobilenet_v2\"):\n",
    "                self.img_enc = TorchvisionBackbone(cfg.network, p_drop=0.3)\n",
    "            elif net_name in (\"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\"):\n",
    "                self.img_enc = TorchvisionVGGBackbone(cfg.network, p_drop=0.3)\n",
    "            elif net_name in TIMM_NAME_MAP:\n",
    "                self.img_enc = TimmBackbone(net_name, img_size=int(cfg.img_size), insert_post3x3=True, p_drop=0.3)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported backbone: {cfg.network} (name='{net_name}')\")\n",
    "            img_out = int(getattr(self.img_enc, \"out_dim\"))\n",
    "\n",
    "        # ----- Text encoder -----\n",
    "        txt_hid = 0\n",
    "        if cfg.modality in (MOD.txt, MOD.multimodal):\n",
    "            self.txt_enc = build_text_encoder_from_cfg(cfg)\n",
    "            if self.txt_enc is None:\n",
    "                raise RuntimeError(\"Could not build text encoder from cfg.\")\n",
    "            configure_text_finetune(self.txt_enc, text_ft)\n",
    "            txt_hid = int(self.txt_enc.config.hidden_size)\n",
    "\n",
    "        # ----- Heads -----\n",
    "        if self.img_enc is not None:\n",
    "            self.img_proj = nn.Linear(img_out, self.hidden)\n",
    "        if self.txt_enc is not None:\n",
    "            self.txt_proj = nn.Linear(txt_hid, self.hidden)\n",
    "\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "        self.mid  = nn.Linear(self.hidden, self.hidden)\n",
    "        self.cls  = nn.Linear(self.hidden, self.n_classes)\n",
    "\n",
    "    def _head(self, feat: torch.Tensor) -> torch.Tensor:\n",
    "        return self.cls(self.mid(self.act(feat)))\n",
    "\n",
    "    def forward(self, image=None, text=None):\n",
    "        out = {\"logits_img\": None, \"z_img\": None, \"logits_txt\": None, \"z_txt\": None}\n",
    "        if (self.img_enc is not None) and (image is not None):\n",
    "            f_img = self.img_enc(image)                 # (B, C)\n",
    "            z_img = self.mid(self.act(self.img_proj(f_img)))\n",
    "            out[\"z_img\"] = z_img\n",
    "            out[\"logits_img\"] = self.cls(z_img)\n",
    "        if (self.txt_enc is not None) and (text is not None):\n",
    "            enc = self.txt_enc(input_ids=text[\"input_ids\"], attention_mask=text[\"attention_mask\"])\n",
    "            pooled = enc.pooler_output if getattr(enc, \"pooler_output\", None) is not None else enc.last_hidden_state[:, 0]\n",
    "            z_txt = self.mid(self.act(self.txt_proj(pooled)))\n",
    "            out[\"z_txt\"] = z_txt\n",
    "            out[\"logits_txt\"] = self.cls(z_txt)\n",
    "        return out\n",
    "\n",
    "    # image/text only entrypoints (used by CAM/eval helpers)\n",
    "    def forward_image(self, image):\n",
    "        return self.forward(image=image, text=None)[\"logits_img\"]\n",
    "\n",
    "    def forward_text(self, text):\n",
    "        return self.forward(image=None, text=text)[\"logits_txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1a30d-b624-4f4b-ae91-5c291e86ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses (CE-image + CE-text +  cosine (img, text) + supervised NT-Xent(img, text)\n",
    "\n",
    "@_dataclass2\n",
    "class LossCfg:\n",
    "    use_align_losses: bool = True\n",
    "    align_lambda: float = 0.5\n",
    "    contrastive_temperature: float = 0.5\n",
    "\n",
    "_ce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "_cos = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "\n",
    "def onehot_to_idx(y_onehot: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.argmax(y_onehot, dim=1)\n",
    "\n",
    "def sup_nt_xent(z1: torch.Tensor, z2: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    z1 = F.normalize(z1, dim=-1); z2 = F.normalize(z2, dim=-1)\n",
    "    B = z1.size(0)\n",
    "    logits_12 = (z1 @ z2.t()) / temperature\n",
    "    logits_21 = (z2 @ z1.t()) / temperature\n",
    "    targets = torch.arange(B, device=z1.device)\n",
    "    return 0.5 * (F.cross_entropy(logits_12, targets) + F.cross_entropy(logits_21, targets))\n",
    "\n",
    "def compute_total_loss(out: Dict[str, Optional[torch.Tensor]], batch: Dict[str, torch.Tensor],\n",
    "                       cfg: RunConfig, lcfg: LossCfg) -> (torch.Tensor, Dict[str, float]):\n",
    "    dev = None\n",
    "    for v in out.values():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            dev = v.device; break\n",
    "    if dev is None: dev = batch[\"y_onehot\"].device\n",
    "    y_idx = onehot_to_idx(batch[\"y_onehot\"].to(dev))\n",
    "    total = torch.zeros((), device=dev)\n",
    "    scalars = {\"ce_img\": 0.0, \"ce_txt\": 0.0, \"cosine\": 0.0, \"contrastive\": 0.0}\n",
    "    if out[\"logits_img\"] is not None:\n",
    "        ce_img = _ce(out[\"logits_img\"], y_idx); total = total + ce_img; scalars[\"ce_img\"] = float(ce_img.detach().item())\n",
    "    if out[\"logits_txt\"] is not None:\n",
    "        ce_txt = _ce(out[\"logits_txt\"], y_idx); total = total + ce_txt; scalars[\"ce_txt\"] = float(ce_txt.detach().item())\n",
    "    if lcfg.use_align_losses and (out[\"z_img\"] is not None) and (out[\"z_txt\"] is not None):\n",
    "        target = torch.ones(out[\"z_img\"].size(0), device=dev)\n",
    "        cos = _cos(out[\"z_img\"], out[\"z_txt\"], target); total = total + cos\n",
    "        con = sup_nt_xent(out[\"z_img\"], out[\"z_txt\"], lcfg.contrastive_temperature)\n",
    "        total = total + lcfg.align_lambda * con\n",
    "        scalars[\"cosine\"] = float(cos.detach().item())\n",
    "        scalars[\"contrastive\"] = float(con.detach().item())\n",
    "    scalars[\"total\"] = float(total.detach().item())\n",
    "    return total, scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a0fb1-7711-48f6-b3cc-efb2e16b6191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def logits_to_probs(logits: torch.Tensor) -> np.ndarray:\n",
    "    return F.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_logits(logits: torch.Tensor, y_onehot: torch.Tensor, class_names: Tuple[str,...]=(\"normal\",\"tb\")) -> Dict[str, object]:\n",
    "    probs = logits_to_probs(logits)\n",
    "    y_true_idx = torch.argmax(y_onehot, dim=1).cpu().numpy()\n",
    "    y_pred_idx = np.argmax(probs, axis=1)\n",
    "    cm_std = confusion_matrix(y_true_idx, y_pred_idx, labels=[0,1])\n",
    "    TP = int(((y_true_idx==1)&(y_pred_idx==1)).sum())\n",
    "    FP = int(((y_true_idx==0)&(y_pred_idx==1)).sum())\n",
    "    FN = int(((y_true_idx==1)&(y_pred_idx==0)).sum())\n",
    "    TN = int(((y_true_idx==0)&(y_pred_idx==0)).sum())\n",
    "    cm_clin = np.array([[TP, FP], [FN, TN]], dtype=int)\n",
    "    try:\n",
    "        auc_macro = roc_auc_score(y_true_idx, probs, multi_class=\"ovr\", average=\"macro\")\n",
    "        auc_per_class = roc_auc_score(y_true_idx, probs, multi_class=\"ovr\", average=None)\n",
    "    except ValueError:\n",
    "        auc_macro, auc_per_class = float(\"nan\"), [float(\"nan\")] * probs.shape[1]\n",
    "    report = \"\"  # optional\n",
    "    return {\n",
    "        \"confusion_pred_rows_true_cols\": cm_std,\n",
    "        \"confusion_clinician_TPFP_FN_TN\": cm_clin,\n",
    "        \"report\": report,\n",
    "        \"auc_macro\": float(auc_macro) if auc_macro == auc_macro else None,\n",
    "        \"auc_per_class\": [float(a) if a == a else None for a in auc_per_class],\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_dataloader_classifier(model: nn.Module, loader: DataLoader, modality: MOD) -> Dict[str, float]:\n",
    "    device = next(model.parameters()).device\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    for batch in loader:\n",
    "        if batch[\"y_onehot\"].numel() == 0: continue\n",
    "        img, txt = batch.get(\"image\"), batch.get(\"text\")\n",
    "        if img is not None: img = img.to(device, non_blocking=True)\n",
    "        if txt is not None: txt = {k: v.to(device, non_blocking=True) for k, v in txt.items()}\n",
    "        out = model(img, txt)\n",
    "        # Use image head for classification when available, else fallback to text\n",
    "        logits = out[\"logits_img\"] if out[\"logits_img\"] is not None else out[\"logits_txt\"]\n",
    "        probs1 = F.softmax(logits.float(), dim=1)[:,1].detach().cpu().numpy()\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        y_idx = torch.argmax(batch[\"y_onehot\"], dim=1).detach().cpu().numpy()\n",
    "        y_true.extend(y_idx.tolist()); y_pred.extend(preds.tolist()); y_prob.extend(probs1.tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "    TP = int(((y_true==1)&(y_pred==1)).sum())\n",
    "    FP = int(((y_true==0)&(y_pred==1)).sum())\n",
    "    FN = int(((y_true==1)&(y_pred==0)).sum())\n",
    "    TN = int(((y_true==0)&(y_pred==0)).sum())\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred) if y_true.size else 0.0\n",
    "    sens = recall_score(y_true, y_pred, pos_label=1) if y_true.size else 0.0\n",
    "    spec = TN / (TN + FP) if (TN + FP) else 0.0\n",
    "    prec = precision_score(y_true, y_pred, pos_label=1) if (TP + FP) else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) else 0.0\n",
    "    f1s = f1_score(y_true, y_pred, pos_label=1) if y_true.size else 0.0\n",
    "    youden = sens + spec - 1.0\n",
    "    mcc = matthews_corrcoef(y_true, y_pred) if y_true.size else 0.0\n",
    "    kappa = cohen_kappa_score(y_true, y_pred) if y_true.size else 0.0\n",
    "    try: roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError: roc_auc = float(\"nan\")\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "        \"balanced_accuracy\": float(bal_acc), \"sensitivity\": float(sens), \"specificity\": float(spec),\n",
    "        \"precision\": float(prec), \"NPV\": float(npv), \"F1_score\": float(f1s), \"Youden_J\": youden,\n",
    "        \"MCC\": float(mcc), \"Cohen_Kappa\": float(kappa), \"ROC_AUC\": float(roc_auc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da374791-84eb-49c9-8b2e-3f645bf0ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop (save best by highest validation MCC\n",
    "\n",
    "def _ckpt_stem(cfg: \"RunConfig\") -> str:\n",
    "    return f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "\n",
    "def _ckpt_paths(cfg: \"RunConfig\") -> Dict[str, str]:\n",
    "    \"\"\"All artifacts WITHOUT any _dimension_### suffix.\"\"\"\n",
    "    stem = _ckpt_stem(cfg)\n",
    "    base = cfg.out_dir\n",
    "    return {\n",
    "        \"best_pt\": os.path.join(base, f\"best_{stem}_val_loss.pt\"),\n",
    "        \"curves_png\": os.path.join(base, f\"best_{stem}_val_loss_curves.png\"),\n",
    "        \"curves_csv\": os.path.join(base, f\"best_{stem}_val_loss_history.csv\"),\n",
    "        \"curves_png_full\": os.path.join(base, f\"best_{stem}_val_loss_curves_full.png\"),\n",
    "        \"curves_csv_full\": os.path.join(base, f\"best_{stem}_val_loss_history_full.csv\"),\n",
    "        \"log_json\": os.path.join(base, f\"best_{stem}_val_loss_log.json\"),\n",
    "        \"mcc_png\": os.path.join(base, f\"best_{stem}_val_mcc_curve.png\"),\n",
    "    }\n",
    "\n",
    "# (Optional) alias for compatibility if other code expects ckpt_paths(...)\n",
    "ckpt_paths = _ckpt_paths\n",
    "\n",
    "# utils & printing\n",
    "\n",
    "def _count_params(m: nn.Module) -> Tuple[int, int, int]:\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    train = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    frozen = total - train\n",
    "    return total, train, frozen\n",
    "\n",
    "def _fmt_params(n: int) -> str:\n",
    "    return f\"{n/1e6:.2f}M\" if n >= 1e6 else (f\"{n/1e3:.2f}k\" if n >= 1e3 else str(n))\n",
    "\n",
    "def _print_model_overview_full(model: nn.Module, cfg: \"RunConfig\") -> None:\n",
    "    print(\"\\n========== Model: FULL ARCHITECTURE ==========\")\n",
    "    print(model)\n",
    "    print(\"==============================================\")\n",
    "    print(f\"Modality   : {cfg.modality.name}\")\n",
    "    print(f\"Backbone   : {cfg.network.name}\")\n",
    "    print(f\"Hidden dim : {cfg.hidden_dim} | Classes: {cfg.n_classes}\")\n",
    "    if hasattr(model, \"img_enc\") and (model.img_enc is not None):\n",
    "        img_out = getattr(model.img_enc, \"out_dim\", None)\n",
    "        print(f\"[Image] out_dim={img_out}\")\n",
    "    if hasattr(model, \"txt_enc\") and (model.txt_enc is not None):\n",
    "        hid = getattr(getattr(model.txt_enc, \"config\", None), \"hidden_size\", None)\n",
    "        print(f\"[Text ] hidden_size={hid}\")\n",
    "    tot, trn, frz = _count_params(model)\n",
    "    print(f\"Parameters : total={_fmt_params(tot)} | trainable={_fmt_params(trn)} | frozen={_fmt_params(frz)}\")\n",
    "    print(\"==============================================\\n\", flush=True)\n",
    "\n",
    "def _save_curves_png_csv(history: Dict[str, List[float]], out_png: str, out_csv: str) -> None:\n",
    "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "    df = pd.DataFrame(history); df.to_csv(out_csv, index=False)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    if \"train_total\" in df and \"val_total\" in df:\n",
    "        ax.plot(df[\"epoch\"], df[\"train_total\"], label=\"train_total\", linewidth=2)\n",
    "        ax.plot(df[\"epoch\"], df[\"val_total\"],   label=\"val_total\",   linewidth=2, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss\"); ax.set_title(\"Training / Validation Loss\")\n",
    "    ax.legend(loc=\"upper right\"); ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout(); fig.savefig(out_png, dpi=200); plt.close(fig)\n",
    "\n",
    "def _save_mcc_curve(history: Dict[str, List[float]], out_png: str) -> None:\n",
    "    try:\n",
    "        epochs = history[\"epoch\"]; val_mcc = history[\"val_MCC\"]\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        ax.plot(epochs, val_mcc, linewidth=2)\n",
    "        ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Validation MCC\"); ax.set_title(\"Validation MCC\")\n",
    "        ax.grid(True, alpha=0.3); fig.tight_layout(); fig.savefig(out_png, dpi=200); plt.close(fig)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ------------------ collate for multimodal ------------------\n",
    "\n",
    "def make_mm_collate(modality: \"MOD\", n_classes: int) -> Callable:\n",
    "    def _stack(xs: List[torch.Tensor]) -> torch.Tensor:\n",
    "        return torch.stack(xs, dim=0) if len(xs) > 0 else torch.empty(0)\n",
    "    def _collate(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        imgs, ylist, ids_list, attn_list = [], [], [], []\n",
    "        for s in batch:\n",
    "            img, txt, y = s.get(\"image\"), s.get(\"text\"), s.get(\"y_onehot\")\n",
    "            if (img is None) or (txt is None) or (txt.get(\"input_ids\") is None) or (y is None):\n",
    "                continue\n",
    "            imgs.append(img)\n",
    "            ids_list.append(txt[\"input_ids\"])\n",
    "            attn_list.append(txt[\"attention_mask\"])\n",
    "            ylist.append(y)\n",
    "        if not ylist:\n",
    "            return {\"y_onehot\": torch.zeros(0, n_classes, dtype=torch.float32)}\n",
    "        return {\n",
    "            \"image\": _stack(imgs),\n",
    "            \"text\": {\"input_ids\": _stack(ids_list), \"attention_mask\": _stack(attn_list)},\n",
    "            \"y_onehot\": _stack(ylist),\n",
    "        }\n",
    "    return _collate\n",
    "\n",
    "def wrap_loader_with_collate(loader: DataLoader, modality: \"MOD\", n_classes: int) -> DataLoader:\n",
    "    collate_fn = make_mm_collate(modality, n_classes)\n",
    "    return DataLoader(\n",
    "        loader.dataset,\n",
    "        batch_size=loader.batch_size,\n",
    "        sampler=loader.sampler,\n",
    "        shuffle=False if loader.sampler is not None else True,\n",
    "        num_workers=loader.num_workers,\n",
    "        pin_memory=loader.pin_memory,\n",
    "        drop_last=loader.drop_last,\n",
    "        persistent_workers=getattr(loader, \"persistent_workers\", False),\n",
    "        prefetch_factor=getattr(loader, \"prefetch_factor\", None),\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "# ------------------ runtime cfg ------------------\n",
    "\n",
    "@dataclass\n",
    "class TrainRuntimeCfg:\n",
    "    amp: bool = True\n",
    "    patience: int = 10\n",
    "    grad_clip_norm: Optional[float] = 1.0\n",
    "    amp_dtype: torch.dtype = torch.float16\n",
    "    log_every: int = 1\n",
    "\n",
    "# ------------------ one epoch ------------------\n",
    "\n",
    "def run_epoch(model: nn.Module, loader: DataLoader, cfg: \"RunConfig\", lcfg: \"LossCfg\",\n",
    "              *, train: bool, optim: Optional[torch.optim.Optimizer], trcfg: TrainRuntimeCfg) -> Dict[str, float]:\n",
    "    device = next(model.parameters()).device\n",
    "    use_amp = trcfg.amp and (device.type == \"cuda\")\n",
    "    model.train(mode=train)\n",
    "    sums = {\"total\": 0.0, \"ce_img\": 0.0, \"ce_txt\": 0.0, \"cosine\": 0.0, \"contrastive\": 0.0}\n",
    "    n_samples = 0\n",
    "\n",
    "    scaler = (torch.amp.GradScaler(\"cuda\") if (train and use_amp and hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"))\n",
    "              else (torch.cuda.amp.GradScaler() if (train and use_amp and hasattr(torch.cuda, \"amp\")) else None))\n",
    "\n",
    "    for batch in loader:\n",
    "        if batch[\"y_onehot\"].numel() == 0:\n",
    "            continue\n",
    "        img = batch[\"image\"].to(device, non_blocking=True)\n",
    "        txt = {k: v.to(device, non_blocking=True) for k, v in batch[\"text\"].items()}\n",
    "        bsz = batch[\"y_onehot\"].size(0); n_samples += bsz\n",
    "\n",
    "        autocast_ctx = (torch.autocast(\"cuda\", dtype=trcfg.amp_dtype) if use_amp else nullcontext())\n",
    "        with torch.set_grad_enabled(train), autocast_ctx:\n",
    "            out = model(img, txt)\n",
    "            loss, scalars = compute_total_loss(out, batch, cfg, lcfg)\n",
    "\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                if trcfg.grad_clip_norm is not None:\n",
    "                    scaler.unscale_(optim); nn.utils.clip_grad_norm_(model.parameters(), trcfg.grad_clip_norm)\n",
    "                scaler.step(optim); scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if trcfg.grad_clip_norm is not None:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), trcfg.grad_clip_norm)\n",
    "                optim.step()\n",
    "\n",
    "        for k in sums.keys():\n",
    "            sums[k] += float(scalars.get(k, 0.0)) * bsz\n",
    "\n",
    "    eps = 1e-12\n",
    "    return {k: (v / max(n_samples, eps)) for k, v in sums.items()}\n",
    "\n",
    "# ------------------ fit (select best by validation MCC) ------------------\n",
    "\n",
    "def fit_with_val_loss_checkpointing(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    cfg: \"RunConfig\",\n",
    "    lcfg: Optional[\"LossCfg\"] = None,\n",
    "    trcfg: TrainRuntimeCfg = TrainRuntimeCfg(),\n",
    ") -> Dict[str, object]:\n",
    "\n",
    "    assert cfg.modality == MOD.multimodal, \"This function trains the MULTIMODAL model.\"\n",
    "    if lcfg is None:\n",
    "        lcfg = LossCfg(use_align_losses=cfg.use_align_losses,\n",
    "                       align_lambda=cfg.align_lambda,\n",
    "                       contrastive_temperature=cfg.contrastive_temperature)\n",
    "\n",
    "    train_loader = wrap_loader_with_collate(train_loader, cfg.modality, cfg.n_classes)\n",
    "    valid_loader = wrap_loader_with_collate(valid_loader, cfg.modality, cfg.n_classes)\n",
    "\n",
    "    _print_model_overview_full(model, cfg)\n",
    "    print(f\"[ALIGN] λ={getattr(cfg, 'align_lambda', None)}  |  τ={getattr(cfg, 'contrastive_temperature', None)}\", flush=True)\n",
    "\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    paths = _ckpt_paths(cfg) \n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    history = {\"epoch\": [], \"train_total\": [], \"val_total\": [], \"val_MCC\": []}\n",
    "\n",
    "    best_mcc = -1.0\n",
    "    best_epoch = 0\n",
    "    bad_epochs = 0\n",
    "    improvements = 0\n",
    "    stopped_early = False\n",
    "    last_va_total = None\n",
    "    last_va_mcc = None\n",
    "\n",
    "    for ep in range(1, int(cfg.epochs) + 1):\n",
    "        tr = run_epoch(model, train_loader, cfg, lcfg, train=True,  optim=optim, trcfg=trcfg)\n",
    "        va = run_epoch(model, valid_loader, cfg, lcfg, train=False, optim=None, trcfg=trcfg)\n",
    "        last_va_total = va[\"total\"]\n",
    "\n",
    "        val_metrics = evaluate_dataloader_classifier(model, valid_loader, cfg.modality)\n",
    "        last_va_mcc = float(val_metrics[\"MCC\"])\n",
    "\n",
    "        best_str = (\"-∞\" if best_mcc < 0 else f\"{best_mcc:.4f}\")\n",
    "        streak_str = f\"{bad_epochs}/{trcfg.patience}\"\n",
    "        print(f\"[{ep:03d}/{cfg.epochs} | best_val_MCC={best_str} | no_improve={streak_str}] \"\n",
    "              f\"Train: total={tr['total']:.4f}  |  Valid: total={va['total']:.4f}  MCC={last_va_mcc:.4f}\",\n",
    "              flush=True)\n",
    "\n",
    "        history[\"epoch\"].append(ep)\n",
    "        history[\"train_total\"].append(tr[\"total\"])\n",
    "        history[\"val_total\"].append(va[\"total\"])\n",
    "        history[\"val_MCC\"].append(last_va_mcc)\n",
    "\n",
    "        if last_va_mcc > best_mcc + 1e-8:\n",
    "            print(f\"  ↑ val_MCC improved: {best_mcc:.6f} → {last_va_mcc:.6f}  (saving checkpoint; reset no_improve=0)\", flush=True)\n",
    "            best_mcc = last_va_mcc; best_epoch = ep; bad_epochs = 0; improvements += 1\n",
    "            torch.save(model.state_dict(), paths[\"best_pt\"])\n",
    "            print(f\"  ↳ saved: {paths['best_pt']}\", flush=True)\n",
    "            with open(paths[\"log_json\"], \"w\") as f:\n",
    "                json.dump({\"best_val_MCC\": float(best_mcc), \"best_epoch\": int(best_epoch)}, f, indent=2)\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            print(f\"  ↳ no MCC improvement (no_improve={bad_epochs}/{trcfg.patience})\", flush=True)\n",
    "            if bad_epochs >= trcfg.patience:\n",
    "                print(f\"EARLY STOP: validation MCC did not improve for {bad_epochs} consecutive epochs \"\n",
    "                      f\"(patience={trcfg.patience}). Stopping at epoch {ep}.\",\n",
    "                      flush=True)\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "    _save_curves_png_csv(history, paths[\"curves_png\"], paths[\"curves_csv\"])\n",
    "    _save_curves_png_csv(history, paths[\"curves_png_full\"], paths[\"curves_csv_full\"])\n",
    "    _save_mcc_curve(history, paths[\"mcc_png\"])\n",
    "\n",
    "    try:\n",
    "        state = torch.load(paths[\"best_pt\"], map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=True)\n",
    "        print(f\"Restored best (by MCC) from {paths['best_pt']}  |  best_epoch={best_epoch}  best_val_MCC={best_mcc:.6f}\",\n",
    "              flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not reload best checkpoint ({e}).\", flush=True)\n",
    "\n",
    "    return {\n",
    "        \"best_val_MCC\": float(best_mcc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"improvements\": int(improvements),\n",
    "        \"early_stopped\": bool(stopped_early),\n",
    "        \"last_epoch_val_total\": float(last_va_total) if last_va_total is not None else None,\n",
    "        \"last_epoch_val_MCC\": float(last_va_mcc) if last_va_mcc is not None else None,\n",
    "        \"paths\": paths,\n",
    "        \"history\": history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1d221-ec09-484b-82af-966f04102120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN: Multimodal-only grid over λ and τ (saves best per run by MCC)\n",
    "\n",
    "# Base config (MULTIMODAL ONLY)\n",
    "cfg = RunConfig(\n",
    "    modality=MOD.multimodal,\n",
    "    network=NETWORK.vgg11,\n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-4,\n",
    "    img_size=224,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset_root=\"/dataset\",\n",
    "    csv_train=\"/label_train.csv\",\n",
    "    csv_valid=\"/label_valid.csv\",\n",
    "    csv_test =\"/label_test.csv\",\n",
    "    images_subdir=\"images\",\n",
    "    reports_subdir=\"reports\", #path to raw or structured reports, one per training and validation image\n",
    "    out_dir=\"/models_str\",\n",
    "    class_names=(\"normal\",\"tb\"),\n",
    "    use_align_losses=True,\n",
    "    hf_text_model_name=\"microsoft/BiomedVLP-CXR-BERT-general\",\n",
    "    hf_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "    hf_offline=True\n",
    ")\n",
    "cfg.hf_tokenizer_local_dir = cfg.hf_local_dir\n",
    "assert os.path.isdir(cfg.hf_local_dir), \"hf_local_dir not found\"\n",
    "if cfg.hf_tokenizer_local_dir is not None:\n",
    "    assert os.path.isdir(cfg.hf_tokenizer_local_dir), \"hf_tokenizer_local_dir not found\"\n",
    "\n",
    "# Load CSVs → DataFrames\n",
    "def _load_split(csv_path: str, cfg) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path, header=None, names=[\"filename\", \"label\"])\n",
    "    df[\"filename\"] = df[\"filename\"].astype(str)\n",
    "    df[\"y_index\"] = df[\"label\"].astype(int)\n",
    "    K = int(cfg.n_classes)\n",
    "    eye = np.eye(K, dtype=np.float32)\n",
    "    df[\"y_onehot\"] = df[\"y_index\"].apply(lambda i: eye[int(i)])\n",
    "    img_dir = getattr(cfg, \"images_dir\", os.path.join(cfg.dataset_root, cfg.images_subdir))\n",
    "    rep_dir = getattr(cfg, \"reports_dir\", os.path.join(cfg.dataset_root, cfg.reports_subdir))\n",
    "    df[\"image_path\"] = df[\"filename\"].apply(lambda fn: os.path.join(img_dir, fn))\n",
    "    df[\"report_path\"] = df[\"filename\"].apply(lambda fn: os.path.join(rep_dir, Path(fn).with_suffix(\".txt\").name))\n",
    "    return df\n",
    "\n",
    "df_train = _load_split(cfg.csv_train, cfg)\n",
    "df_valid = _load_split(cfg.csv_valid, cfg)\n",
    "df_test = _load_split(cfg.csv_test,  cfg)\n",
    "\n",
    "# Tokenizer + DataLoaders \n",
    "tokenizer = build_tokenizer(cfg)\n",
    "train_loader, valid_loader, test_loader = make_loaders(cfg, df_train, df_valid, df_test, tokenizer=tokenizer)\n",
    "assert train_loader is not None and valid_loader is not None\n",
    "\n",
    "# Grid of (λ, τ) and training\n",
    "lambda_grid = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "tau_grid    = [0.05, 0.06, 0.07, 0.08, 0.09]\n",
    "\n",
    "overall_best = {\"val_MCC\": -1.0, \"lam\": None, \"tau\": None, \"ckpt\": None, \"subdir\": None,\n",
    "                \"metrics_valid\": None, \"metrics_test\": None}\n",
    "\n",
    "for lam in lambda_grid:\n",
    "    for tau in tau_grid:\n",
    "        subdir = f\"mm_grid_lam{lam:.2f}_tau{tau:.2f}\"\n",
    "        run_cfg = RunConfig(\n",
    "            **{**cfg.__dict__,\n",
    "               \"out_dir\": os.path.join(cfg.out_dir, subdir),\n",
    "               \"align_lambda\": lam,\n",
    "               \"contrastive_temperature\": tau}\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[RUN] Multimodal training with λ={lam:.2f}, τ={tau:.2f}\")\n",
    "        print(f\"→ artifacts dir: {run_cfg.out_dir}\")\n",
    "        print(\"=\"*80, flush=True)\n",
    "\n",
    "        os.makedirs(run_cfg.out_dir, exist_ok=True)\n",
    "        model = MultimodalNet(run_cfg).to(device)\n",
    "        try:\n",
    "            _print_model_overview_full(model, run_cfg)\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        trcfg = TrainRuntimeCfg(amp=True, patience=10, grad_clip_norm=1.0, amp_dtype=torch.float16)\n",
    "        out = fit_with_val_loss_checkpointing(model, train_loader, valid_loader, run_cfg, trcfg=trcfg)\n",
    "\n",
    "        paths = out[\"paths\"]\n",
    "        valid_metrics = evaluate_dataloader_classifier(\n",
    "            model,\n",
    "            wrap_loader_with_collate(valid_loader, run_cfg.modality, run_cfg.n_classes),\n",
    "            run_cfg.modality\n",
    "        )\n",
    "        test_metrics = evaluate_dataloader_classifier(\n",
    "            model,\n",
    "            wrap_loader_with_collate(test_loader, run_cfg.modality, run_cfg.n_classes),\n",
    "            run_cfg.modality\n",
    "        )\n",
    "\n",
    "        print(\"\\n[BEST CHECKPOINT (by MCC)]\")\n",
    "        print(f\"λ={lam:.2f}  τ={tau:.2f}\")\n",
    "        print(f\"checkpoint : {paths['best_pt']}\")\n",
    "        print(f\"validation MCC : {valid_metrics['MCC']:.6f}\")\n",
    "        print(f\"test MCC : {test_metrics['MCC']:.6f}\")\n",
    "        print(f\"valid metrics : {valid_metrics}\")\n",
    "        print(f\"test  metrics : {test_metrics}\", flush=True)\n",
    "\n",
    "        if valid_metrics[\"MCC\"] > overall_best[\"val_MCC\"]:\n",
    "            overall_best.update({\n",
    "                \"val_MCC\": valid_metrics[\"MCC\"],\n",
    "                \"lam\": lam, \"tau\": tau,\n",
    "                \"ckpt\": paths[\"best_pt\"],\n",
    "                \"subdir\": subdir,\n",
    "                \"metrics_valid\": valid_metrics,\n",
    "                \"metrics_test\": test_metrics\n",
    "            })\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"[OVERALL BEST across grid (by VALIDATION MCC)]\")\n",
    "print(f\"λ={overall_best['lam']:.2f}  τ={overall_best['tau']:.2f}\")\n",
    "print(f\"best ckpt : {overall_best['ckpt']}\")\n",
    "print(f\"valid MCC : {overall_best['val_MCC']:.6f}\")\n",
    "print(f\"valid metrics : {overall_best['metrics_valid']}\")\n",
    "print(f\"test  metrics : {overall_best['metrics_test']}\")\n",
    "print(\"#\"*80, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386304dc-2493-44e9-ab94-ea92c74654da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference (manually provided checkpoint path)\n",
    "# Save per-sample softmax with ORIGINAL filenames into softmax_preds.csv\n",
    "\n",
    "# User config\n",
    "cfg = RunConfig(\n",
    "    modality=MOD.multimodal,          # default; can be overridden from ckpt name below\n",
    "    network=NETWORK.vgg11,            # must match the trained checkpoint family\n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    img_size=224,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    balance=False,\n",
    "    dataset_root=\"/dataset\",\n",
    "    csv_train=\"/label_train.csv\",\n",
    "    csv_valid=\"/label_valid.csv\",\n",
    "    csv_test =\"/label_test.csv\",\n",
    "    images_subdir=\"images\",\n",
    "    reports_subdir=\"reports\",\n",
    "    out_dir=\"/output\",\n",
    "    class_names=(\"normal\",\"tb\"),\n",
    "    use_align_losses=True,\n",
    "    hf_text_model_name=\"microsoft/BiomedVLP-CXR-BERT-general\",\n",
    "    hf_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "    hf_offline=True\n",
    ")\n",
    "cfg.hf_tokenizer_local_dir = cfg.hf_local_dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------- Manually set your checkpoint -------------------- #\n",
    "BEST_CKPT = \"best_vgg11_multimodal_val_loss.pt\"\n",
    "SAVE_SUBDIR_OVERRIDE = None \n",
    "\n",
    "# -------------------- Helpers -------------------- #\n",
    "def _ckpt_stem_from_name(ckpt_path: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Parse e.g. best_vgg16_img_val_loss.pt → (vgg16, img).\"\"\"\n",
    "    base = os.path.basename(ckpt_path)\n",
    "    m = re.match(r\"best_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_val_loss\\.pt$\", base)\n",
    "    if m: return m.group(1).lower(), m.group(2).lower()\n",
    "    return None, None\n",
    "\n",
    "def _infer_mod_from_token(tok: Optional[str]) -> Optional[\"MOD\"]:\n",
    "    if tok is None: return None\n",
    "    tl = tok.lower()\n",
    "    if \"img\" in tl: return MOD.img\n",
    "    if \"multimodal\" in tl: return MOD.multimodal\n",
    "    if \"txt\" in tl or \"text\" in tl: return MOD.txt\n",
    "    return None\n",
    "\n",
    "def _load_split(csv_path: str, cfg) -> pd.DataFrame:\n",
    "    \"\"\"Same CSV→DF logic used during training.\"\"\"\n",
    "    df = pd.read_csv(csv_path, header=None, names=[\"filename\", \"label\"])\n",
    "    df[\"filename\"] = df[\"filename\"].astype(str)\n",
    "    df[\"y_index\"]  = df[\"label\"].astype(int)\n",
    "    K = int(cfg.n_classes)\n",
    "    eye = np.eye(K, dtype=np.float32)\n",
    "    df[\"y_onehot\"] = df[\"y_index\"].apply(lambda i: eye[int(i)])\n",
    "    img_dir = getattr(cfg, \"images_dir\", os.path.join(cfg.dataset_root, cfg.images_subdir))\n",
    "    rep_dir = getattr(cfg, \"reports_dir\", os.path.join(cfg.dataset_root, cfg.reports_subdir))\n",
    "    from pathlib import Path\n",
    "    df[\"image_path\"] = df[\"filename\"].apply(lambda fn: os.path.join(img_dir, fn))\n",
    "    df[\"report_path\"] = df[\"filename\"].apply(lambda fn: os.path.join(rep_dir, Path(fn).with_suffix(\".txt\").name))\n",
    "    return df\n",
    "\n",
    "def _count_params(m: nn.Module) -> Tuple[int, int, int]:\n",
    "    tot = sum(p.numel() for p in m.parameters())\n",
    "    trn = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    frz = tot - trn\n",
    "    return tot, trn, frz\n",
    "\n",
    "def _print_and_save_model_overview(model: nn.Module, cfg_run: RunConfig, out_txt: str) -> None:\n",
    "    print(\"\\n========== Model (loaded for inference) ==========\")\n",
    "    print(f\"Modality : {cfg_run.modality.name}\")\n",
    "    print(f\"Backbone : {cfg_run.network.name}\")\n",
    "    print(model)  # full architecture\n",
    "    tot, trn, frz = _count_params(model)\n",
    "    print(f\"Parameters : total={tot:,}  |  trainable={trn:,}  |  frozen={frz:,}\")\n",
    "    print(\"===============================================\\n\")\n",
    "    os.makedirs(os.path.dirname(out_txt), exist_ok=True)\n",
    "    with open(out_txt, \"w\") as f:\n",
    "        f.write(str(model) + \"\\n\")\n",
    "        f.write(f\"total={tot:,} trainable={trn:,}  frozen={frz:,}\\n\")\n",
    "\n",
    "# -------------------- NEW: plain (no-wrap) loader for filename-safe CSV -------------------- #\n",
    "def _build_plain_test_loader_from_wrapped(wrapped_loader: DataLoader, cfg_run: \"RunConfig\") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Use the SAME dataset as the wrapped test loader, but default collate_fn so that\n",
    "    keys like 'filename' remain simple lists of strings. This keeps original filenames.\n",
    "    \"\"\"\n",
    "    ds = wrapped_loader.dataset                       \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(cfg_run.batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=int(cfg_run.num_workers),\n",
    "        pin_memory=bool(cfg_run.pin_memory),\n",
    "        drop_last=False,\n",
    "        persistent_workers=bool(int(cfg_run.num_workers) > 0),\n",
    "    )\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_manual_ckpt_with_training_stack(cfg_base: \"RunConfig\",\n",
    "                                             ckpt_path: str,\n",
    "                                             save_subdir: Optional[str] = None) -> Dict[str, float]:\n",
    "    assert os.path.isfile(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "\n",
    "    # Infer modality token from filename (img/multimodal/txt); keep network as provided.\n",
    "    net_tok, mod_tok = _ckpt_stem_from_name(ckpt_path)\n",
    "    infer_mod = _infer_mod_from_token(mod_tok)\n",
    "    cfg_run = dataclasses.replace(cfg_base, modality=(infer_mod or cfg_base.modality))\n",
    "    cfg_run.out_dir = os.path.dirname(ckpt_path)\n",
    "\n",
    "    # Rebuild DataFrames exactly like training\n",
    "    df_train = _load_split(cfg_run.csv_train, cfg_run)\n",
    "    df_valid = _load_split(cfg_run.csv_valid, cfg_run)\n",
    "    df_test = _load_split(cfg_run.csv_test,  cfg_run)\n",
    "\n",
    "    # Tokenizer + DataLoaders with the *same* function\n",
    "    tok = build_tokenizer(cfg_run)\n",
    "    _, _, test_loader_wrapped = make_loaders(cfg_run, df_train, df_valid, df_test, tokenizer=tok)\n",
    "\n",
    "    # Safe MM collate identical to training eval (kept for metrics)\n",
    "    test_loader_wrapped = wrap_loader_with_collate(test_loader_wrapped, cfg_run.modality, cfg_run.n_classes)\n",
    "\n",
    "    # ALSO build a plain loader from the same dataset so 'filename' is preserved\n",
    "    test_loader_plain = _build_plain_test_loader_from_wrapped(test_loader_wrapped, cfg_run)\n",
    "\n",
    "    # Load model & weights\n",
    "    model = MultimodalNet(cfg_run).to(device).eval()\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state, strict=True)\n",
    "\n",
    "    # Print & save overview\n",
    "    stem = f\"{(net_tok or cfg_run.network.name)}_{(mod_tok or cfg_run.modality.name)}\"\n",
    "    save_subdir = save_subdir or f\"test_infer_{stem}_raw_shenzhen_new\"    \n",
    "    save_dir = os.path.join(cfg_run.out_dir, save_subdir)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    _print_and_save_model_overview(model, cfg_run, os.path.join(save_dir, \"model_summary.txt\"))\n",
    "\n",
    "    # Evaluate with the SAME helper as training (uses wrapped loader)\n",
    "    metrics = evaluate_dataloader_classifier(model, test_loader_wrapped, cfg_run.modality)\n",
    "\n",
    "    # Persist metrics (unchanged)\n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(save_dir, \"test_metrics.csv\"), index=False)\n",
    "\n",
    "    # ----------------------- Save per-sample softmax to CSV with ORIGINAL filenames -----------------------\n",
    "    rows: List[dict] = []\n",
    "    for batch in test_loader_plain:\n",
    "        # Batch is a dict from TBMultimodalDataset with filename as a list[str]\n",
    "        if (\"image\" not in batch) or (\"y_onehot\" not in batch) or (\"filename\" not in batch):\n",
    "            continue\n",
    "        if batch[\"y_onehot\"].numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # tensors\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        txt = None\n",
    "        if (\"text\" in batch) and (batch[\"text\"] is not None):\n",
    "            txt = {k: v.to(device, non_blocking=True) for k, v in batch[\"text\"].items()}\n",
    "\n",
    "        # forward\n",
    "        out = model(x, txt)\n",
    "        if isinstance(out, dict):\n",
    "            if out.get(\"logits_img\", None) is not None:\n",
    "                logits = out[\"logits_img\"]\n",
    "            elif out.get(\"logits_txt\", None) is not None:\n",
    "                logits = out[\"logits_txt\"]\n",
    "            elif out.get(\"logits\", None) is not None:\n",
    "                logits = out[\"logits\"]\n",
    "            else:\n",
    "                # last resort: any (B,2) tensor in dict\n",
    "                logits = None\n",
    "                for v in out.values():\n",
    "                    if isinstance(v, torch.Tensor) and v.dim() == 2 and v.size(-1) == 2:\n",
    "                        logits = v\n",
    "                        break\n",
    "        else:\n",
    "            logits = out  # some models return a tensor directly\n",
    "\n",
    "        if not isinstance(logits, torch.Tensor) or logits.dim() != 2 or logits.size(-1) != 2:\n",
    "            raise RuntimeError(f\"Expected logits (B,2), got {type(logits)} with shape \"\n",
    "                               f\"{tuple(logits.shape) if isinstance(logits, torch.Tensor) else 'N/A'}\")\n",
    "\n",
    "        probs = F.softmax(logits.float(), dim=1).detach().cpu().numpy()  # (B,2)\n",
    "        pred = probs.argmax(axis=1).astype(int)\n",
    "        y_idx = torch.argmax(batch[\"y_onehot\"], dim=1).detach().cpu().numpy().astype(int)\n",
    "\n",
    "        # filenames are already as provided by dataset (strings with extension)\n",
    "        names = [os.path.basename(str(s)) for s in batch[\"filename\"]]\n",
    "        B = probs.shape[0]\n",
    "        assert len(names) == B, \"Filename list length must match batch size.\"\n",
    "\n",
    "        for i in range(B):\n",
    "            rows.append({\n",
    "                \"img\": names[i],\n",
    "                \"true_label\": int(y_idx[i]),\n",
    "                \"prob_0\": float(probs[i, 0]),\n",
    "                \"prob_1\": float(probs[i, 1]),\n",
    "                \"predicted_label\": int(pred[i]),\n",
    "            })\n",
    "\n",
    "    out_csv = os.path.join(save_dir, \"softmax_preds.csv\")\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f\"[SOFTMAX] Saved per-sample probabilities to: {out_csv}\")\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Echo results (unchanged)\n",
    "    print(f\"\\n=== Test results (manual ckpt) [{net_tok or cfg_run.network.name}/{cfg_run.modality.name}] ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:>18s}: {v}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Runner\n",
    "assert os.path.isfile(BEST_CKPT), f\"Checkpoint not found: {BEST_CKPT}\"\n",
    "print(\"\\n================ Inference Configuration (manual ckpt) ================\")\n",
    "print(f\"Checkpoint (pt)  : {BEST_CKPT}\")\n",
    "try:\n",
    "    fsize_mb = os.path.getsize(BEST_CKPT) / (1024**2)\n",
    "    print(f\"Checkpoint size  : {fsize_mb:.2f} MB\")\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"=======================================================================\\n\")\n",
    "evaluate_manual_ckpt_with_training_stack(cfg, BEST_CKPT, save_subdir=SAVE_SUBDIR_OVERRIDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f28d42-ff7b-49de-b260-778a5b6738ac",
   "metadata": {},
   "source": [
    "### GRAD-CAM VISUALIZATION WITH INTERNAL SHENZHEN TEST SET\n",
    "\n",
    "1. Uses manual checkpoint.\n",
    "2. Reads filenames from csv_test, so we won’t accidentally process extra images in the 1024 dir).\n",
    "3. For each filename, it prefers the 1024×1024 version (if missing, it resizes the original to 1024×1024 on the fly.\n",
    "4. Loads YOLO GT boxes (normalized) from the original annotation .txt, converts to original pixels, then rescales to 1024×1024. GT parsing assumes YOLO normalized lines cls cx cy w h in shen_mask_crop/<name>.txt. Each box is converted from original pixels to 1024×1024 via (sx, sy) = (1024/orig_W, 1024/orig_H).\n",
    "5. Saves into gradcam_manual_<network>_<modality>_overlap/ under the model directory, with subfolders images/, heatmaps/, contours/, bboxes/.\n",
    "6. uses blue for model bboxes and red for GT bboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b77f3-51cf-417d-aeb2-6dd0333996ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# User config\n",
    "cfg = RunConfig(\n",
    "    modality=MOD.multimodal,\n",
    "    network=NETWORK.vgg11,\n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-4,\n",
    "    img_size=224,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    balance=False,\n",
    "    dataset_root=\"/dataset\",\n",
    "    class_names=(\"normal\", \"tb\"),\n",
    "    use_align_losses=True,\n",
    "    hf_text_model_name=\"microsoft/BiomedVLP-CXR-BERT-general\",\n",
    "    hf_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "    hf_offline=True\n",
    ")\n",
    "\n",
    "# Paths (manual checkpoint + data)\n",
    "MANUAL_CKPT_PATH = \"/best_vgg11_multimodal_val_loss.pt\"  \n",
    "CSV_TEST_PATH = \"/dataset/label_test.csv\"\n",
    "ORIG_DIR = \"/dataset/shen_orig_crop\"\n",
    "YOLO_GT_DIR = \"/dataset/shen_mask_crop\"\n",
    "RESIZED_1024_DIR = \"/dataset/images\"  # contains many images; we will only use those present in csv_test\n",
    "\n",
    "# Output folder name\n",
    "SAVE_ROOT_NAME = f\"gradcam_{cfg.network.name}_{cfg.modality.name}_internal_overlap\"\n",
    "\n",
    "# Grad-CAM method & drawing params\n",
    "CAM_METHOD = \"gradcam\"             # \"gradcam\", \"gradcam++\", \"xgradcam\", \"layercam\", ...\n",
    "HEATMAP_ALPHA = 0.5\n",
    "BIN_THR = 0.4                   # threshold for binarizing CAM to extract contours/bboxes\n",
    "\n",
    "# Colors / thickness (BGR)\n",
    "CONTOUR_COLOR = (0, 0, 255)           # red for contours\n",
    "CONTOUR_THICK = 3\n",
    "BB_MODEL_COLOR = (255, 0, 0)           # blue for model bboxes (requested)\n",
    "BB_GT_COLOR = (0, 0, 255)           # red  for GT boxes (requested)\n",
    "BB_THICK = 4\n",
    "TARGET_CLASS_IDX = 1                    # \"abnormal\" class\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "RESIZE_TO = 1024                        # target side for visualization base\n",
    "\n",
    "# Helpers\n",
    "def _ckpt_stem(cfg: \"RunConfig\") -> str:\n",
    "    return f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "\n",
    "def _parse_lam_tau_from_path(path: str) -> Tuple[Optional[float], Optional[float]]:\n",
    "    lam = tau = None\n",
    "    m1 = re.search(r\"lam([0-9]+(?:\\.[0-9]+)?)\", path)\n",
    "    m2 = re.search(r\"tau([0-9]+(?:\\.[0-9]+)?)\", path)\n",
    "    if m1: lam = float(m1.group(1))\n",
    "    if m2: tau = float(m2.group(1))\n",
    "    return lam, tau\n",
    "\n",
    "def print_model_overview_for_cam(model: nn.Module, img_size: int = 224):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Grad-CAM — Model Overview\")\n",
    "    print(f\"Device: {device.type}\")\n",
    "    print(f\"Input size (C,H,W): (3, {img_size}, {img_size})\")\n",
    "    print(f\"Total params:  {total:,}\")\n",
    "    print(f\"Trainable params: {train:,}\")\n",
    "    print(f\"Frozen params: {total-train:,}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def _kernel_tuple(m: nn.Conv2d) -> Tuple[int,int]:\n",
    "    k = m.kernel_size\n",
    "    return (k if isinstance(k, tuple) else (k, k))\n",
    "\n",
    "def _last_conv_kgt1(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    last_any = None; last_kgt1 = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_any = m\n",
    "            if max(_kernel_tuple(m)) > 1:\n",
    "                last_kgt1 = m\n",
    "    return last_kgt1 if last_kgt1 is not None else last_any\n",
    "\n",
    "def _vit_patch_embed_conv(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    target = None\n",
    "    for name, m in module.named_modules():\n",
    "        if isinstance(m, nn.Conv2d) and (\"patch_embed\" in name or \"patch\" in name):\n",
    "            target = m\n",
    "    return target\n",
    "\n",
    "def _qualname_of_module(root: nn.Module, target: nn.Module) -> str:\n",
    "    for n, m in root.named_modules():\n",
    "        if m is target:\n",
    "            return n or \"<root>\"\n",
    "    return \"<unknown>\"\n",
    "\n",
    "def select_target_layer_for_cam(model: nn.Module) -> nn.Conv2d:\n",
    "    enc = getattr(model, \"img_enc\", None)\n",
    "    if enc is not None:\n",
    "        p3 = getattr(enc, \"post3x3\", None)\n",
    "        if isinstance(p3, nn.Sequential):\n",
    "            for m in p3.modules():\n",
    "                if isinstance(m, nn.Conv2d) and _kernel_tuple(m) == (3,3):\n",
    "                    return m\n",
    "        elif isinstance(p3, nn.Conv2d) and _kernel_tuple(p3) == (3,3):\n",
    "            return p3\n",
    "        tgt = _last_conv_kgt1(enc)\n",
    "        if isinstance(tgt, nn.Conv2d): return tgt\n",
    "        vitp = _vit_patch_embed_conv(enc)\n",
    "        if isinstance(vitp, nn.Conv2d): return vitp\n",
    "    tgt = _last_conv_kgt1(model)\n",
    "    if isinstance(tgt, nn.Conv2d): return tgt\n",
    "    raise RuntimeError(\"No suitable Conv2d layer found for Grad-CAM.\")\n",
    "\n",
    "class CamImageWrapper(nn.Module):\n",
    "    def __init__(self, mm: nn.Module): super().__init__(); self.mm = mm\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mm.forward_image(x)  # image head\n",
    "\n",
    "def build_preprocess(size: int) -> T.Compose:\n",
    "    return T.Compose([\n",
    "        T.ToPILImage(), T.Resize((size, size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "def _init_cam(CAMClass, model, target_layers):\n",
    "    sig = inspect.signature(CAMClass.__init__)\n",
    "    if \"use_cuda\" in sig.parameters:\n",
    "        return CAMClass(model=model, target_layers=target_layers, use_cuda=(device.type == \"cuda\"))\n",
    "    return CAMClass(model=model, target_layers=target_layers)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_ckpt_model(cfg: \"RunConfig\", ckpt_path: str) -> \"MultimodalNet\":\n",
    "    m = MultimodalNet(cfg).to(device).eval()\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    return m\n",
    "\n",
    "def _read_csv_test(csv_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, header=None, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"label_test.csv must have >= 2 columns (filename, label).\")\n",
    "    df = df.iloc[:, :2].copy(); df.columns = [\"img\", \"label\"]\n",
    "\n",
    "    def _is_hdr(s: str) -> bool:\n",
    "        s = (str(s) or \"\").strip().lower()\n",
    "        return s in {\"img\",\"image\",\"filename\",\"file\",\"path\",\"label\",\"class\",\"target\",\"y\"} or s.startswith(\"img\")\n",
    "\n",
    "    if _is_hdr(df.iloc[0,0]) and _is_hdr(df.iloc[0,1]):\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    df[\"img\"] = df[\"img\"].astype(str).str.strip()\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "def _is_image_name(name: str) -> bool:\n",
    "    return os.path.splitext(name)[1].lower() in IMG_EXTS\n",
    "\n",
    "def _load_original_size(name: str) -> Tuple[np.ndarray, int, int]:\n",
    "    \"\"\"Load the original image from ORIG_DIR to get W,H and for fallback resize.\"\"\"\n",
    "    p = os.path.join(ORIG_DIR, name)\n",
    "    bgr = cv2.imread(p)\n",
    "    if bgr is None:\n",
    "        raise FileNotFoundError(f\"Original image not found: {p}\")\n",
    "    H, W = bgr.shape[:2]\n",
    "    return bgr, W, H\n",
    "\n",
    "def _load_resized_1024(name: str) -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"Load the 1024×1024 image from RESIZED_1024_DIR by exact filename; return (bgr, exists_in_dir).\"\"\"\n",
    "    p = os.path.join(RESIZED_1024_DIR, name)\n",
    "    bgr = cv2.imread(p)\n",
    "    if bgr is None:\n",
    "        return None, False\n",
    "    return bgr, True\n",
    "\n",
    "def _ensure_1024_base_image(name: str) -> Tuple[np.ndarray, int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      base_bgr(1024×1024), base_W(1024), base_H(1024), orig_W, orig_H\n",
    "    If 1024 version is missing, resize the original to 1024×1024 on the fly.\n",
    "    \"\"\"\n",
    "    # Try 1024 file first\n",
    "    bgr_1024, ok = _load_resized_1024(name)\n",
    "    orig_bgr, orig_W, orig_H = _load_original_size(name)  # we need orig dims for GT scaling\n",
    "    if not ok:\n",
    "        base_bgr = cv2.resize(orig_bgr, (RESIZE_TO, RESIZE_TO), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        # Be safe: if that file isn’t exactly 1024×1024, force-resize to 1024×1024\n",
    "        H1, W1 = bgr_1024.shape[:2]\n",
    "        base_bgr = bgr_1024 if (H1 == RESIZE_TO and W1 == RESIZE_TO) else cv2.resize(bgr_1024, (RESIZE_TO, RESIZE_TO), interpolation=cv2.INTER_AREA)\n",
    "    return base_bgr, RESIZE_TO, RESIZE_TO, orig_W, orig_H\n",
    "\n",
    "def _read_yolo_gt_scaled_to_1024(name: str, orig_W: int, orig_H: int) -> List[Tuple[int,int,int,int]]:\n",
    "    \"\"\"\n",
    "    YOLO txt: 'cls cx cy w h' with values normalized to original image (W,H).\n",
    "    Convert to original pixels, then scale to 1024×1024.\n",
    "    \"\"\"\n",
    "    txt_path = os.path.join(YOLO_GT_DIR, os.path.splitext(name)[0] + \".txt\")\n",
    "    if not os.path.isfile(txt_path):\n",
    "        return []\n",
    "    boxes = []\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = re.split(r\"[,\\s]+\", line)\n",
    "            if len(toks) < 5:\n",
    "                continue\n",
    "            try:\n",
    "                # YOLO: cls cx cy w h (normalized)\n",
    "                _ = int(float(toks[0])) # class (unused)\n",
    "                cx = float(toks[1]); cy = float(toks[2])\n",
    "                w  = float(toks[3]); h  = float(toks[4])\n",
    "            except Exception:\n",
    "                continue\n",
    "            # original pixels\n",
    "            x1 = (cx - w/2.0) * orig_W\n",
    "            y1 = (cy - h/2.0) * orig_H\n",
    "            x2 = (cx + w/2.0) * orig_W\n",
    "            y2 = (cy + h/2.0) * orig_H\n",
    "            # scale to 1024\n",
    "            sx = RESIZE_TO / float(orig_W)\n",
    "            sy = RESIZE_TO / float(orig_H)\n",
    "            X1 = int(max(0, min(RESIZE_TO-1, round(x1 * sx))))\n",
    "            Y1 = int(max(0, min(RESIZE_TO-1, round(y1 * sy))))\n",
    "            X2 = int(max(0, min(RESIZE_TO-1, round(x2 * sx))))\n",
    "            Y2 = int(max(0, min(RESIZE_TO-1, round(y2 * sy))))\n",
    "            if X2 > X1 and Y2 > Y1:\n",
    "                boxes.append((X1, Y1, X2, Y2))\n",
    "    return boxes\n",
    "\n",
    "# Main: Grad-CAM over csv_test, overlay GT (scaled)\n",
    "def run_gradcam_from_csv_resized_overlap(\n",
    "    cfg: \"RunConfig\",\n",
    "    ckpt_path: str,\n",
    "    csv_test_path: str,\n",
    "    save_root_overlap: str,\n",
    "    cam_method: str = CAM_METHOD,\n",
    "    heatmap_alpha: float = HEATMAP_ALPHA,\n",
    "    bin_thr: float = BIN_THR,\n",
    "):\n",
    "    assert os.path.isfile(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    os.makedirs(save_root_overlap, exist_ok=True)\n",
    "    out_dirs = {\n",
    "        \"images\": os.path.join(save_root_overlap, \"images\"),\n",
    "        \"heatmaps\": os.path.join(save_root_overlap, \"heatmaps\"),\n",
    "        \"contours\": os.path.join(save_root_overlap, \"contours\"),\n",
    "        \"bboxes\": os.path.join(save_root_overlap, \"bboxes\"),\n",
    "    }\n",
    "    for d in out_dirs.values():\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Load model & CAM\n",
    "    mm = _load_ckpt_model(cfg, ckpt_path)\n",
    "    print_model_overview_for_cam(mm, img_size=int(getattr(cfg, \"img_size\", 224)))\n",
    "    cam_model = CamImageWrapper(mm).to(device).eval()\n",
    "    target_layer = select_target_layer_for_cam(mm)\n",
    "    for p in target_layer.parameters(): p.requires_grad_(True)\n",
    "    tl_name = _qualname_of_module(mm, target_layer)\n",
    "    print(f\"[CAM] Target layer: {tl_name}  kernel={getattr(target_layer, 'kernel_size', None)}\")\n",
    "    methods = {\n",
    "        \"gradcam\": GradCAM, \"gradcam++\": GradCAMPlusPlus, \"hirescam\": HiResCAM,\n",
    "        \"xgradcam\": XGradCAM, \"layercam\": LayerCAM, \"eigencam\": EigenCAM,\n",
    "        \"eigengradcam\": EigenGradCAM, \"scorecam\": ScoreCAM, \"ablationcam\": AblationCAM\n",
    "    }\n",
    "    mkey = cam_method.lower()\n",
    "    if mkey not in methods:\n",
    "        raise ValueError(f\"Unknown CAM method '{cam_method}'\")\n",
    "    CAMClass = methods[mkey]\n",
    "    cam = _init_cam(CAMClass, model=cam_model, target_layers=[target_layer])\n",
    "    try: cam.batch_size = 1\n",
    "    except: pass\n",
    "\n",
    "    preprocess = build_preprocess(int(getattr(cfg, \"img_size\", 224)))\n",
    "    df = _read_csv_test(csv_test_path)\n",
    "\n",
    "    # Iterate only over test filenames from CSV (ignore any extra files in 1024 dir)\n",
    "    for _, row in df.iterrows():\n",
    "        name = str(row[\"img\"]).strip()\n",
    "        if not _is_image_name(name):\n",
    "            continue\n",
    "\n",
    "        # Load base visualization image (1024×1024), and original dims for GT scaling\n",
    "        try:\n",
    "            base_bgr, W_vis, H_vis, orig_W, orig_H = _ensure_1024_base_image(name)\n",
    "        except FileNotFoundError:\n",
    "            # If the original is missing, skip safely\n",
    "            print(f\"[WARN] Skipping (image missing): {name}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare model input from the 1024 image\n",
    "        rgb = cv2.cvtColor(base_bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = preprocess(rgb).unsqueeze(0).to(device)\n",
    "\n",
    "        # Run CAM\n",
    "        with torch.enable_grad():\n",
    "            mask = cam(\n",
    "                input_tensor=x,\n",
    "                targets=[ClassifierOutputTarget(int(TARGET_CLASS_IDX))],\n",
    "                aug_smooth=True, eigen_smooth=True\n",
    "            )[0] if mkey != \"eigencam\" else cam(input_tensor=x)[0]\n",
    "\n",
    "        # Normalize & upsample to 1024\n",
    "        mmin, mmax = float(np.min(mask)), float(np.max(mask))\n",
    "        mask = (mask - mmin) / (mmax - mmin + 1e-8)\n",
    "        mask_u8 = (np.clip(cv2.resize(mask, (W_vis, H_vis), interpolation=cv2.INTER_NEAREST), 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        # Heatmap overlay (on 1024 image)\n",
    "        heat = cv2.applyColorMap(mask_u8, cv2.COLORMAP_HOT)\n",
    "        heat_overlay = cv2.addWeighted(heat, float(heatmap_alpha), base_bgr, 1.0 - float(heatmap_alpha), 0.0)\n",
    "\n",
    "        # Contours from thresholded mask\n",
    "        thr = int(255 * float(bin_thr))\n",
    "        _, binm = cv2.threshold(mask_u8, thr, 255, cv2.THRESH_BINARY)\n",
    "        contours,_ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw contours (red)\n",
    "        cont_img = base_bgr.copy()\n",
    "        if len(contours) > 0:\n",
    "            cv2.drawContours(cont_img, contours, -1, CONTOUR_COLOR, CONTOUR_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # Draw model bounding boxes (blue)\n",
    "        box_img = base_bgr.copy()\n",
    "        model_boxes = []\n",
    "        for c in contours:\n",
    "            x0, y0, w, h = cv2.boundingRect(c)\n",
    "            x1, y1, x2, y2 = x0, y0, x0 + w, y0 + h\n",
    "            model_boxes.append((x1, y1, x2, y2))\n",
    "            cv2.rectangle(box_img, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # Load & overlay GT boxes scaled to 1024\n",
    "        gt_boxes_1024 = _read_yolo_gt_scaled_to_1024(name, orig_W, orig_H)\n",
    "\n",
    "        # Write base image\n",
    "        stem = os.path.splitext(name)[0]\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"images\"], f\"{stem}.png\"), base_bgr)\n",
    "\n",
    "        # 1) Heatmaps + GT red boxes\n",
    "        heat_ov = heat_overlay.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes_1024:\n",
    "            cv2.rectangle(heat_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"heatmaps\"], f\"{stem}__{cfg.network.name}__{mkey}.png\"), heat_ov)\n",
    "\n",
    "        # 2) Contours + GT red boxes\n",
    "        cont_ov = cont_img.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes_1024:\n",
    "            cv2.rectangle(cont_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"contours\"], f\"{stem}__{cfg.network.name}__{mkey}.png\"), cont_ov)\n",
    "\n",
    "        # 3) BBoxes: blue model boxes + red GT boxes (requested colors)\n",
    "        box_ov = base_bgr.copy()\n",
    "        # model (blue)\n",
    "        for (x1, y1, x2, y2) in model_boxes:\n",
    "            cv2.rectangle(box_ov, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        # GT (red)\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes_1024:\n",
    "            cv2.rectangle(box_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(out_dirs[\"bboxes\"], f\"{stem}__{cfg.network.name}__{mkey}.png\"), box_ov)\n",
    "\n",
    "    # Cleanup\n",
    "    try:\n",
    "        if hasattr(cam, \"activations_and_grads\") and (cam.activations_and_grads is not None):\n",
    "            cam.activations_and_grads.release()\n",
    "    except Exception:\n",
    "        pass\n",
    "    del cam, cam_model, mm\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"[CAM] Saved to: {save_root_overlap}\")\n",
    "\n",
    "# Runner (manual ckpt, csv_test)\n",
    "assert os.path.isfile(MANUAL_CKPT_PATH), f\"Checkpoint not found: {MANUAL_CKPT_PATH}\"\n",
    "best_subdir = os.path.dirname(MANUAL_CKPT_PATH)\n",
    "save_root_overlap = os.path.join(best_subdir, SAVE_ROOT_NAME)\n",
    "\n",
    "print(\"\\n================ Grad-CAM Configuration ================\")\n",
    "print(f\"Checkpoint : {MANUAL_CKPT_PATH}\")\n",
    "print(f\"Backbone   : {cfg.network.name}\")\n",
    "print(f\"Modality   : {cfg.modality.name}\")\n",
    "print(f\"csv_test   : {CSV_TEST_PATH}\")\n",
    "print(f\"Orig dir   : {ORIG_DIR}\")\n",
    "print(f\"GT (YOLO)  : {YOLO_GT_DIR}\")\n",
    "print(f\"ResizedDir : {RESIZED_1024_DIR}  (will only use files present in csv_test)\")\n",
    "print(f\"Save root  : {save_root_overlap}\")\n",
    "print(\"=======================================================\\n\")\n",
    "\n",
    "run_gradcam_from_csv_resized_overlap(\n",
    "    cfg=cfg,\n",
    "    ckpt_path=MANUAL_CKPT_PATH,\n",
    "    csv_test_path=CSV_TEST_PATH,\n",
    "    save_root_overlap=save_root_overlap,\n",
    "    cam_method=CAM_METHOD,\n",
    "    heatmap_alpha=HEATMAP_ALPHA,\n",
    "    bin_thr=BIN_THR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d0fe4-fc21-42d2-9b70-379d6ccbd3e5",
   "metadata": {},
   "source": [
    "### Computing GRAD-CAM visualizations with the external TBX11K test set\n",
    "\n",
    "1. Annotations (512×512) → pre-crop (256×256); Scale the authors’ ground truth boxes by 0.5.\n",
    "2. Pre-crop (256×256) → lung-crop: Use the provided 256×256 mask to recover the actual crop ROI (the tight bounding rectangle of non-zero mask, optionally with a small margin). Intersect the pre-crop box with this ROI and translate it to the crop coordinate frame.\n",
    "3. Lung-crop → final saved TB image (256×256): The crop ROI is then resized to 256×256 when saving the lung-cropped images. So scale the translated box by: sx = 256 / (roi_w),  sy = 256 / (roi_h)\n",
    "4. Use floor for x1,y1 and ceil for x2,y2 to preserve coverage; clip to [0,255].\n",
    "5. Run Grad-CAM on those TB images and overlays blue model CAM boxes + red GT boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31180f5-91c3-447b-9b37-fad54b09d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"gradcam-tbx11k-mask-mapped\")\n",
    "\n",
    "def set_deterministic(seed: int = 1337) -> None:\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_deterministic(1337)\n",
    "\n",
    "# User config\n",
    "cfg = RunConfig(\n",
    "    modality=MOD.multimodal,\n",
    "    network=NETWORK.vgg11,\n",
    "    n_classes=2,\n",
    "    hidden_dim=256,\n",
    "    epochs=64,\n",
    "    lr=5e-5,\n",
    "    weight_decay=1e-4,\n",
    "    img_size=224,   \n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset_root=\"/dataset\",\n",
    "    class_names=(\"normal\", \"tb\"),\n",
    "    use_align_losses=True,\n",
    "    hf_text_model_name=\"microsoft/BiomedVLP-CXR-BERT-general\",\n",
    "    hf_local_dir=\"/huggingface/hub/models--microsoft--BiomedVLP-CXR-BERT-general\",\n",
    "    hf_offline=True\n",
    ")\n",
    "\n",
    "# Manual checkpoint + TBX11K paths\n",
    "MANUAL_CKPT_PATH = \"/best_vgg11_multimodal_val_loss.pt\" \n",
    "CSV_TEST_PATH    = \"/dataset/label_test_tbx11k.csv\"\n",
    "\n",
    "# Final lung-cropped TB images (256×256)\n",
    "TB_256_DIR = \"/dataset/tbx11k/cropped/tb\"\n",
    "# Original TB images (512×512)\n",
    "ORIG_TB_DIR = \"/dataset/tbx11k/orig/tb\"\n",
    "# 256×256 masks aligned to the pre-crop 256×256 space\n",
    "TB_MASKS_256_DIR = \"/dataset/tbx11k/masks/tb\"\n",
    "# Authors' lesion boxes (512×512 coordinates)\n",
    "BBOX_CSV_PATH = \"/dataset/data_tbx11k.csv\"\n",
    "SAVE_ROOT_NAME = f\"gradcam_tbx11k_{cfg.network.name}_{cfg.modality.name}_external_overlap\"\n",
    "\n",
    "# Grad-CAM & drawing params\n",
    "CAM_METHOD  = \"gradcam\"   \n",
    "HEATMAP_ALPHA = 0.5\n",
    "BIN_THR = 0.4 \n",
    "\n",
    "# Colors / thickness (BGR)\n",
    "CONTOUR_COLOR = (0,0,255)\n",
    "CONTOUR_THICK = 2\n",
    "BB_MODEL_COLOR = (255,0,0)\n",
    "BB_GT_COLOR = (0,0,255)\n",
    "BB_THICK = 3\n",
    "TARGET_CLASS_IDX = 1\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "FINAL_SIZE = 256              # visualization base size (final TB images are 256×256)\n",
    "# Optional crop behavior \n",
    "ROI_MARGIN_FRAC = 0.00        # e.g., 0.02 to add 2% per side\n",
    "ROI_ENFORCE_SQUARE = False    # set True if you padded to square before resizing\n",
    "\n",
    "# Path/CSV utils\n",
    "JUNK_DIR_TOKENS = {\"__pycache__\", \".ipynb_checkpoints\", \".git\", \".svn\", \".DS_Store\"}\n",
    "JUNK_FILE_BASENAMES = {\"thumbs.db\", \"desktop.ini\"}\n",
    "_CHECKPOINT_RE = re.compile(r\"(?i)(?:^|[^A-Za-z0-9])checkpoint(?:$|[^A-Za-z0-9])\")\n",
    "_DOTFILE_RE    = re.compile(r\"^\\.\")  # dot files\n",
    "\n",
    "def _is_img(path_or_name: str) -> bool:\n",
    "    return os.path.splitext(path_or_name)[1].lower() in IMG_EXTS\n",
    "\n",
    "def _is_junk_path(path: str) -> bool:\n",
    "    p = pathlib.Path(path)\n",
    "    for part in p.parts:\n",
    "        if part in JUNK_DIR_TOKENS or _DOTFILE_RE.search(part):\n",
    "            return True\n",
    "    base = p.name\n",
    "    if base.lower() in JUNK_FILE_BASENAMES:\n",
    "        return True\n",
    "    if _DOTFILE_RE.match(base) or _CHECKPOINT_RE.search(os.path.splitext(base)[0]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _list_images(dir_path: str) -> List[str]:\n",
    "    out = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        dirs[:] = [d for d in dirs if not _is_junk_path(os.path.join(root, d))]\n",
    "        for fn in files:\n",
    "            fp = os.path.join(root, fn)\n",
    "            if not _is_junk_path(fp) and _is_img(fn):\n",
    "                out.append(fp)\n",
    "    return sorted(out)\n",
    "\n",
    "def _read_test_csv(csv_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, header=None, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"Test CSV must have >= 2 columns (filename, label).\")\n",
    "    df = df.iloc[:, :2]; df.columns = [\"img\", \"label\"]\n",
    "    # strip header row if present\n",
    "    def _ishdr(x): \n",
    "        s = str(x).strip().lower()\n",
    "        return s in {\"img\",\"image\",\"filename\",\"file\",\"path\",\"label\",\"class\",\"target\",\"y\"} or s.startswith(\"img\")\n",
    "    if _ishdr(df.iloc[0,0]) and _ishdr(df.iloc[0,1]):\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "    df[\"img\"]   = df[\"img\"].astype(str).map(lambda s: os.path.basename(s.strip()))\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"img\"].map(_is_img)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _find_by_basename(dir_path: str, basename: str) -> Optional[str]:\n",
    "    base, _ = os.path.splitext(basename)\n",
    "    for ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"]:\n",
    "        p = os.path.join(dir_path, base + ext)\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def crosscheck_tb(csv_tb_names: List[str], tb_dir: str) -> List[str]:\n",
    "    csv_tb_set = {n for n in csv_tb_names if n.lower().startswith(\"tb\")}\n",
    "    dir_tb_set = {os.path.basename(p) for p in _list_images(tb_dir) if os.path.basename(p).lower().startswith(\"tb\")}\n",
    "    inter = sorted(csv_tb_set & dir_tb_set)\n",
    "    log.info(\"[TB CHECK] CSV tb count : %d\", len(csv_tb_set))\n",
    "    log.info(\"[TB CHECK] Dir tb count : %d\", len(dir_tb_set))\n",
    "    log.info(\"[TB CHECK] Match count  : %d\", len(inter))\n",
    "    only_csv = sorted(csv_tb_set - dir_tb_set)\n",
    "    only_dir = sorted(dir_tb_set - csv_tb_set)\n",
    "    if only_csv or only_dir:\n",
    "        log.warning(\"[TB CHECK] Mismatch — only_in_csv=%d, only_in_dir=%d\", len(only_csv), len(only_dir))\n",
    "        if only_csv[:5]: log.warning(\"  e.g., only_in_csv: %s\", only_csv[:5])\n",
    "        if only_dir[:5]: log.warning(\"  e.g., only_in_dir: %s\", only_dir[:5])\n",
    "    else:\n",
    "        log.info(\"[TB CHECK] Filename sets MATCH ✅\")\n",
    "    return inter\n",
    "\n",
    "# BBoxes (authors @512) → final 256 via mask-driven crop\n",
    "def _parse_bbox_field(braw: Any) -> List[Dict[str, float]]:\n",
    "    \"\"\"bbox can be list[dict] or dict-as-string; return list of dicts with xmin,ymin,width,height.\"\"\"\n",
    "    if braw is None or (isinstance(braw, float) and math.isnan(braw)): return []\n",
    "    try:\n",
    "        b = ast.literal_eval(braw) if isinstance(braw, str) else braw\n",
    "    except Exception:\n",
    "        return []\n",
    "    if isinstance(b, dict):  return [b]\n",
    "    if isinstance(b, list):  return [x for x in b if isinstance(x, dict)]\n",
    "    return []\n",
    "\n",
    "def _mask_to_roi(mask_256: np.ndarray, margin_frac: float = ROI_MARGIN_FRAC,\n",
    "                 enforce_square: bool = ROI_ENFORCE_SQUARE) -> Optional[Tuple[int,int,int,int]]:\n",
    "    \"\"\"Return (x1,y1,x2,y2) tight ROI of non-zero mask with optional margin/square padding.\"\"\"\n",
    "    if mask_256 is None: return None\n",
    "    if mask_256.max() > 1:\n",
    "        _, binm = cv2.threshold(mask_256, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    else:\n",
    "        binm = (mask_256 > 0).astype(np.uint8)*255\n",
    "    cnts, _ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts: \n",
    "        return None\n",
    "    x1 = min(cv2.boundingRect(c)[0] for c in cnts)\n",
    "    y1 = min(cv2.boundingRect(c)[1] for c in cnts)\n",
    "    x2 = max(cv2.boundingRect(c)[0] + cv2.boundingRect(c)[2] for c in cnts)\n",
    "    y2 = max(cv2.boundingRect(c)[1] + cv2.boundingRect(c)[3] for c in cnts)\n",
    "    # margin\n",
    "    if margin_frac and margin_frac > 0:\n",
    "        w = x2 - x1; h = y2 - y1\n",
    "        mx = int(round(w * margin_frac)); my = int(round(h * margin_frac))\n",
    "        x1 -= mx; y1 -= my; x2 += mx; y2 += my\n",
    "    # enforce square (optional)\n",
    "    if enforce_square:\n",
    "        w = x2 - x1; h = y2 - y1\n",
    "        side = max(w, h)\n",
    "        cx = (x1 + x2) // 2; cy = (y1 + y2) // 2\n",
    "        # re-center to square\n",
    "        x1 = cx - side//2; y1 = cy - side//2\n",
    "        x2 = x1 + side;     y2 = y1 + side\n",
    "    # clip to [0,255]\n",
    "    x1 = max(0, min(255, x1)); y1 = max(0, min(255, y1))\n",
    "    x2 = max(1, min(256, x2)); y2 = max(1, min(256, y2))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def _scale_512_to_256_box(xmin: float, ymin: float, width: float, height: float,\n",
    "                          src_w: int = 512, src_h: int = 512) -> Tuple[float,float,float,float]:\n",
    "    \"\"\"Return (x1,y1,x2,y2) in pre-crop 256×256 space.\"\"\"\n",
    "    sx = 256.0 / float(src_w); sy = 256.0 / float(src_h)\n",
    "    x1 = (xmin) * sx; y1 = (ymin) * sy\n",
    "    x2 = (xmin + width) * sx; y2 = (ymin + height) * sy\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def _map_box_pre256_to_final256_via_roi(box_pre: Tuple[float,float,float,float],\n",
    "                                        roi: Tuple[int,int,int,int]) -> Optional[Tuple[int,int,int,int]]:\n",
    "    \"\"\"Apply crop by ROI then scale to 256 final image (floor/ceil to preserve coverage).\"\"\"\n",
    "    x1, y1, x2, y2 = box_pre\n",
    "    rx1, ry1, rx2, ry2 = roi\n",
    "    # intersect with ROI\n",
    "    ix1 = max(x1, rx1); iy1 = max(y1, ry1)\n",
    "    ix2 = min(x2, rx2); iy2 = min(y2, ry2)\n",
    "    if ix2 <= ix1 or iy2 <= iy1:\n",
    "        return None\n",
    "    # translate to crop coords\n",
    "    cx1 = ix1 - rx1; cy1 = iy1 - ry1\n",
    "    cx2 = ix2 - rx1; cy2 = iy2 - ry1\n",
    "    rw = max(1, rx2 - rx1); rh = max(1, ry2 - ry1)\n",
    "    sx = 256.0 / float(rw); sy = 256.0 / float(rh)\n",
    "    fx1 = math.floor(cx1 * sx); fy1 = math.floor(cy1 * sy)\n",
    "    fx2 = math.ceil (cx2 * sx); fy2 = math.ceil (cy2 * sy)\n",
    "    # clip to [0,255], ensure min size\n",
    "    fx1 = max(0, min(255, fx1)); fy1 = max(0, min(255, fy1))\n",
    "    fx2 = max(fx1+1, min(256, fx2)); fy2 = max(fy1+1, min(256, fy2))\n",
    "    return (int(fx1), int(fy1), int(fx2), int(fy2))\n",
    "\n",
    "def build_gt_box_map_final256(bbox_csv_path: str, masks_dir_256: str,\n",
    "                              names_final_256: List[str]) -> Dict[str, List[Tuple[int,int,int,int]]]:\n",
    "    \"\"\"\n",
    "    For each tb image name in names_final_256:\n",
    "      * Read its mask (256×256) to derive the crop ROI\n",
    "      * Load authors’ boxes (512×512), scale to pre-crop 256×256\n",
    "      * Apply ROI crop+resize mapping to final 256×256\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(bbox_csv_path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    needed = [\"fname\",\"image_height\",\"image_width\",\"bbox\",\"target\",\"image_type\"]\n",
    "    for k in needed:\n",
    "        if k not in cols: raise ValueError(f\"Column '{k}' missing in {bbox_csv_path}\")\n",
    "\n",
    "    df = df[\n",
    "        (df[cols[\"image_type\"]].astype(str).str.lower() == \"tb\") &\n",
    "        (df[cols[\"target\"]].astype(str).str.lower() == \"tb\")\n",
    "    ].copy()\n",
    "\n",
    "    # Collect authors' boxes keyed by lowercase basename (no extension)\n",
    "    authors_boxes_512: Dict[str, List[Tuple[float,float,float,float,int,int]]] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        name_raw = os.path.basename(str(r[cols[\"fname\"]]).strip())\n",
    "        base_key = os.path.splitext(name_raw)[0].lower()\n",
    "        boxes = _parse_bbox_field(r[cols[\"bbox\"]])\n",
    "        if not boxes:\n",
    "            continue\n",
    "        W = int(r[cols[\"image_width\"]]) if not pd.isna(r[cols[\"image_width\"]]) else 512\n",
    "        H = int(r[cols[\"image_height\"]]) if not pd.isna(r[cols[\"image_height\"]]) else 512\n",
    "        W = 512 if W <= 0 else W; H = 512 if H <= 0 else H\n",
    "        pre_list = authors_boxes_512.setdefault(base_key, [])\n",
    "        for b in boxes:\n",
    "            try:\n",
    "                xmin = float(b.get(\"xmin\",0.0)); ymin = float(b.get(\"ymin\",0.0))\n",
    "                width= float(b.get(\"width\",0.0)); height= float(b.get(\"height\",0.0))\n",
    "                if width <= 0 or height <= 0: continue\n",
    "                pre_list.append((xmin, ymin, width, height, W, H))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # Map to final 256×256 using the 256×256 mask ROI\n",
    "    out: Dict[str, List[Tuple[int,int,int,int]]] = {}\n",
    "    for name in names_final_256:\n",
    "        base = os.path.splitext(name)[0]\n",
    "        base_key = base.lower()\n",
    "        # find mask file for this name\n",
    "        mask_path = _find_by_basename(masks_dir_256, name)\n",
    "        if mask_path is None:\n",
    "            raise FileNotFoundError(f\"[GT MAP] Missing mask for {name}\")\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise RuntimeError(f\"[GT MAP] Unreadable mask for {name}: {mask_path}\")\n",
    "        roi = _mask_to_roi(mask, ROI_MARGIN_FRAC, ROI_ENFORCE_SQUARE)\n",
    "        if roi is None:\n",
    "            raise RuntimeError(f\"[GT MAP] Empty ROI from mask for {name}\")\n",
    "\n",
    "        auth_list = authors_boxes_512.get(base_key, [])\n",
    "        if not auth_list:\n",
    "            # Enforce: every processed TB image must have at least one GT\n",
    "            raise RuntimeError(f\"[GT MAP] No author GT boxes found for TB image {name}\")\n",
    "\n",
    "        final_boxes = []\n",
    "        for (xmin, ymin, width, height, W, H) in auth_list:\n",
    "            # 512 → pre-crop 256\n",
    "            x1p, y1p, x2p, y2p = _scale_512_to_256_box(xmin, ymin, width, height, src_w=W, src_h=H)\n",
    "            mapped = _map_box_pre256_to_final256_via_roi((x1p, y1p, x2p, y2p), roi)\n",
    "            if mapped is not None:\n",
    "                final_boxes.append(mapped)\n",
    "        if not final_boxes:\n",
    "            raise RuntimeError(f\"[GT MAP] All author boxes cropped out for {name} (no intersection with ROI).\")\n",
    "        out[name] = final_boxes\n",
    "    return out\n",
    "\n",
    "# Model / CAM helpers\n",
    "def _ckpt_stem(cfg: \"RunConfig\") -> str:\n",
    "    return f\"{cfg.network.name}_{cfg.modality.name}\"\n",
    "\n",
    "def print_model_overview_for_cam(model: nn.Module, img_size: int = 224) -> None:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(\"Grad-CAM — Model Overview\")\n",
    "    log.info(f\"Device: {device.type}\")\n",
    "    log.info(f\"Input size (C,H,W): (3, {img_size}, {img_size})\")\n",
    "    log.info(f\"Total params: {total:,}\")\n",
    "    log.info(f\"Trainable params: {train:,}\")\n",
    "    log.info(f\"Frozen params: {total-train:,}\")\n",
    "\n",
    "def _kernel_tuple(m: nn.Conv2d) -> Tuple[int,int]:\n",
    "    k = m.kernel_size\n",
    "    return (k if isinstance(k, tuple) else (k, k))\n",
    "\n",
    "def _last_conv_kgt1(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    last_any = None; last_kgt1 = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_any = m\n",
    "            if max(_kernel_tuple(m)) > 1:\n",
    "                last_kgt1 = m\n",
    "    return last_kgt1 if last_kgt1 is not None else last_any\n",
    "\n",
    "def _vit_patch_embed_conv(module: nn.Module) -> Optional[nn.Conv2d]:\n",
    "    target = None\n",
    "    for name, m in module.named_modules():\n",
    "        if isinstance(m, nn.Conv2d) and (\"patch_embed\" in name or \"patch\" in name):\n",
    "            target = m\n",
    "    return target\n",
    "\n",
    "def _qualname_of_module(root: nn.Module, target: nn.Module) -> str:\n",
    "    for n, m in root.named_modules():\n",
    "        if m is target:\n",
    "            return n or \"<root>\"\n",
    "    return \"<unknown>\"\n",
    "\n",
    "def select_target_layer_for_cam(model: nn.Module) -> nn.Conv2d:\n",
    "    enc = getattr(model, \"img_enc\", None)\n",
    "    if enc is not None:\n",
    "        p3 = getattr(enc, \"post3x3\", None)\n",
    "        if isinstance(p3, nn.Sequential):\n",
    "            for m in p3.modules():\n",
    "                if isinstance(m, nn.Conv2d) and _kernel_tuple(m) == (3,3):\n",
    "                    return m\n",
    "        elif isinstance(p3, nn.Conv2d) and _kernel_tuple(p3) == (3,3):\n",
    "            return p3\n",
    "        tgt = _last_conv_kgt1(enc)\n",
    "        if isinstance(tgt, nn.Conv2d): return tgt\n",
    "        vitp = _vit_patch_embed_conv(enc)\n",
    "        if isinstance(vitp, nn.Conv2d): return vitp\n",
    "    tgt = _last_conv_kgt1(model)\n",
    "    if isinstance(tgt, nn.Conv2d): return tgt\n",
    "    raise RuntimeError(\"No suitable Conv2d layer found for Grad-CAM.\")\n",
    "\n",
    "class CamImageWrapper(nn.Module):\n",
    "    def __init__(self, mm: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.mm = mm\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mm.forward_image(x)  # image head only\n",
    "\n",
    "def build_preprocess(size: int) -> T.Compose:\n",
    "    return T.Compose([\n",
    "        T.ToPILImage(), T.Resize((size, size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "def _init_cam(CAMClass, model, target_layers):\n",
    "    sig = inspect.signature(CAMClass.__init__)\n",
    "    if \"use_cuda\" in sig.parameters:\n",
    "        return CAMClass(model=model, target_layers=target_layers, use_cuda=(device.type == \"cuda\"))\n",
    "    return CAMClass(model=model, target_layers=target_layers)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_ckpt_model(cfg: \"RunConfig\", ckpt_path: str) -> \"MultimodalNet\":\n",
    "    m = MultimodalNet(cfg).to(device).eval()\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    return m\n",
    "\n",
    "# Grad-CAM main\n",
    "def run_gradcam_tbx11k_tb_mask_mapped(\n",
    "    cfg: \"RunConfig\",\n",
    "    ckpt_path: str,\n",
    "    csv_test_path: str,\n",
    "    tb_256_dir: str,\n",
    "    masks_256_dir: str,\n",
    "    bbox_csv_path: str,\n",
    "    save_root: str,\n",
    "    cam_method: str = CAM_METHOD,\n",
    "    heatmap_alpha: float = HEATMAP_ALPHA,\n",
    "    bin_thr: float = BIN_THR,\n",
    ") -> None:\n",
    "    assert os.path.isfile(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    for p in [csv_test_path, tb_256_dir, masks_256_dir, bbox_csv_path]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Required path missing: {p}\")\n",
    "\n",
    "    # Output structure\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "    out_dirs = {\n",
    "        \"images\": os.path.join(save_root, \"images\"),\n",
    "        \"heatmaps\": os.path.join(save_root, \"heatmaps\"),\n",
    "        \"contours\": os.path.join(save_root, \"contours\"),\n",
    "        \"bboxes\": os.path.join(save_root, \"bboxes\"),\n",
    "        \"bbox_coord\": os.path.join(save_root, \"bbox_coord\"),  # YOLO label dump\n",
    "    }\n",
    "    for d in out_dirs.values(): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Checks: TB_256_DIR vs ORIG_TB_DIR (WARN instead of raise)\n",
    "    tb256_list = [p for p in _list_images(tb_256_dir) if os.path.basename(p).lower().startswith(\"tb\")]\n",
    "    orig_list = [p for p in _list_images(ORIG_TB_DIR) if os.path.basename(p).lower().startswith(\"tb\")]\n",
    "    tb256_stems = {os.path.splitext(os.path.basename(p))[0] for p in tb256_list}\n",
    "    orig_stems = {os.path.splitext(os.path.basename(p))[0] for p in orig_list}\n",
    "\n",
    "    if len(tb256_stems) == len(orig_stems) and tb256_stems == orig_stems:\n",
    "        log.info(\"[DIR CHECK] TB_256_DIR and ORIG_TB_DIR counts & stems MATCH ✅ (%d images)\", len(tb256_stems))\n",
    "    else:\n",
    "        missing_in_tb256 = sorted(orig_stems - tb256_stems)\n",
    "        missing_in_orig = sorted(tb256_stems - orig_stems)\n",
    "        log.warning(\"[DIR CHECK] Count or stem mismatch: TB_256_DIR=%d, ORIG_TB_DIR=%d\",\n",
    "                    len(tb256_stems), len(orig_stems))\n",
    "        if missing_in_tb256:\n",
    "            log.warning(\"[DIR CHECK] Present in ORIG_TB_DIR but missing in TB_256_DIR (showing up to 10): %s\",\n",
    "                        missing_in_tb256[:10])\n",
    "        if missing_in_orig:\n",
    "            log.warning(\"[DIR CHECK] Present in TB_256_DIR but missing in ORIG_TB_DIR (showing up to 10): %s\",\n",
    "                        missing_in_orig[:10])\n",
    "        # Proceed with what we can actually process: images that exist in TB_256_DIR (+ listed in CSV below)\n",
    "\n",
    "    # Model & CAM\n",
    "    mm = _load_ckpt_model(cfg, ckpt_path)\n",
    "    print_model_overview_for_cam(mm, img_size=int(getattr(cfg, \"img_size\", 224)))\n",
    "    cam_model = CamImageWrapper(mm).to(device).eval()\n",
    "    target_layer = select_target_layer_for_cam(mm)\n",
    "    for p in target_layer.parameters(): p.requires_grad_(True)\n",
    "    tl_name = _qualname_of_module(mm, target_layer)\n",
    "    log.info(f\"[CAM] Target layer: {tl_name}  kernel={getattr(target_layer, 'kernel_size', None)}\")\n",
    "\n",
    "    methods = {\n",
    "        \"gradcam\": GradCAM, \"gradcam++\": GradCAMPlusPlus, \"hirescam\": HiResCAM,\n",
    "        \"xgradcam\": XGradCAM, \"layercam\": LayerCAM, \"eigencam\": EigenCAM,\n",
    "        \"eigengradcam\": EigenGradCAM, \"scorecam\": ScoreCAM, \"ablationcam\": AblationCAM\n",
    "    }\n",
    "    mkey = cam_method.lower()\n",
    "    if mkey not in methods:\n",
    "        raise ValueError(f\"Unknown CAM method '{cam_method}'\")\n",
    "    cam = _init_cam(methods[mkey], model=cam_model, target_layers=[target_layer])\n",
    "    try: cam.batch_size = 1\n",
    "    except Exception: pass\n",
    "    preprocess = build_preprocess(int(getattr(cfg, \"img_size\", 224)))\n",
    "\n",
    "    # TB filenames from CSV & directory cross-check (works off TB_256_DIR presence)\n",
    "    df_all = _read_test_csv(csv_test_path)\n",
    "    df_tb  = df_all[df_all[\"img\"].str.lower().str.startswith(\"tb\")].copy()\n",
    "    csv_tb_names = sorted(df_tb[\"img\"].astype(str).unique().tolist())\n",
    "    tb_names = crosscheck_tb(csv_tb_names, tb_256_dir)  # intersection(CSV TB, TB_256_DIR TB)\n",
    "    # tb_names is the exact set we will process; typically 799 in your scenario.\n",
    "    expected_stems = {os.path.splitext(n)[0] for n in tb_names}\n",
    "    log.info(\"[PROCESS SET] Will process %d TB images present in TB_256_DIR & CSV.\", len(tb_names))\n",
    "\n",
    "    # Build GT box map in final 256×256 space using mask-driven mapping\n",
    "    # Enforces: every processed image must produce ≥1 mapped GT box\n",
    "    gt_boxes_map = build_gt_box_map_final256(bbox_csv_path, masks_256_dir, tb_names)\n",
    "\n",
    "    # Write YOLO label files (normalized [0,1], no class id)\n",
    "    yolo_dir = out_dirs[\"bbox_coord\"]\n",
    "    img_side = float(FINAL_SIZE)  # 256.0\n",
    "    n_written = 0\n",
    "    for name in tb_names:\n",
    "        gt_boxes = gt_boxes_map.get(name, gt_boxes_map.get(os.path.basename(name), []))\n",
    "        if not gt_boxes:\n",
    "            # Should not happen due to enforcement above; keep guard.\n",
    "            raise RuntimeError(f\"[YOLO] No mapped GT boxes for {name}, but all processed TB images must have GT.\")\n",
    "        stem = os.path.splitext(os.path.basename(name))[0]\n",
    "        txt_path = os.path.join(yolo_dir, f\"{stem}.txt\")\n",
    "        lines = []\n",
    "        for (x1, y1, x2, y2) in gt_boxes:\n",
    "            w = max(0.0, (x2 - x1) / img_side)\n",
    "            h = max(0.0, (y2 - y1) / img_side)\n",
    "            cx = ((x1 + x2) / 2.0) / img_side\n",
    "            cy = ((y1 + y2) / 2.0) / img_side\n",
    "            # Clamp to [0,1]\n",
    "            cx = min(1.0, max(0.0, cx))\n",
    "            cy = min(1.0, max(0.0, cy))\n",
    "            w = min(1.0, max(0.0, w))\n",
    "            h = min(1.0, max(0.0, h))\n",
    "            lines.append(f\"{cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")  # NO class id\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(lines) + (\"\\n\" if lines else \"\"))\n",
    "        n_written += 1\n",
    "    log.info(\"[YOLO] Wrote %d label files to: %s\", n_written, yolo_dir)\n",
    "\n",
    "        # Iterate TB images and draw CAM + GT\n",
    "    for name in tb_names:\n",
    "        img_path = _find_by_basename(tb_256_dir, name) or os.path.join(tb_256_dir, name)\n",
    "        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            log.warning(\"[CAM] Skipping unreadable: %s\", img_path)\n",
    "            continue\n",
    "        H, W = bgr.shape[:2]\n",
    "        if (H, W) != (FINAL_SIZE, FINAL_SIZE):\n",
    "            bgr = cv2.resize(bgr, (FINAL_SIZE, FINAL_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # model input\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = preprocess(rgb).unsqueeze(0).to(device)\n",
    "\n",
    "        # CAM\n",
    "        with torch.enable_grad():\n",
    "            mask = cam(\n",
    "                input_tensor=x,\n",
    "                targets=[ClassifierOutputTarget(int(TARGET_CLASS_IDX))],\n",
    "                aug_smooth=True, eigen_smooth=True\n",
    "            )[0] if mkey != \"eigencam\" else cam(input_tensor=x)[0]\n",
    "\n",
    "        # Normalize + upsample to 256\n",
    "        mmin, mmax = float(np.min(mask)), float(np.max(mask))\n",
    "        mask = (mask - mmin) / (mmax - mmin + 1e-8)\n",
    "        mask_u8 = (np.clip(cv2.resize(mask, (FINAL_SIZE, FINAL_SIZE), interpolation=cv2.INTER_NEAREST), 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        # Heatmap overlay\n",
    "        heat = cv2.applyColorMap(mask_u8, cv2.COLORMAP_HOT)\n",
    "        heat_overlay = cv2.addWeighted(heat, float(heatmap_alpha), bgr, 1.0 - float(heatmap_alpha), 0.0)\n",
    "\n",
    "        # Contours & model boxes\n",
    "        thr_val = int(255 * float(bin_thr))\n",
    "        _, binm = cv2.threshold(mask_u8, thr_val, 255, cv2.THRESH_BINARY)\n",
    "        contours,_ = cv2.findContours(binm, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        cont_img = bgr.copy()\n",
    "        if contours: cv2.drawContours(cont_img, contours, -1, CONTOUR_COLOR, CONTOUR_THICK, lineType=cv2.LINE_AA)\n",
    "\n",
    "        model_boxes = []\n",
    "        for c in contours:\n",
    "            x0, y0, w0, h0 = cv2.boundingRect(c)\n",
    "            x1, y1, x2, y2 = x0, y0, x0+w0, y0+h0\n",
    "            model_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "        # GT boxes (already mapped to final 256×256)\n",
    "        gt_boxes = gt_boxes_map.get(name, gt_boxes_map.get(os.path.basename(name), []))\n",
    "\n",
    "        # Write outputs\n",
    "        stem = os.path.splitext(os.path.basename(name))[0]\n",
    "        cv2.imwrite(os.path.join(save_root, \"images\",   f\"{stem}.png\"), bgr)\n",
    "\n",
    "        # heatmap + GT\n",
    "        heat_ov = heat_overlay.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "            cv2.rectangle(heat_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(save_root, \"heatmaps\", f\"{stem}__{cfg.network.name}__{mkey}.png\"), heat_ov)\n",
    "\n",
    "        # contours + GT\n",
    "        cont_ov = cont_img.copy()\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "            cv2.rectangle(cont_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(save_root, \"contours\", f\"{stem}__{cfg.network.name}__{mkey}.png\"), cont_ov)\n",
    "\n",
    "        # model boxes + GT\n",
    "        box_ov = bgr.copy()\n",
    "        for (x1, y1, x2, y2) in model_boxes:\n",
    "            cv2.rectangle(box_ov, (x1, y1), (x2, y2), BB_MODEL_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        for (gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "            cv2.rectangle(box_ov, (gx1, gy1), (gx2, gy2), BB_GT_COLOR, BB_THICK, lineType=cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(save_root, \"bboxes\",   f\"{stem}__{cfg.network.name}__{mkey}.png\"), box_ov)\n",
    "\n",
    "    # Post-run verification of bbox_coord vs image dirs\n",
    "    yolo_files = sorted([p for p in os.listdir(yolo_dir) if p.lower().endswith(\".txt\")])\n",
    "    yolo_stems = {os.path.splitext(f)[0] for f in yolo_files}\n",
    "    \n",
    "    # Strict: bbox_coord must match the processed set (TB_256_DIR ∩ CSV)\n",
    "    if yolo_stems != expected_stems:\n",
    "        diff_missing = sorted(expected_stems - yolo_stems)[:10]\n",
    "        diff_extra = sorted(yolo_stems - expected_stems)[:10]\n",
    "        raise RuntimeError(\n",
    "            f\"[VERIFY] bbox_coord stems mismatch with processed set (TB_256_DIR ∩ CSV). \"\n",
    "            f\"Missing labels (first10): {diff_missing} | Extra labels (first10): {diff_extra}\"\n",
    "        )\n",
    "\n",
    "    # Lenient: vs ORIG_TB_DIR — only warn (known one missing in ORIG without GT crop)\n",
    "    if yolo_stems != orig_stems:\n",
    "        diff_missing_vs_orig = sorted(orig_stems - yolo_stems)[:10]\n",
    "        diff_extra_vs_orig = sorted(yolo_stems - orig_stems)[:10]\n",
    "        log.warning(\n",
    "            \"[VERIFY] bbox_coord stems differ from ORIG_TB_DIR (tolerated). \"\n",
    "            \"Missing vs ORIG (first10): %s | Extra vs ORIG (first10): %s\",\n",
    "            diff_missing_vs_orig, diff_extra_vs_orig\n",
    "        )\n",
    "    else:\n",
    "        log.info(\"[VERIFY] bbox_coord stems also match ORIG_TB_DIR ✅\")\n",
    "\n",
    "    log.info(\"[VERIFY] bbox_coord count & stems MATCH processed TB set ✅ (%d files)\", len(yolo_stems))\n",
    "\n",
    "    # Cleanup\n",
    "    try:\n",
    "        if hasattr(cam, \"activations_and_grads\") and (cam.activations_and_grads is not None):\n",
    "            cam.activations_and_grads.release()\n",
    "    except Exception:\n",
    "        pass\n",
    "    del cam, cam_model, mm\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    log.info(\"[CAM] Saved to: %s\", save_root)\n",
    "\n",
    "# Runner\n",
    "assert os.path.isfile(MANUAL_CKPT_PATH), f\"Checkpoint not found: {MANUAL_CKPT_PATH}\"\n",
    "best_subdir = os.path.dirname(MANUAL_CKPT_PATH)\n",
    "save_root = os.path.join(best_subdir, SAVE_ROOT_NAME)\n",
    "log.info(\"================ Grad-CAM (TBX11K TB; mask-mapped GT + YOLO dump) ================\")\n",
    "log.info(f\"Checkpoint : {MANUAL_CKPT_PATH}\")\n",
    "log.info(f\"Backbone   : {cfg.network.name}\")\n",
    "log.info(f\"Modality   : {cfg.modality.name}\")\n",
    "log.info(f\"csv_test   : {CSV_TEST_PATH}\")\n",
    "log.info(f\"TB dir     : {TB_256_DIR} (final lung-cropped 256×256)\")\n",
    "log.info(f\"orig TB dir: {ORIG_TB_DIR} (original 512×512)\")\n",
    "log.info(f\"Masks dir  : {TB_MASKS_256_DIR} (pre-crop masks 256×256)\")\n",
    "log.info(f\"BBoxes csv : {BBOX_CSV_PATH} (authors' boxes @512×512)\")\n",
    "log.info(f\"Save root  : {save_root}\")\n",
    "log.info(\"==================================================================================\")\n",
    "\n",
    "# Cross-checks and YOLO writing happen inside the function (logged to console)\n",
    "run_gradcam_tbx11k_tb_mask_mapped(\n",
    "    cfg=cfg,\n",
    "    ckpt_path=MANUAL_CKPT_PATH,\n",
    "    csv_test_path=CSV_TEST_PATH,\n",
    "    tb_256_dir=TB_256_DIR,\n",
    "    masks_256_dir=TB_MASKS_256_DIR,\n",
    "    bbox_csv_path=BBOX_CSV_PATH,\n",
    "    save_root=save_root,\n",
    "    cam_method=CAM_METHOD,\n",
    "    heatmap_alpha=HEATMAP_ALPHA,\n",
    "    bin_thr=BIN_THR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5bfb9-e010-4479-9779-478ec3914931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sanity check: overlay YOLO bbox_coord on cropped TB images (random 3) ===\n",
    "\n",
    "# Config\n",
    "TB_256_DIR = \"/dataset/tbx11k/cropped/tb\"\n",
    "BBOX_COORD_DIR = \"/gradcam_tbx11k_vgg11_multimodal_external_overlap/bbox_coord\"\n",
    "FINAL_SIZE = 256  # expected image side (H=W=256)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"sanity-yolo-overlay\")\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "def _is_img(path: str) -> bool:\n",
    "    return os.path.splitext(path)[1].lower() in IMG_EXTS\n",
    "\n",
    "def _list_images(dir_path: str) -> List[str]:\n",
    "    out = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for fn in files:\n",
    "            fp = os.path.join(root, fn)\n",
    "            if _is_img(fp):\n",
    "                out.append(fp)\n",
    "    return sorted(out)\n",
    "\n",
    "def _stem(path: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "def _find_by_stem(dir_path: str, stem: str) -> str | None:\n",
    "    for ext in IMG_EXTS:\n",
    "        p = os.path.join(dir_path, stem + ext)\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _read_yolo_no_class(txt_path: str) -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Returns list of (xc, yc, w, h) normalized to [0,1].\n",
    "    Skips blank/comment lines; robust to extra spaces.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = s.split()\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            try:\n",
    "                xc, yc, w, h = map(float, parts[:4])\n",
    "                # clamp defensively\n",
    "                xc = min(1.0, max(0.0, xc))\n",
    "                yc = min(1.0, max(0.0, yc))\n",
    "                w  = min(1.0, max(0.0, w))\n",
    "                h  = min(1.0, max(0.0, h))\n",
    "                boxes.append((xc, yc, w, h))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return boxes\n",
    "\n",
    "def _yolo_to_xyxy(box: Tuple[float,float,float,float], img_w: int, img_h: int) -> Tuple[int,int,int,int]:\n",
    "    xc, yc, w, h = box\n",
    "    bw = w * img_w\n",
    "    bh = h * img_h\n",
    "    x1 = int(round((xc * img_w) - bw / 2.0))\n",
    "    y1 = int(round((yc * img_h) - bh / 2.0))\n",
    "    x2 = int(round((xc * img_w) + bw / 2.0))\n",
    "    y2 = int(round((yc * img_h) + bh / 2.0))\n",
    "    # clip\n",
    "    x1 = max(0, min(img_w-1, x1))\n",
    "    y1 = max(0, min(img_h-1, y1))\n",
    "    x2 = max(0, min(img_w-1, x2))\n",
    "    y2 = max(0, min(img_h-1, y2))\n",
    "    if x2 <= x1: x2 = min(img_w-1, x1+1)\n",
    "    if y2 <= y1: y2 = min(img_h-1, y1+1)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def sanity_check_yolo_overlays(\n",
    "    tb_dir: str = TB_256_DIR,\n",
    "    yolo_dir: str = BBOX_COORD_DIR,\n",
    "    n_samples: int = 3,\n",
    "    seed: int | None = None,\n",
    ") -> None:\n",
    "    assert os.path.isdir(tb_dir), f\"Missing TB dir: {tb_dir}\"\n",
    "    assert os.path.isdir(yolo_dir), f\"Missing bbox_coord dir: {yolo_dir}\"\n",
    "    imgs = _list_images(tb_dir)\n",
    "    stems_img = {_stem(p) for p in imgs if os.path.basename(p).lower().startswith(\"tb\")}\n",
    "    yolo_txts = [os.path.join(yolo_dir, f) for f in os.listdir(yolo_dir) if f.lower().endswith(\".txt\")]\n",
    "    stems_yolo = {_stem(p) for p in yolo_txts}\n",
    "    only_in_imgs = sorted(stems_img - stems_yolo)\n",
    "    only_in_yolo = sorted(stems_yolo - stems_img)\n",
    "    log.info(\"Cropped TB images: %d | YOLO label files: %d | intersection: %d\",\n",
    "             len(stems_img), len(stems_yolo), len(stems_img & stems_yolo))\n",
    "    if only_in_imgs:\n",
    "        log.warning(\"There are %d TB images without a YOLO label file (first 5): %s\",\n",
    "                    len(only_in_imgs), only_in_imgs[:5])\n",
    "    if only_in_yolo:\n",
    "        log.warning(\"There are %d YOLO label files without a matching TB image (first 5): %s\",\n",
    "                    len(only_in_yolo), only_in_yolo[:5])\n",
    "    common_stems = sorted(list(stems_img & stems_yolo))\n",
    "    assert len(common_stems) >= n_samples, f\"Need at least {n_samples} common items; got {len(common_stems)}.\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    picks = random.sample(common_stems, n_samples)\n",
    "\n",
    "    # Show overlays\n",
    "    for stem in picks:\n",
    "        img_path = _find_by_stem(tb_dir, stem)\n",
    "        txt_path = os.path.join(yolo_dir, stem + \".txt\")\n",
    "\n",
    "        assert img_path is not None and os.path.isfile(img_path), f\"Missing image for {stem}\"\n",
    "        assert os.path.isfile(txt_path), f\"Missing label file for {stem}\"\n",
    "\n",
    "        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            log.error(\"Unreadable image: %s\", img_path)\n",
    "            continue\n",
    "        H, W = bgr.shape[:2]\n",
    "        if (H, W) != (FINAL_SIZE, FINAL_SIZE):\n",
    "            log.warning(\"Image %s is %dx%d (expected %dx%d); resizing for display.\",\n",
    "                        stem, W, H, FINAL_SIZE, FINAL_SIZE)\n",
    "            bgr = cv2.resize(bgr, (FINAL_SIZE, FINAL_SIZE), interpolation=cv2.INTER_AREA)\n",
    "            H, W = FINAL_SIZE, FINAL_SIZE\n",
    "\n",
    "        boxes_norm = _read_yolo_no_class(txt_path)\n",
    "        log.info(\"[%s] %d GT box(es) from %s\", stem, len(boxes_norm), pathlib.Path(txt_path).name)\n",
    "\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        ax = plt.gca()\n",
    "        ax.imshow(rgb)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(f\"{stem} — {len(boxes_norm)} box(es)\")\n",
    "\n",
    "        # draw boxes\n",
    "        for (xc, yc, w, h) in boxes_norm:\n",
    "            x1, y1, x2, y2 = _yolo_to_xyxy((xc, yc, w, h), W, H)\n",
    "            rect = Rectangle((x1, y1), (x2 - x1), (y2 - y1),\n",
    "                             linewidth=2, edgecolor=\"r\", facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "            # mark center\n",
    "            ax.plot([xc*W], [yc*H], marker=\"x\", markersize=6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run once per check (random 3 each time)\n",
    "sanity_check_yolo_overlays(n_samples=4, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5fcab-c098-415b-bd09-6908c259694d",
   "metadata": {},
   "source": [
    "## END OF CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_class_py39)",
   "language": "python",
   "name": "pytorch_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
