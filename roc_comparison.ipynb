{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10820f-f2c8-43ff-9b60-1ee5a07e9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ca4a0-88eb-4d8b-a1f4-aae3ad1bb8f2",
   "metadata": {},
   "source": [
    "## ROC Comparison\n",
    "\n",
    "The code:\n",
    "\n",
    "1. loads three checkpoints (unimodal VGG-11; multimodal raw; multimodal structured),\n",
    "\n",
    "2. prints each model architecture,\n",
    "\n",
    "3. runs image-only inference on the user-selected test set (Shenzhen / Montgomery / TBX11K),\n",
    "\n",
    "4. computes AUROC and best-MCC (with its threshold),\n",
    "\n",
    "5. and plots a smooth ROC comparison using customizable colors (here, red/blue/green).\n",
    "\n",
    "The codes includes a loader that reconstructs the unimodal VGG-11 and the MultimodalNet (image head) so the multimodal weights can load with strict=False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31aafdf-ce60-43bb-9362-a9fe3dd1f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, math, random, logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models as tvm\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch._utils \n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch._utils  # force-load the private module so torch._weights_only_unpickler can see it\n",
    "import cv2, albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ee88e-dc7a-4999-888d-fbb89359ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC comparison for VGG11-based unimodal & multimodal models\n",
    "# - Prints model architectures\n",
    "# - Evaluates on selected test CSV(s): Shenzhen / Montgomery / TBX11K (headerless \"filename,label\")\n",
    "# - Plots AUROC & best-MCC for each model \n",
    "\n",
    "# -------------------------- Reproducibility -------------------------- #\n",
    "def set_deterministic(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_deterministic(42)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"roc-compare\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d7684-ab0f-4989-b60a-8445a3874b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "\n",
    "DATASET_ROOT = Path(\"/gpfs/gsfs12/users/rajaramans2/projects/omsakthi_multimodal/multimodal_shenzhen/dataset\")\n",
    "\n",
    "# Shenzhen:\n",
    "IMAGES_DIR = DATASET_ROOT / \"images\"\n",
    "CSV_PATHS = [DATASET_ROOT / \"label_test.csv\"]\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Model checkpoints\n",
    "CKPT_UNIMODAL_VGG11 = Path(\"/best_vgg11_img_val_loss.pt\")\n",
    "CKPT_MM_RAW = Path(\"/mm_grid1/best_vgg11_multimodal_val_loss.pt\")\n",
    "CKPT_MM_STRUCT = Path(\"/mm_grid2/best_vgg11_multimodal_val_loss.pt\")\n",
    "\n",
    "# name -> (ckpt_path, color, pipeline)\n",
    "# pipeline ∈ {\"uni\",\"mm\"} decides which loader/model-ctor to use\n",
    "MODEL_STYLES: Dict[str, Tuple[Path, str, str]] = {\n",
    "    \"Unimodal\": (\"{}\".format(CKPT_UNIMODAL_VGG11), \"red\", \"uni\"),\n",
    "    \"Multimodal (Raw Text)\": (\"{}\".format(CKPT_MM_RAW), \"blue\", \"mm\"),\n",
    "    \"Multimodal (Structured Text)\": (\"{}\".format(CKPT_MM_STRUCT), \"green\", \"mm\"),\n",
    "}\n",
    "FONT = {\"title\": 14, \"axis\": 12, \"tick\": 10, \"legend\": 11,\n",
    "        \"legend_color\": \"black\", \"axis_color\": \"black\"}\n",
    "\n",
    "# CSV\n",
    "def _is_image_name(name: str) -> bool:\n",
    "    return Path(name).suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "def _load_csv_headerless(csv_path: Path) -> pd.DataFrame:\n",
    "    if not Path(csv_path).is_file():\n",
    "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"CSV must have >=2 columns (filename,label): {csv_path}\")\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = [\"img\", \"label\"]\n",
    "    df[\"img\"] = df[\"img\"].astype(str).map(lambda s: os.path.basename(s.strip()))\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"img\"].map(_is_image_name)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def union_csvs(paths: List[Path]) -> pd.DataFrame:\n",
    "    frames = [ _load_csv_headerless(p) for p in paths ]\n",
    "    df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=[\"img\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# UNIMODAL PIPELINE\n",
    "#  - Albumentations + cv2: GRAY→RGB, Resize(INTER_AREA), Normalize(ImageNet)\n",
    "#  - ImageOnlyHead with forward_image()\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "class UniCsvImageDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, df: pd.DataFrame, size: int = 224):\n",
    "        super().__init__()\n",
    "        self.root, self.names = Path(images_dir), df[\"img\"].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "        self.tf = A.Compose([\n",
    "            A.Resize(size, size, interpolation=cv2.INTER_AREA),\n",
    "            A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    def __len__(self) -> int: return len(self.names)\n",
    "    def __getitem__(self, i: int):\n",
    "        name = self.names[i]; y = self.labels[i]\n",
    "        p = self.root / name\n",
    "        img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found or unreadable: {p}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        x = self.tf(image=img)[\"image\"]\n",
    "        return x, int(y), name\n",
    "\n",
    "def _ends_with_3x3(module: nn.Module) -> bool:\n",
    "    last = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last = m\n",
    "    return (last is not None) and (tuple(getattr(last, \"kernel_size\", (0,0))) == (3,3))\n",
    "\n",
    "def _post3x3(ch: int) -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Conv2d(ch, ch, 3, 1, 1, bias=False), nn.BatchNorm2d(ch), nn.SiLU(inplace=True))\n",
    "\n",
    "class VGG11ImageEncoder(nn.Module):\n",
    "    def __init__(self, p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        m = tvm.vgg11_bn(weights=None)\n",
    "        self.encoder = m.features\n",
    "        ch = None\n",
    "        for mod in self.encoder.modules():\n",
    "            if isinstance(mod, nn.Conv2d): ch = mod.out_channels\n",
    "        self.post3x3 = _post3x3(ch) if not _ends_with_3x3(self.encoder) else nn.Identity()\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = int(ch)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.encoder(x); f = self.post3x3(f); f = self.gap(f).flatten(1); f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "class ImageOnlyHead(nn.Module):\n",
    "    def __init__(self, hidden: int = 256, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.img_enc = VGG11ImageEncoder(p_drop=0.3)\n",
    "        self.proj = nn.Linear(self.img_enc.out_dim, hidden)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.mid = nn.Linear(hidden, hidden)\n",
    "        self.cls = nn.Linear(hidden, n_classes)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.img_enc(x); z = self.mid(self.act(self.proj(f))); return self.cls(z)\n",
    "    def forward_image(self, x: torch.Tensor) -> torch.Tensor: return self.forward(x)\n",
    "\n",
    "def _strip_prefix(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    if not sd: return sd\n",
    "    keys = list(sd.keys())\n",
    "    prefixes = [pref for k in keys for pref in (\"module.\",\"model.\",\"net.\") if k.startswith(pref)]\n",
    "    if not prefixes: return sd\n",
    "    pref = max(set(prefixes), key=prefixes.count)\n",
    "    return { (k[len(pref):] if k.startswith(pref) else k): v for k,v in sd.items() }\n",
    "\n",
    "def _load_checkpoint(ckpt_path: Path):\n",
    "    try:\n",
    "        return torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    except Exception as e:\n",
    "        log.warning(f\"[CKPT] load failed ({e}); retry with explicit pickle_module\")\n",
    "        return torch.load(ckpt_path, map_location=\"cpu\", weights_only=False, pickle_module=pickle)\n",
    "\n",
    "def build_unimodal_from_ckpt(ckpt_path: Path) -> nn.Module:\n",
    "    chk = _load_checkpoint(ckpt_path)\n",
    "    if isinstance(chk, nn.Module): return chk\n",
    "    sd = chk.get(\"state_dict\", chk.get(\"model\", chk))\n",
    "    if not isinstance(sd, dict): raise RuntimeError(\"No state_dict in unimodal ckpt.\")\n",
    "    sd = _strip_prefix(sd)\n",
    "    model = ImageOnlyHead(hidden=256, n_classes=NUM_CLASSES)\n",
    "    model.load_state_dict(sd, strict=False)\n",
    "    return model\n",
    "\n",
    "# MULTIMODAL PIPELINE\n",
    "#  - PIL + torchvision.transforms.Resize → ToTensor → Normalize(ImageNet)\n",
    "#  - MultimodalImageHead mapping + forward_image()\n",
    "\n",
    "class MmCsvImageDataset(Dataset):\n",
    "    \"\"\"PIL/torchvision test-time pipeline as in your custom_code.\"\"\"\n",
    "    def __init__(self, images_dir: Path, df: pd.DataFrame, size: int = 224):\n",
    "        super().__init__()\n",
    "        self.root, self.names = Path(images_dir), df[\"img\"].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize((size, size)),\n",
    "            T.Lambda(lambda x: x.convert(\"RGB\") if x.mode != \"RGB\" else x),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "        ])\n",
    "    def __len__(self) -> int: return len(self.names)\n",
    "    def __getitem__(self, i: int):\n",
    "        name = self.names[i]; y = self.labels[i]\n",
    "        p = self.root / name\n",
    "        if not p.is_file(): raise FileNotFoundError(f\"Image not found: {p}\")\n",
    "        img = Image.open(p)\n",
    "        x = self.tf(img)\n",
    "        return x, int(y), name\n",
    "\n",
    "def _ends_with_3x3_tv(module: nn.Module) -> bool:\n",
    "    last_conv = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d): last_conv = m\n",
    "    return (last_conv is not None) and (tuple(getattr(last_conv, \"kernel_size\",(0,0)))==(3,3))\n",
    "\n",
    "def _insert_post3x3_if_needed_tv(ch: int) -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Conv2d(ch, ch, 3, 1, 1, bias=False),\n",
    "                         nn.BatchNorm2d(ch), nn.SiLU(inplace=True))\n",
    "\n",
    "class TorchvisionVGGBackbone(nn.Module):\n",
    "    \"\"\"Matches your custom_code's multimodal image backbone wrapper.\"\"\"\n",
    "    def __init__(self, which: str = \"vgg11_bn\", p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        tvm = torchvision.models\n",
    "        m = tvm.vgg11_bn(weights=None)\n",
    "        self.encoder = m.features\n",
    "        ch = None\n",
    "        for mod in self.encoder.modules():\n",
    "            if isinstance(mod, nn.Conv2d): ch = mod.out_channels\n",
    "        self.post3x3 = _insert_post3x3_if_needed_tv(ch) if not _ends_with_3x3_tv(self.encoder) else nn.Identity()\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.drop = nn.Dropout(p=p_drop)\n",
    "        self.out_dim = int(ch)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.encoder(x); f = self.post3x3(f); f = self.gap(f).flatten(1); f = self.drop(f)\n",
    "        return f\n",
    "\n",
    "class MultimodalImageHead(nn.Module):\n",
    "    \"\"\"Exact structure as in your custom_code: img_enc -> img_proj -> ReLU -> mid -> cls with forward_image().\"\"\"\n",
    "    def __init__(self, hidden: int = 256, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.img_enc = TorchvisionVGGBackbone(\"vgg11_bn\", p_drop=0.3)\n",
    "        ch  = self.img_enc.out_dim\n",
    "        self.img_proj = nn.Linear(ch, hidden)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.mid = nn.Linear(hidden, hidden)\n",
    "        self.cls = nn.Linear(hidden, n_classes)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f_img = self.img_enc(x)\n",
    "        z = self.mid(self.act(self.img_proj(f_img)))\n",
    "        return self.cls(z)\n",
    "    def forward_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(image)\n",
    "\n",
    "def _strip_prefix_mm(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    if not sd: return sd\n",
    "    keys = list(sd.keys())\n",
    "    prefixes = [pref for k in keys for pref in (\"module.\",\"model.\",\"net.\") if k.startswith(pref)]\n",
    "    if not prefixes: return sd\n",
    "    pref = max(set(prefixes), key=prefixes.count)\n",
    "    return { (k[len(pref):] if k.startswith(pref) else k): v for k,v in sd.items() }\n",
    "\n",
    "def build_multimodal_from_ckpt(ckpt_path: Path) -> nn.Module:\n",
    "    \"\"\"Use the same robust torch.load and mapping you used in custom_code.\"\"\"\n",
    "    try:\n",
    "        chk = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError:\n",
    "        chk = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        chk = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False, pickle_module=pickle)\n",
    "\n",
    "    if isinstance(chk, nn.Module):\n",
    "        log.info(f\"[CKPT/MM] Pickled nn.Module loaded: {ckpt_path.name}\")\n",
    "        return chk\n",
    "\n",
    "    # tease out state_dict (as in your custom_code)\n",
    "    if \"state_dict\" in chk and isinstance(chk[\"state_dict\"], dict):\n",
    "        sd = chk[\"state_dict\"]\n",
    "    elif \"model\" in chk and isinstance(chk[\"model\"], dict):\n",
    "        sd = chk[\"model\"]\n",
    "    elif all(isinstance(v, torch.Tensor) for v in chk.values()):\n",
    "        sd = chk\n",
    "    else:\n",
    "        tensorish = {k: v for k, v in chk.items() if isinstance(v, torch.Tensor)}\n",
    "        if tensorish: sd = tensorish\n",
    "        else: raise RuntimeError(\"No tensor state_dict in multimodal ckpt.\")\n",
    "\n",
    "    sd = _strip_prefix_mm(sd)\n",
    "    keys = list(sd.keys())\n",
    "\n",
    "    looks_multimodal = any(k.startswith(\"img_enc.\") for k in keys) or \\\n",
    "                       any(k.startswith(\"img_proj\") for k in keys) or \\\n",
    "                       any(k.startswith(\"mid\") for k in keys) or \\\n",
    "                       any(k.startswith(\"cls\") for k in keys)\n",
    "\n",
    "    if looks_multimodal:\n",
    "        model = MultimodalImageHead(hidden=256, n_classes=NUM_CLASSES)\n",
    "        model.load_state_dict(sd, strict=False)\n",
    "        return model\n",
    "\n",
    "    # fallback (rare): treat as unimodal vgg11_bn classifier\n",
    "    base = torchvision.models.vgg11_bn(weights=None)\n",
    "    base.classifier[-1] = nn.Linear(base.classifier[-1].in_features, NUM_CLASSES)\n",
    "    base.load_state_dict(sd, strict=False)\n",
    "    return base\n",
    "\n",
    "# Shared inference helpers\n",
    "@torch.no_grad()\n",
    "def predict_scores(model: nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval().to(DEVICE)\n",
    "    probs, labels = [], []\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        logits = model.forward_image(x) if hasattr(model, \"forward_image\") else model(x)\n",
    "        if logits.ndim == 1 or logits.shape[-1] == 1:\n",
    "            p1 = torch.sigmoid(logits.float()).view(-1).cpu().numpy()\n",
    "        else:\n",
    "            p1 = torch.softmax(logits.float(), dim=-1)[:, 1].cpu().numpy()\n",
    "        probs.append(p1); labels.append(y.numpy())\n",
    "    return np.concatenate(probs), np.concatenate(labels).astype(np.int64)\n",
    "\n",
    "def smooth_for_plot(fpr: np.ndarray, tpr: np.ndarray, n: int = 1200) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    grid = np.linspace(0, 1, n)\n",
    "    return grid, np.interp(grid, fpr, tpr)\n",
    "\n",
    "# Build two loaders on the same CSVs: uni_loader & mm_loader\n",
    "assert IMAGES_DIR.is_dir(), f\"Images directory not found: {IMAGES_DIR}\"\n",
    "df_all = union_csvs(CSV_PATHS)\n",
    "log.info(f\"[DATA] Using {len(df_all)} images from {len(CSV_PATHS)} CSV file(s).\")\n",
    "\n",
    "uni_ds = UniCsvImageDataset(IMAGES_DIR, df_all, size=IMG_SIZE)   # cv2/Albumentations\n",
    "mm_ds = MmCsvImageDataset (IMAGES_DIR, df_all, size=IMG_SIZE)   # PIL/torchvision\n",
    "\n",
    "uni_loader = DataLoader(uni_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
    "mm_loader = DataLoader(mm_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
    "\n",
    "# Evaluate models, each with its own pipeline\n",
    "results = []\n",
    "for name, (ckpt_path_str, color, pipe) in MODEL_STYLES.items():\n",
    "    ckpt_path = Path(ckpt_path_str)\n",
    "    assert ckpt_path.is_file(), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    log.info(f\"\\n===== {name} | pipeline={pipe} =====\")\n",
    "    if pipe == \"uni\":\n",
    "        model = build_unimodal_from_ckpt(ckpt_path)\n",
    "        loader = uni_loader\n",
    "    elif pipe == \"mm\":\n",
    "        model = build_multimodal_from_ckpt(ckpt_path)\n",
    "        loader = mm_loader\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pipeline tag: {pipe}\")\n",
    "\n",
    "    # Print architecture\n",
    "    print(f\"\\n================ Architecture: {name} ================\\n{model}\\n=====================================================\\n\")\n",
    "    y_score, y_true = predict_scores(model, loader)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
    "    auroc = roc_auc_score(y_true, y_score)\n",
    "    fpr_s, tpr_s = smooth_for_plot(fpr, tpr, n=1200)\n",
    "    results.append({\"name\": name, \"color\": color, \"fpr\": fpr_s, \"tpr\": tpr_s, \"auroc\": float(auroc)})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 7), dpi=120)\n",
    "for r in results:\n",
    "    plt.plot(r[\"fpr\"], r[\"tpr\"], color=r[\"color\"], lw=2.2,\n",
    "             label=f\"{r['name']}  (AUROC={r['auroc']:.4f})\")\n",
    "plt.plot([0,1],[0,1], color=\"gray\", lw=1.0, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=FONT[\"axis\"], color=FONT[\"axis_color\"])\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=FONT[\"axis\"], color=FONT[\"axis_color\"])\n",
    "plt.xticks(fontsize=FONT[\"tick\"], color=FONT[\"axis_color\"])\n",
    "plt.yticks(fontsize=FONT[\"tick\"], color=FONT[\"axis_color\"])\n",
    "leg = plt.legend(loc=\"lower right\", fontsize=FONT[\"legend\"])\n",
    "for txt in leg.get_texts(): txt.set_color(FONT[\"legend_color\"])\n",
    "plt.grid(alpha=0.2, linestyle=\"--\"); plt.tight_layout()\n",
    "out_png = DATASET_ROOT / \"roc_compare.png\"\n",
    "plt.savefig(str(out_png), dpi=400, bbox_inches=\"tight\")\n",
    "log.info(f\"[ROC] Saved: {out_png}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4985238-b2e6-4e48-911e-2e7f367cc146",
   "metadata": {},
   "source": [
    "## END OF CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_ultra_new)",
   "language": "python",
   "name": "pytorch_ultra_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
